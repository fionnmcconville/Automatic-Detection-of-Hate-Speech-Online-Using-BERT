{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HatEval_Tensorflow_TPU.ipynb","provenance":[{"file_id":"1EBH_dCvSIfTM-VlKeBfV2Em3exIJzkih","timestamp":1576806467871},{"file_id":"1JuDopUfWUBPOSoIOBuqs7S2tH2FJHokl","timestamp":1575402454368},{"file_id":"1VNukx0WgDZ6kdgs4gvDj6zZ5czaNCAO6","timestamp":1575299325999},{"file_id":"https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb","timestamp":1574879594425}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"j0a4mTk9o1Qg","colab_type":"code","colab":{}},"source":["# Copyright 2019 Google Inc.\n","\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Q4eUHbzUO-c","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/kpe/bert-for-tf2/blob/master/examples/tpu_movie_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"dCpvgG0vwXAZ","colab_type":"text"},"source":["#Detecting Hate speech Tweets with BERT"]},{"cell_type":"markdown","metadata":{"id":"xiYrZKaHwV81","colab_type":"text"},"source":["We are using bert-tensorflow for this classification task. At the moment I'm making sure it's tensorflow version 1.x because tensorflow version 2 gives issues with Bert at the moment. I believe Tensorflow hopes to have this issue resolved in tensorflow v 2.1\n","\n","We are using a TPU as a GPU does not have the required memory for Large BERT models- it can only cope with the base model. We'll see if there a TPU detected and we'll set it to a global environment variable so it can be accessed by our BERT functions later."]},{"cell_type":"code","metadata":{"id":"mYEcG2vlPumC","colab_type":"code","outputId":"2c4595d3-6254-4bc9-86c0-21a88c5123fa","executionInfo":{"status":"ok","timestamp":1581511058785,"user_tz":0,"elapsed":41565,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["%pip install bert-tensorflow\n","!pip install gcsfs\n","import pandas as pd\n","import numpy as np\n","\n","#Make sure to use tensorflow version 1.x, version 2 doesn't work with bert\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import os\n","\n","#For cross-validation and grid search\n","from itertools import product\n","from tensorflow.python.summary.summary_iterator import summary_iterator\n","from google.cloud import storage\n","import ipywidgets as widgets\n","from IPython.display import display\n","\n","import sklearn\n","from sklearn.model_selection import KFold\n","from sklearn import metrics\n","\n","import html\n","import re\n","import json\n","import pprint\n","import random\n","import string\n","from datetime import datetime\n","\n","import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization\n","from bert import modeling\n","\n","assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n","TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","print('TPU address is', TPU_ADDRESS)\n","\n","#Below we give ourselves as well as the TPU access to our private GCS bucket\n","from google.colab import auth\n","auth.authenticate_user()\n","with tf.Session(TPU_ADDRESS) as session:\n","  # Upload credentials to TPU.\n","  with open('/content/adc.json', 'r') as f:\n","    auth_info = json.load(f)\n","  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n","\n","USE_TPU=True\n","try:\n","  #tf.config.experimental_connect_to_host(TPU_ADDRESS)\n","  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n","    tpu=TPU_ADDRESS)\n","  tf.config.experimental_connect_to_cluster(cluster_resolver)\n","  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n","  tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n","except Exception as ex:\n","  print(ex)\n","  USE_TPU=False\n","\n","print(\"        USE_TPU:\", USE_TPU)\n","print(\"Eager Execution:\", tf.executing_eagerly())\n","\n","assert not tf.executing_eagerly(), \"Eager execution on TPUs have issues currently\"\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting bert-tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n","\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñâ                           | 10kB 20.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                      | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 30kB 2.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç            | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71kB 2.5MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.1\n","Collecting gcsfs\n","  Downloading https://files.pythonhosted.org/packages/3e/9f/864a9ff497ed4ba12502c4037db8c66fde0049d9dd0388bd55b67e5c4249/gcsfs-0.6.0-py2.py3-none-any.whl\n","Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.4.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.21.0)\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.6.2)\n","Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n","Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n","Installing collected packages: gcsfs\n","Successfully installed gcsfs-0.6.0\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","TPU address is grpc://10.79.186.138:8470\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","INFO:tensorflow:Initializing the TPU system: 10.79.186.138:8470\n","INFO:tensorflow:Finished initializing TPU system.\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.79.186.138:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1226287058234983842)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1096397407441273561)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 17391528316428931330)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 7397182021820605127)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9651995611987978998)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 5681953145207326128)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 7357120651941993345)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 18078827992442064428)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7347625494731594129)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 5792709967717720067)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2382550830743014401)\n","        USE_TPU: True\n","Eager Execution: False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sRh9YYYfgUTE","colab_type":"text"},"source":["Setting a random seed for reproducability of results and checking version of tensorflow"]},{"cell_type":"code","metadata":{"id":"dgVteKeTgXrg","colab_type":"code","outputId":"a876a217-6216-4138-d801-4928ca96da9b","executionInfo":{"status":"ok","timestamp":1581511058789,"user_tz":0,"elapsed":41550,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tf.set_random_seed(3060)\n","print(\"Tensorflow Version:\", tf.__version__)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Tensorflow Version: 1.15.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KVB3eOcjxxm1","colab_type":"text"},"source":["Below we will set the directory where we will store our output model. To ensure the right variables are loaded in our run config function later, our ouput directory must be in the same directory as our pre-trained bert model directory.\n","\n","Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."]},{"cell_type":"code","metadata":{"id":"US_EAnICvP7f","colab_type":"code","cellView":"both","outputId":"e51d2bf3-512c-4411-b696-56c8d7bcc6e8","executionInfo":{"status":"ok","timestamp":1581511063257,"user_tz":0,"elapsed":45996,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["#Choose which model you'd like - MUST be in GCP bucket\n","\n","#Bert uncased Large \n","#bert_model_name = 'uncased_L-24_H-1024_A-16' \n","\n","#Large whole word masking\n","bert_model_name = 'wwm_uncased_L-24_H-1024_A-16' \n","\n","#Adding further pretrained model\n","further_pretrained_model = \\\n","os.path.join(bert_model_name, 'further_pretrained_model')\n","\n","#Where we output the final, fine tuned model\n","output_dir = \\\n","os.path.join(further_pretrained_model, 'output')\n","\n","#@markdown Whether or not to clear/delete the directory and create a new one\n","DO_DELETE = True #@param {type:\"boolean\"}\n","#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","USE_BUCKET = True #@param {type:\"boolean\"}\n","BUCKET = 'csc3002' #@param {type:\"string\"}\n","os.environ[\"GCLOUD_PROJECT\"] = \"csc3002\"\n","\n","if USE_BUCKET:\n","  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, output_dir)\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","if DO_DELETE:\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","    # Doesn't matter if the directory didn't exist\n","    pass\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["***** Model output directory: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output *****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yDFCBYMB-m3v","colab_type":"text"},"source":["<b> If you're not connected to a TPU environment but still want to access GCS bucket - run below: </b>"]},{"cell_type":"code","metadata":{"id":"65aCrle2Bmmi","colab_type":"code","outputId":"3e9a75db-f0e8-4602-abc9-52696148e26f","executionInfo":{"status":"ok","timestamp":1581511063258,"user_tz":0,"elapsed":45975,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"from google.colab import drive\n","drive.mount('/content/drive')\n","!gcloud auth activate-service-account --key-file '/content/drive/My Drive/storageCreds.json'\n","\"\"\""],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"from google.colab import drive\\ndrive.mount('/content/drive')\\n!gcloud auth activate-service-account --key-file '/content/drive/My Drive/storageCreds.json'\\n\""]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"pmFYvkylMwXn","colab_type":"text"},"source":["#Data\n","I've stored all of the data in my google bucket for ease of access, authentication will have to be provided"]},{"cell_type":"code","metadata":{"id":"UvknR984WeDW","colab_type":"code","outputId":"ef8b2324-f7e5-4b2f-a9d0-bf0f0d1b4b57","executionInfo":{"status":"ok","timestamp":1581511066444,"user_tz":0,"elapsed":49135,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!gcloud config set project 'my-project-csc3002'\n","\n","train = pd.read_csv('gs://csc3002/hateval2019/hateval2019_en_train.csv', sep=',',  index_col = False, encoding = 'utf-8')\n","train.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","\n","train = train.sample(frac=1, random_state = 3020) #Shuffling really helps model performance\n","train.reset_index(drop = True, inplace = True)\n","train.id = train.index\n","pd.set_option('display.max_colwidth', -1)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Updated property [core/project].\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G_GsYauDVpTB","colab_type":"code","outputId":"0c9b1ecb-bd95-474d-856f-fe36772a5d9a","executionInfo":{"status":"ok","timestamp":1581511066446,"user_tz":0,"elapsed":49114,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(\"Out of {} tweets in this database, {} are not hate, {} are hate\".format(len(train.index), \n","                                                      len(train[train['label']==0]),\n","                                                      len(train[train['label']==1])))\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Out of 9000 tweets in this database, 5217 are not hate, 3783 are hate\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"57uQmKX8yQGq","colab_type":"text"},"source":["<b>Original Dataset </b>"]},{"cell_type":"code","metadata":{"id":"Y2QQv2JDyPiu","colab_type":"code","outputId":"6fbf3ad2-ee21-4e24-e5fa-9705df5a1268","executionInfo":{"status":"ok","timestamp":1581511066447,"user_tz":0,"elapsed":49094,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train.head(30)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","      <th>TR</th>\n","      <th>AG</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Good Morning to;#MAGA #NoDACA #NoDACAAmnesty #BuildTheWall #LockThemAllUp #AmericaFirst #NotABot RT FOREVER We Love you #POTUS @realDonaldTrump</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>People deprived of the vital family support promised to them by the state, during the hardest time of their lives, thanks to... the state: Home Office delays + this country‚Äôs incoherent, cruel, racist, unworkable bureaucratic mess of an immigration system. https://t.co/awyB9MncSU</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>In New Orleans, .pPresenting results on immigrants and civic engagement project @ilctr @MasonCHSS https://t.co/fHUBLcX2ou</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>@keenondevonte üòòüòò down bitch. That‚Äôs what women do. Provide for her man and put his head towards God when he have doubts. I love you üíô</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>@LisaBloom It's women like Kathy Griffin that the word cunt is becoming just as popular here as it is in the United Kingdom and Australia.</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>Hole in one and you deserve for it.Congrats https://t.co/EXzIzEXWhM</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>Guys relax. Leave the twitter fights to the professionals. I mean vaginals. I mean women. #WomenSuck</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>Why? Everywhere they go!! WHY?? Act like normal phucking people!!  #Scum #NoMoreRefugees https://t.co/xlitWTPmJ5</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>Credit card boy Ian Duncan Smith and other Politicians should be forced to pay their dues mind u its not tax credits cuts that's for us</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>@FareedFromSyria @noyalevy_ @ElianeAlhussein @CrazyNormie @UN @HasanAlthoffy @UNICEFEMOPS @talentosprecato @Marcnelsonart @KenRoth @AmnestySyria @UOSSM_France Try @msf or @unicefmena. @ICRC_sy and @refugees, can you help or give direction?</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>@GinnyGinny_1 @AngryAmerican78 @melindamrtn @ImaPatriot4life @PTSD_SURVIVOR @nana8_coney @American2b @stand4honor @bindyb123 @RMills08 @mommofour @GinnyWith_A_G @high_rollerx @_timothysmith_ @4LyingLibs Please follow back #Turnout2018 #MAGA #BUILDTHATWALL</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>@pmbairrins It's not hysterical or exaggeration. Abortion is horrendous. There are other ways to help women in crisis</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>Undocumented my a$$, ILLEGAL!! This child is 11 years old.  https://t.co/SfbJRnl1wD</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>Illegal alien rapes 6-year-old girl, police say https://t.co/6dUGD3KAEP #SecureTheBorder #StopTheInvasion #PreventableCrime</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>These are kids, their little minds not made to grasp the diabolic forces at work using them as props. Thank God it's all going to end soon. https://t.co/77EJQOIvK8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>Hear from some of the victims of Algeria's unlawful mass summary deportation of 1000s of migrants, many of whom are being dumped mercilessly in the scorching desert on the Niger border @HRW https://t.co/qXI2FmAN2G https://t.co/utonArNC6Y</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>Glam Skanks -Karma/Bad Bitch Double Feature [Official Video]: https://t.co/hhU4YeYc3u via @YouTube</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>Madonna whore complex, pedaled by the illusion that women somehow embody the honor of the men with whom they‚Äôre ass‚Ä¶ https://t.co/ge9cqQzX80</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>@JoeNBC The cowards are the corrupt establishment. Trump and the American working class will win over the D.C. establishment. #MAGA #Kavanaugh #walkaway #burtreynolds #BuildThatWall @realDonaldTrump @RealJamesWoods @IngrahamAngle @DonaldJTrumpJr @LouDobbs</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>@lobnaashraff I know walahi.. mna lsa kunt basm3ha m3rfsh influenced you wala sudfa</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>Desperate to do whatever it takes at whatever moral or financial cost to help Libyan coastguard &amp;amp; authorities trap even more refugees &amp;amp; migrants in horrific detention centers, Italy revives \"Friendship\" deal signed 10 years ago https://t.co/Eq4ueES8iD @HRW https://t.co/Pi2jPL5meH https://t.co/uU68Jsy5by</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>UN says Libya recovered some 100 bodies of migrants in 2018 https://t.co/uekURE7xfj https://t.co/7W4JUefT9g</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>Friend- any plans for the weekend? Me- https://t.co/ubhL9ABWCz</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>@POTUS @WhiteHouse Only time of the year you look normal ya kunt</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>In America, only 7% of rapists are convicted. It's about power - men want to know they can rape, harass, abuse any woman &amp;amp; get away with it.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>STFU! Like we need your worthless 2 cents of wisdom! Go crawl back under your rock!!!</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>If I was you I'd hate me too</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>BUILD THAT WALL@realDonaldTrump #BuildThatWall #BuildThatWall</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>Sweden‚Äôs U-Turn: How Liberals‚Äô refugee policy turned public AGAINST migrants - https://t.co/LopEO2waz8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>@dongsass Not all men youre right Doyoung wouldnt treat me like this !!!!</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  ... AG\n","0   0   ...  1\n","1   1   ...  0\n","2   2   ...  0\n","3   3   ...  0\n","4   4   ...  0\n","5   5   ...  0\n","6   6   ...  0\n","7   7   ...  0\n","8   8   ...  0\n","9   9   ...  0\n","10  10  ...  1\n","11  11  ...  0\n","12  12  ...  0\n","13  13  ...  1\n","14  14  ...  0\n","15  15  ...  0\n","16  16  ...  0\n","17  17  ...  0\n","18  18  ...  1\n","19  19  ...  0\n","20  20  ...  1\n","21  21  ...  0\n","22  22  ...  0\n","23  23  ...  0\n","24  24  ...  0\n","25  25  ...  0\n","26  26  ...  0\n","27  27  ...  0\n","28  28  ...  0\n","29  29  ...  1\n","\n","[30 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"v7jzuGG3dSJq","colab_type":"text"},"source":["# Text Pre-Processing"]},{"cell_type":"markdown","metadata":{"id":"jJrF6Zikp5xS","colab_type":"text"},"source":["### Translating emojis to text"]},{"cell_type":"code","metadata":{"id":"dXj3fJZLlM0q","colab_type":"code","outputId":"61d3fa88-eaf6-4e71-bf0c-2e98a9329c33","executionInfo":{"status":"ok","timestamp":1581511109480,"user_tz":0,"elapsed":92110,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":601}},"source":["#The below function translates emojis to text\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/My Drive'\n","%pip install demoji-0.1.5-py3-none-any.whl\n","%cd ../.. # Reset back to original directory\n","\n","import demoji\n","demoji.download_codes()\n","def emojiReplace(text_string):\n","    \n","    emoji_dict = demoji.findall(text_string)\n","    for emoji in emoji_dict.keys():\n","        text_string = text_string.replace(emoji, ' '+  emoji_dict[emoji])\n","    \n","    return text_string\n","\n","#demoji.replace(preprocess(testtweet1), repl = )\n","testtweet1 = train.loc[4521]\n","testtweet2 = train.loc[4549]\n","\n","print(\"\\nOriginal:\", testtweet1['tweet'])\n","print('Label:', testtweet1['label'])\n","print(\"\\nPreprocessed:\", emojiReplace(testtweet1['tweet']))\n","\n","print(\"\\nOriginal:\", testtweet2['tweet'])\n","print('Label:', testtweet2['label'])\n","print(\"\\nPreprocessed:\", emojiReplace(testtweet2['tweet']))\n","train['tweet'] = train['tweet'].apply(emojiReplace)\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n","Mounted at /content/drive\n","/content/drive/My Drive\n","Processing ./demoji-0.1.5-py3-none-any.whl\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from demoji==0.1.5) (45.1.0)\n","Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from demoji==0.1.5) (2.21.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji==0.1.5) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji==0.1.5) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji==0.1.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji==0.1.5) (2019.11.28)\n","Installing collected packages: demoji\n","Successfully installed demoji-0.1.5\n","[Errno 2] No such file or directory: '../.. # Reset back to original directory'\n","/content/drive/My Drive\n","\u001b[33mDownloading emoji data ...\u001b[0m\n","\u001b[92m... OK\u001b[0m (Got response in 0.11 seconds)\n","\u001b[33mWriting emoji data to /root/.demoji/codes.json ...\u001b[0m\n","\u001b[92m... OK\u001b[0m\n","\n","Original: UK rejects Christian refugees recommended by UN, admits only Muslims among 1,112 Syrians admitted Jan-March 2018 https://t.co/vpvmMFaAnf via @jihadwatchRS\n","Label: 0\n","\n","Preprocessed: UK rejects Christian refugees recommended by UN, admits only Muslims among 1,112 Syrians admitted Jan-March 2018 https://t.co/vpvmMFaAnf via @jihadwatchRS\n","\n","Original: Best gift ever üòò https://t.co/xdZL6RVWLU\n","Label: 0\n","\n","Preprocessed: Best gift ever  face blowing a kiss https://t.co/xdZL6RVWLU\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4HsgTLkE4s1-","colab_type":"text"},"source":["<b>Set after emoji replacement"]},{"cell_type":"code","metadata":{"id":"0mxHLtBt4qIh","colab_type":"code","outputId":"206c1e7d-0657-4a8b-9afa-453dbc8b095d","executionInfo":{"status":"ok","timestamp":1581511109482,"user_tz":0,"elapsed":92091,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train.head(50)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","      <th>TR</th>\n","      <th>AG</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Good Morning to;#MAGA #NoDACA #NoDACAAmnesty #BuildTheWall #LockThemAllUp #AmericaFirst #NotABot RT FOREVER We Love you #POTUS @realDonaldTrump</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>People deprived of the vital family support promised to them by the state, during the hardest time of their lives, thanks to... the state: Home Office delays + this country‚Äôs incoherent, cruel, racist, unworkable bureaucratic mess of an immigration system. https://t.co/awyB9MncSU</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>In New Orleans, .pPresenting results on immigrants and civic engagement project @ilctr @MasonCHSS https://t.co/fHUBLcX2ou</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>@keenondevonte  face blowing a kiss face blowing a kiss down bitch. That‚Äôs what women do. Provide for her man and put his head towards God when he have doubts. I love you  blue heart</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>@LisaBloom It's women like Kathy Griffin that the word cunt is becoming just as popular here as it is in the United Kingdom and Australia.</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>Hole in one and you deserve for it.Congrats https://t.co/EXzIzEXWhM</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>Guys relax. Leave the twitter fights to the professionals. I mean vaginals. I mean women. #WomenSuck</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>Why? Everywhere they go!! WHY?? Act like normal phucking people!!  #Scum #NoMoreRefugees https://t.co/xlitWTPmJ5</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>Credit card boy Ian Duncan Smith and other Politicians should be forced to pay their dues mind u its not tax credits cuts that's for us</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>@FareedFromSyria @noyalevy_ @ElianeAlhussein @CrazyNormie @UN @HasanAlthoffy @UNICEFEMOPS @talentosprecato @Marcnelsonart @KenRoth @AmnestySyria @UOSSM_France Try @msf or @unicefmena. @ICRC_sy and @refugees, can you help or give direction?</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>@GinnyGinny_1 @AngryAmerican78 @melindamrtn @ImaPatriot4life @PTSD_SURVIVOR @nana8_coney @American2b @stand4honor @bindyb123 @RMills08 @mommofour @GinnyWith_A_G @high_rollerx @_timothysmith_ @4LyingLibs Please follow back #Turnout2018 #MAGA #BUILDTHATWALL</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>@pmbairrins It's not hysterical or exaggeration. Abortion is horrendous. There are other ways to help women in crisis</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>Undocumented my a$$, ILLEGAL!! This child is 11 years old.  https://t.co/SfbJRnl1wD</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>Illegal alien rapes 6-year-old girl, police say https://t.co/6dUGD3KAEP #SecureTheBorder #StopTheInvasion #PreventableCrime</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>These are kids, their little minds not made to grasp the diabolic forces at work using them as props. Thank God it's all going to end soon. https://t.co/77EJQOIvK8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>Hear from some of the victims of Algeria's unlawful mass summary deportation of 1000s of migrants, many of whom are being dumped mercilessly in the scorching desert on the Niger border @HRW https://t.co/qXI2FmAN2G https://t.co/utonArNC6Y</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>Glam Skanks -Karma/Bad Bitch Double Feature [Official Video]: https://t.co/hhU4YeYc3u via @YouTube</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>Madonna whore complex, pedaled by the illusion that women somehow embody the honor of the men with whom they‚Äôre ass‚Ä¶ https://t.co/ge9cqQzX80</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>@JoeNBC The cowards are the corrupt establishment. Trump and the American working class will win over the D.C. establishment. #MAGA #Kavanaugh #walkaway #burtreynolds #BuildThatWall @realDonaldTrump @RealJamesWoods @IngrahamAngle @DonaldJTrumpJr @LouDobbs</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>@lobnaashraff I know walahi.. mna lsa kunt basm3ha m3rfsh influenced you wala sudfa</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>Desperate to do whatever it takes at whatever moral or financial cost to help Libyan coastguard &amp;amp; authorities trap even more refugees &amp;amp; migrants in horrific detention centers, Italy revives \"Friendship\" deal signed 10 years ago https://t.co/Eq4ueES8iD @HRW https://t.co/Pi2jPL5meH https://t.co/uU68Jsy5by</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>UN says Libya recovered some 100 bodies of migrants in 2018 https://t.co/uekURE7xfj https://t.co/7W4JUefT9g</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>Friend- any plans for the weekend? Me- https://t.co/ubhL9ABWCz</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>@POTUS @WhiteHouse Only time of the year you look normal ya kunt</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>In America, only 7% of rapists are convicted. It's about power - men want to know they can rape, harass, abuse any woman &amp;amp; get away with it.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>STFU! Like we need your worthless 2 cents of wisdom! Go crawl back under your rock!!!</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>If I was you I'd hate me too</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>BUILD THAT WALL@realDonaldTrump #BuildThatWall #BuildThatWall</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>Sweden‚Äôs U-Turn: How Liberals‚Äô refugee policy turned public AGAINST migrants - https://t.co/LopEO2waz8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>@dongsass Not all men youre right Doyoung wouldnt treat me like this !!!!</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>30</td>\n","      <td>I've pledged to make sure my surgery is a safe space free from the #hostileenvironment. I'm an MP not a border guard https://t.co/X5RPvm5SJY @GlobalJusticeUK @migrantsorg https://t.co/tVioaJm12v</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>31</td>\n","      <td>@taball1 This isn't a 'not all men' bullshit you talking about. This is clearly a 'you mother fuckers' deal. Y'all here is not appropriate.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>32</td>\n","      <td>Just so you know, I'm a righteous Dick Cheney Impersonator, which is not like a thing or anything.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>33</td>\n","      <td>I'll just leave this here.....  https://t.co/WdKVIDFo5p</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>34</td>\n","      <td>STAY THE HELL OUT OF THE WATER. #ARABIANSEA Stupid cunts. #nuclearban #solarban #PSE #MondayMotivation #plasticfree #horses #UN #China #India #Iran #WaterBan #NO #NoDeal #NoDACA #immigrationfraud #Mexico</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>35</td>\n","      <td>Thank you to the @DunhamFund and Dick Ebeling for their support in continuing to inspire students and promote #STEM renovation projects! https://t.co/mq8xWiAc5x</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>36</td>\n","      <td>@RealDonaldTrump Dont Listen To #Democrats Illegal #immigration is NOT a Victimless Crime over 1.5 Million Citizens SSN used by Illegals according to the IRS they need to be turned over To  @ICEgov and Deported #Trump #MAGA #RedNationRising @AVIACUSA http</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>37</td>\n","      <td>* Hungary √∞≈∏‚Ä°¬≠√∞≈∏‚Ä°¬∫:  Leading security expert J√É¬≥zsef Horv√É¬°th revealed that between 85-90% of migrants are not ‚Äúrefugees‚Äù seeking genuine help but economic migrants. https://t.co/hccQmsQ9Vl #v4 #visegrad https://t.co/1pPCNwi8yD</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>38</td>\n","      <td>What is important to you now?  I'm sad for those unfortunate immigrant children who have yet to be reunited with their families.  Mental health ought to be as important as physical health.   #SaveSCOTUS #BlueWave2018 #mentalhealth #health #FamiliesBelongTogether #VoteBlue #Vote</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>39</td>\n","      <td>Med not a house/EU not adult day care camp for migrants seeking handouts or to profit from crime.Tunisia abusing #Italy EU, which has leverage bt lacks courage to use it. #Salvini must send message√∞≈∏‚Äò‚Ä∞either stop boats &amp;amp; take back citizens or we bury you economically, esp. leaders https://t.co/FYEM1x7RWJ</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>40</td>\n","      <td>@Complex Ok people say Em aint good anymore, yet this shit gets a pass? So much content in this..youre such a fucking hoe, I love it...suck my dick bitch wow...groundbreaking material looking like Roblox characters #pass</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>41</td>\n","      <td>@PatriotLexi Actually that is any benefits which would make a Immigrant a \"Public Charge\" law on the books for over 100 years not entitled to hold a green card or become a citizen but #Dems wont enforce it !  That is why 73% of  Mexican and Central Amer</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>42</td>\n","      <td>Owning land and buildings where immigrant detention centers operate is a business worth billions of dollars https://t.co/zk3O04fv0R @ExpressNews #Texas #txlege #Immigration https://t.co/gXmhTLwSsL</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>43</td>\n","      <td>Venezuelan migrants bring Trinidad's flawed refugee policy to light https://t.co/DbMjgfcSsh</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>44</td>\n","      <td>@gabegutierrez @mitchellreports There is only oneway for illigal refugees - go home quick.</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>45</td>\n","      <td>SHOVE IT DEEP IN YOUR WILLING KUNT PULLUNG OUT N B... ‚Äî cunt* https://t.co/ju24EIixMg</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>46</td>\n","      <td>Nigerian ‚ÄòÀúImmigrant‚Äô Doctor Sexually Assaulted Several Patients In Texas https://t.co/NdgWXlwfpp      Scum update...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>47</td>\n","      <td>What do women and condoms have in common? If they're not on your dick they're in your wallet</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>48</td>\n","      <td>@ABCPolitics A illegal aliens showing exactly what they were coming to this country for in the first place. #sendthemback!!!!!</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>49</td>\n","      <td>@ablington Fits with the old paradigm of women in our culture as The Madonna Or The Whore, particularly in popular culture.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  ... AG\n","0   0   ...  1\n","1   1   ...  0\n","2   2   ...  0\n","3   3   ...  0\n","4   4   ...  0\n","5   5   ...  0\n","6   6   ...  0\n","7   7   ...  0\n","8   8   ...  0\n","9   9   ...  0\n","10  10  ...  1\n","11  11  ...  0\n","12  12  ...  0\n","13  13  ...  1\n","14  14  ...  0\n","15  15  ...  0\n","16  16  ...  0\n","17  17  ...  0\n","18  18  ...  1\n","19  19  ...  0\n","20  20  ...  1\n","21  21  ...  0\n","22  22  ...  0\n","23  23  ...  0\n","24  24  ...  0\n","25  25  ...  0\n","26  26  ...  0\n","27  27  ...  0\n","28  28  ...  0\n","29  29  ...  1\n","30  30  ...  0\n","31  31  ...  0\n","32  32  ...  0\n","33  33  ...  0\n","34  34  ...  1\n","35  35  ...  0\n","36  36  ...  0\n","37  37  ...  0\n","38  38  ...  0\n","39  39  ...  0\n","40  40  ...  1\n","41  41  ...  0\n","42  42  ...  0\n","43  43  ...  0\n","44  44  ...  1\n","45  45  ...  0\n","46  46  ...  0\n","47  47  ...  0\n","48  48  ...  0\n","49  49  ...  0\n","\n","[50 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"7s6EmqqujjBh","colab_type":"text"},"source":["### Contraction Mapping"]},{"cell_type":"markdown","metadata":{"id":"7cd_P1rZH3tb","colab_type":"text"},"source":["The contraction mapping below is not perfect. There are many ambigious contractions which are impossible to definitively resolve (e.g. he's - he has or he is).\n","\n","Although this contractions library comes highly rated, and does seem to work often"]},{"cell_type":"code","metadata":{"id":"ZAoLqorwH3bE","colab_type":"code","outputId":"5ce4b6d2-c7e5-4846-8805-91b5aa8c9a75","executionInfo":{"status":"ok","timestamp":1581511115731,"user_tz":0,"elapsed":98322,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":401}},"source":["!pip install contractions\n","import contractions\n","tweet = train.iloc[2312]['tweet']\n","print(\"\\nOriginal: \", tweet)\n","print(\"\\nReplaced Contractions: \", contractions.fix(tweet))\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Collecting contractions\n","  Downloading https://files.pythonhosted.org/packages/85/41/c3dfd5feb91a8d587ed1a59f553f07c05f95ad4e5d00ab78702fbf8fe48a/contractions-0.0.24-py2.py3-none-any.whl\n","Collecting textsearch\n","  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n","Collecting Unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 245kB 4.9MB/s \n","\u001b[?25hCollecting pyahocorasick\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 317kB 60.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n","  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81708 sha256=191d0fa9891a636e3249bfbdf6aa6c7ee70d481a74da05e385d4bcdf4b38387a\n","  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n","Successfully built pyahocorasick\n","Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n","Successfully installed Unidecode-1.1.1 contractions-0.0.24 pyahocorasick-1.4.0 textsearch-0.0.17\n","Original:  Do you guys accept your significant other calling you out of your name! i.e bitch hoe slut etc.... And I‚Äôm not talking about during sex.  thinking face\n","\n","Replaced Contractions:  Do you guys accept your significant other calling you out of your name! i.e bitch hoe slut etc.... And I am not talking about during sex.  thinking face\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3SE6vdzYUoiA","colab_type":"text"},"source":["### Custom tweet text pre-processing\n","Below is our custom preprocess function which performs simple text preprocessing of the tweets. It's functions are explained in the method.\n","\n","One may use the contraction mapping in the code above to convert contractions to their full-word form. You can just uncomment the line at the bottom of the function to do so\n","\n","We always use this method when preprocessing our tweets as it gives us the most basic text preprocessing and it's needed"]},{"cell_type":"code","metadata":{"id":"U7Lh1I1qdR20","colab_type":"code","colab":{}},"source":["import string\n","\n","def preprocess(text_string):\n","    \"\"\"\n","    Accepts a text string and:\n","    1) Removes URLS\n","    2) lots of whitespace with one instance\n","    3) Removes mentions\n","    4) Uses the html.unescape() method to convert unicode to text counterpart\n","    5) Replace & with and\n","    6) Remove the fact the tweet is a retweet if it is - knowing the tweet is \n","       a retweet does not help towards our classification task.\n","    This allows us to get standardized counts of urls and mentions\n","    Without caring about specific people mentioned\n","    \"\"\"\n","    space_pattern = '\\s+'\n","    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[#$-_@.&+]|'\n","        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n","    mention_regex = '@[\\w\\-]+:'\n","    mention_regex1 = '@[\\w\\-]+'\n","    RT_regex = '(RT|rt)[ ]*@[ ]*[\\S]+'\n","    \n","    # Replaces urls with URL\n","    parsed_text = re.sub(giant_url_regex, '', text_string)\n","    parsed_text = re.sub('URL', '', parsed_text)\n","    \n","    # Remove the fact the tweet is a retweet. \n","    # (we're only interested in the language of the tweet here)\n","    parsed_text = re.sub(RT_regex, ' ', parsed_text) \n","    \n","    # Removes mentions as they're redundant information\n","    parsed_text = re.sub(mention_regex, '',  parsed_text)\n","    #including mentions with colons after - this seems to come up often\n","    parsed_text = re.sub(mention_regex1, '',  parsed_text)  \n","\n","    #Replace &amp; with and\n","    parsed_text = re.sub('&amp;', 'and', parsed_text)\n","\n","    # Remove unicode\n","    parsed_text = re.sub(r'[^\\x00-\\x7F]','', parsed_text) \n","    parsed_text = re.sub(r'&#[0-9]+;', '', parsed_text)  \n","\n","    # Convert unicode missed by above regex to text\n","    parsed_text = html.unescape(parsed_text)\n","    \n","    # Remove excess whitespace at the end\n","    parsed_text = re.sub(space_pattern, ' ', parsed_text) \n","    \n","    # Set text to lowercase and strip\n","    parsed_text = parsed_text.lower()\n","    parsed_text = parsed_text.strip()\n","\n","    # Replace contractions with their full worded form\n","    parsed_text = contractions.fix(parsed_text)\n","    \n","    return parsed_text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pVOEaAtad7Nh","colab_type":"text"},"source":["**Let's see what it looks like**"]},{"cell_type":"code","metadata":{"id":"gzRWRFtHeAcp","colab_type":"code","outputId":"332dfb50-83b8-4a29-a2cb-8d64378a7a10","executionInfo":{"status":"ok","timestamp":1581511115733,"user_tz":0,"elapsed":98297,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":237}},"source":["testtweet = train.loc[2100]\n","print(\"Original:\", testtweet['tweet'])\n","print(\"Preprocessed:\", preprocess(testtweet['tweet']))\n","print('Label:', testtweet['label'])\n","\n","testtweet1 = train.loc[4521]\n","print(\"\\nOriginal:\", testtweet1['tweet'])\n","print(\"Preprocessed:\", preprocess(testtweet1['tweet']))\n","print('Label:', testtweet1['label'])\n","\n","testtweet2 = train.loc[4549]\n","print(\"\\nOriginal:\", testtweet2['tweet'])\n","print(\"Preprocessed:\", preprocess(testtweet2['tweet']))\n","print('Label:', testtweet2['label'])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Original: @ABCPolitics This bitch will be rape by Trump too and then denie it.\n","Preprocessed: this bitch will be rape by trump too and then denie it.\n","Label: 1\n","\n","Original: UK rejects Christian refugees recommended by UN, admits only Muslims among 1,112 Syrians admitted Jan-March 2018 https://t.co/vpvmMFaAnf via @jihadwatchRS\n","Preprocessed: uk rejects christian refugees recommended by un, admits only muslims among 1,112 syrians admitted jan-march 2018 via\n","Label: 0\n","\n","Original: Best gift ever  face blowing a kiss https://t.co/xdZL6RVWLU\n","Preprocessed: best gift ever face blowing a kiss\n","Label: 0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C1BZjsA45AXT","colab_type":"text"},"source":["<b> Training set after basic preprocessing"]},{"cell_type":"code","metadata":{"id":"Nlo4jJ7l5GhJ","colab_type":"code","outputId":"174ae129-53b2-4117-aebe-365427ecf57e","executionInfo":{"status":"ok","timestamp":1581511116201,"user_tz":0,"elapsed":98747,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train['tweet'] = train['tweet'].apply(preprocess)\n","train.head(30)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","      <th>TR</th>\n","      <th>AG</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>good morning to;#maga #nodaca #nodacaamnesty #buildthewall #lockthemallup #americafirst #notabot rt forever we love you #potus</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>people deprived of the vital family support promised to them by the state, during the hardest time of their lives, thanks to... the state: home office delays + this countrys incoherent, cruel, racist, unworkable bureaucratic mess of an immigration system.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>in new orleans, .ppresenting results on immigrants and civic engagement project</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>face blowing a kiss face blowing a kiss down bitch. that is what women do. provide for her man and put his head towards god when he have doubts. i love you blue heart</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>it is women like kathy griffin that the word cunt is becoming just as popular here as it is in the united kingdom and australia.</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>hole in one and you deserve for it.congrats</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>guys relax. leave the twitter fights to the professionals. i mean vaginals. i mean women. #womensuck</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>why? everywhere they go!! why?? act like normal phucking people!! #scum #nomorerefugees</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>credit card boy ian duncan smith and other politicians should be forced to pay their dues mind you its not tax credits cuts that is for us</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>try or . and , can you help or give direction?</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>please follow back #turnout2018 #maga #buildthatwall</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>it is not hysterical or exaggeration. abortion is horrendous. there are other ways to help women in crisis</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>undocumented my a$$, illegal!! this child is 11 years old.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>illegal alien rapes 6-year-old girl, police say #securetheborder #stoptheinvasion #preventablecrime</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>these are kids, their little minds not made to grasp the diabolic forces at work using them as props. thank god it is all going to end soon.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>hear from some of the victims of algeria's unlawful mass summary deportation of 1000s of migrants, many of whom are being dumped mercilessly in the scorching desert on the niger border</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>glam skanks -karma/bad bitch double feature [official video]: via</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>madonna whore complex, pedaled by the illusion that women somehow embody the honor of the men with whom they are ass</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>the cowards are the corrupt establishment. trump and the american working class will win over the d.c. establishment. #maga #kavanaugh #walkaway #burtreynolds #buildthatwall</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>i know walahi.. mna lsa kunt basm3ha m3rfsh influenced you wala sudfa</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>desperate to do whatever it takes at whatever moral or financial cost to help libyan coastguard and authorities trap even more refugees and migrants in horrific detention centers, italy revives \"friendship\" deal signed 10 years ago</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>un says libya recovered some 100 bodies of migrants in 2018</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>friend- any plans for the weekend? me-</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>only time of the year you look normal ya kunt</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>in america, only 7% of rapists are convicted. it is about power - men want to know they can rape, harass, abuse any woman and get away with it.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>stfu! like we need your worthless 2 cents of wisdom! go crawl back under your rock!!!</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>if i was you I would hate me too</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>build that wall #buildthatwall #buildthatwall</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>swedens you-turn: how liberals refugee policy turned public against migrants -</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>not all men you are right doyoung would not treat me like this !!!!</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  ... AG\n","0   0   ...  1\n","1   1   ...  0\n","2   2   ...  0\n","3   3   ...  0\n","4   4   ...  0\n","5   5   ...  0\n","6   6   ...  0\n","7   7   ...  0\n","8   8   ...  0\n","9   9   ...  0\n","10  10  ...  1\n","11  11  ...  0\n","12  12  ...  0\n","13  13  ...  1\n","14  14  ...  0\n","15  15  ...  0\n","16  16  ...  0\n","17  17  ...  0\n","18  18  ...  1\n","19  19  ...  0\n","20  20  ...  1\n","21  21  ...  0\n","22  22  ...  0\n","23  23  ...  0\n","24  24  ...  0\n","25  25  ...  0\n","26  26  ...  0\n","27  27  ...  0\n","28  28  ...  0\n","29  29  ...  1\n","\n","[30 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"vMi4f72qVlQG","colab_type":"text"},"source":["### Hashtag Segmentation"]},{"cell_type":"code","metadata":{"id":"zj1DxQmVVmK8","colab_type":"code","outputId":"9f8d445a-0adf-46c1-af67-7da5a749dd4d","executionInfo":{"status":"ok","timestamp":1581511131309,"user_tz":0,"elapsed":113829,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":383}},"source":["!pip install wordsegment\n","import wordsegment as ws\n","from wordsegment import load, segment\n","\n","load()\n","#The values below of the bigrams reflect the amount of search results on google that come up\n","ws.BIGRAMS['alt right'] = 1.17e8 # update wordsegment dict so it recognises altright as \"alt right\" rather than salt right\n","ws.BIGRAMS['white supremacists'] = 3.86e6\n","ws.BIGRAMS['tweets'] = 6.26e10\n","\n","!pip install wordsegment\n","import wordsegment as ws\n","from wordsegment import load, segment\n","\n","load()\n","#The values below of the bigrams reflect the amount of search results on google that come up\n","ws.BIGRAMS['alt right'] = 1.17e8 # update wordsegment dict so \n","                                #it recognises altright as \"alt right\" rather than salt right\n","ws.BIGRAMS['white supremacists'] = 3.86e6\n","ws.BIGRAMS['tweets'] = 6.26e10\n","ws.BIGRAMS['independece day'] = 6.21e7\n","\n","def hashtagSegment(text_string):\n","    \n","    #We target hashtags so that we only segment the hashtag strings.\n","    #Otherwise the segment function may operate on misspelled words also; which\n","    #often appear in hate speech tweets owing to the ill education of those spewing it\n","    temp_str = []\n","    for word in text_string.split(' '):\n","        if word.startswith('#') == False:\n","            temp_str.append(word)\n","        else:\n","            temp_str = temp_str + segment(word)\n","            \n","    text_string = ' '.join(temp_str)       \n","    return text_string\n","\n","teststr = train.iloc[1291]['tweet']\n","teststr1 = train.iloc[3892]['tweet']\n","\n","print('\\nNormal:\\n',teststr,'\\n')\n","print(\"Hashtag-Segmented:\\n\", hashtagSegment(teststr))\n","\n","print('\\n\\nNormal:\\n', teststr1,'\\n')\n","print(\"Hashtag-Segmented:\\n\", hashtagSegment(teststr1))\n","\n","train['tweet'] = train['tweet'].apply(hashtagSegment)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Collecting wordsegment\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/6c/e6f4734d6f7d28305f52ec81377d7ce7d1856b97b814278e9960183235ad/wordsegment-1.3.1-py2.py3-none-any.whl (4.8MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.8MB 3.3MB/s \n","\u001b[?25hInstalling collected packages: wordsegment\n","Successfully installed wordsegment-1.3.1\n","Requirement already satisfied: wordsegment in /usr/local/lib/python3.6/dist-packages (1.3.1)\n","\n","Normal:\n"," the queen of hysterical women tweets bitterly, terrified of losing her crown. \n","\n","Hashtag-Segmented:\n"," the queen of hysterical women tweets bitterly, terrified of losing her crown.\n","\n","\n","Normal:\n"," bad girls get spankings \n","\n","Hashtag-Segmented:\n"," bad girls get spankings\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zEYHxW4l5QIt","colab_type":"text"},"source":["<b>Training set aftter Hashtag segmentation</b>"]},{"cell_type":"code","metadata":{"id":"4M_5CKIVW6Ay","colab_type":"code","outputId":"fb3330fb-0e4c-4d8b-e64d-cb81616f8420","executionInfo":{"status":"ok","timestamp":1581511131311,"user_tz":0,"elapsed":113811,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train.head(50)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","      <th>TR</th>\n","      <th>AG</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>good morning to;#maga no daca no daca amnesty build the wall lock them all up america first not a bot rt forever we love you potus</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>people deprived of the vital family support promised to them by the state, during the hardest time of their lives, thanks to... the state: home office delays + this countrys incoherent, cruel, racist, unworkable bureaucratic mess of an immigration system.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>in new orleans, .ppresenting results on immigrants and civic engagement project</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>face blowing a kiss face blowing a kiss down bitch. that is what women do. provide for her man and put his head towards god when he have doubts. i love you blue heart</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>it is women like kathy griffin that the word cunt is becoming just as popular here as it is in the united kingdom and australia.</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>hole in one and you deserve for it.congrats</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>guys relax. leave the twitter fights to the professionals. i mean vaginals. i mean women. women suck</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>why? everywhere they go!! why?? act like normal phucking people!! scum no more refugees</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>credit card boy ian duncan smith and other politicians should be forced to pay their dues mind you its not tax credits cuts that is for us</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>try or . and , can you help or give direction?</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>please follow back turn out 2018 maga build that wall</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>it is not hysterical or exaggeration. abortion is horrendous. there are other ways to help women in crisis</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>undocumented my a$$, illegal!! this child is 11 years old.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>illegal alien rapes 6-year-old girl, police say secure the border stop the invasion preventable crime</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>these are kids, their little minds not made to grasp the diabolic forces at work using them as props. thank god it is all going to end soon.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>hear from some of the victims of algeria's unlawful mass summary deportation of 1000s of migrants, many of whom are being dumped mercilessly in the scorching desert on the niger border</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>glam skanks -karma/bad bitch double feature [official video]: via</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>madonna whore complex, pedaled by the illusion that women somehow embody the honor of the men with whom they are ass</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>the cowards are the corrupt establishment. trump and the american working class will win over the d.c. establishment. maga kavanaugh walk away burt reynolds build that wall</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>i know walahi.. mna lsa kunt basm3ha m3rfsh influenced you wala sudfa</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>desperate to do whatever it takes at whatever moral or financial cost to help libyan coastguard and authorities trap even more refugees and migrants in horrific detention centers, italy revives \"friendship\" deal signed 10 years ago</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>un says libya recovered some 100 bodies of migrants in 2018</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>friend- any plans for the weekend? me-</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>only time of the year you look normal ya kunt</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>in america, only 7% of rapists are convicted. it is about power - men want to know they can rape, harass, abuse any woman and get away with it.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>stfu! like we need your worthless 2 cents of wisdom! go crawl back under your rock!!!</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>if i was you I would hate me too</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>build that wall build that wall build that wall</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>swedens you-turn: how liberals refugee policy turned public against migrants -</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>not all men you are right doyoung would not treat me like this !!!!</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>30</td>\n","      <td>I have pledged to make sure my surgery is a safe space free from the hostile environment I am an mp not a border guard</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>31</td>\n","      <td>this is not a 'not all men' bullshit you talking about. this is clearly a 'you mother fuckers' deal. you all here is not appropriate.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>32</td>\n","      <td>just so you know, I am a righteous dick cheney impersonator, which is not like a thing or anything.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>33</td>\n","      <td>I will just leave this here.....</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>34</td>\n","      <td>stay the hell out of the water. arabian sea stupid cunts. nuclear ban solar ban pse monday motivation plastic free horses un china india iran water ban no no deal no daca immigration fraud mexico</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>35</td>\n","      <td>thank you to the and dick ebeling for their support in continuing to inspire students and promote stem renovation projects!</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>36</td>\n","      <td>do not listen to democrats illegal immigration is not a victimless crime over 1.5 million citizens ssn used by illegals according to the irs they need to be turned over to and deported trump maga red nation rising http</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>37</td>\n","      <td>* hungary : leading security expert jzsef horvth revealed that between 85-90% of migrants are not refugees seeking genuine help but economic migrants. v4 visegrad</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>38</td>\n","      <td>what is important to you now? I am sad for those unfortunate immigrant children who have yet to be reunited with their families. mental health ought to be as important as physical health. save scotus bluewave 2018 mental health health families belong together vote blue vote</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>39</td>\n","      <td>med not a house/eu not adult day care camp for migrants seeking handouts or to profit from crime.tunisia abusing italy eu, which has leverage bt lacks courage to use it. salvini must send messageeither stop boats and take back citizens or we bury you economically, esp. leaders</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>40</td>\n","      <td>ok people say em are not good anymore, yet this shit gets a pass? so much content in this..you are such a fucking hoe, i love it...suck my dick bitch wow...groundbreaking material looking like roblox characters pass</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>41</td>\n","      <td>actually that is any benefits which would make a immigrant a \"public charge\" law on the books for over 100 years not entitled to hold a green card or become a citizen but dems will not enforce it ! that is why 73% of mexican and central amer</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>42</td>\n","      <td>owning land and buildings where immigrant detention centers operate is a business worth billions of dollars texas tx lege immigration</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>43</td>\n","      <td>venezuelan migrants bring trinidad's flawed refugee policy to light</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>44</td>\n","      <td>there is only oneway for illigal refugees - go home quick.</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>45</td>\n","      <td>shove it deep in your willing kunt pullung out n b... cunt*</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>46</td>\n","      <td>nigerian immigrant doctor sexually assaulted several patients in texas scum update...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>47</td>\n","      <td>what do women and condoms have in common? if they are not on your dick they are in your wallet</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>48</td>\n","      <td>a illegal aliens showing exactly what they were coming to this country for in the first place. send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>49</td>\n","      <td>fits with the old paradigm of women in our culture as the madonna or the whore, particularly in popular culture.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  ... AG\n","0   0   ...  1\n","1   1   ...  0\n","2   2   ...  0\n","3   3   ...  0\n","4   4   ...  0\n","5   5   ...  0\n","6   6   ...  0\n","7   7   ...  0\n","8   8   ...  0\n","9   9   ...  0\n","10  10  ...  1\n","11  11  ...  0\n","12  12  ...  0\n","13  13  ...  1\n","14  14  ...  0\n","15  15  ...  0\n","16  16  ...  0\n","17  17  ...  0\n","18  18  ...  1\n","19  19  ...  0\n","20  20  ...  1\n","21  21  ...  0\n","22  22  ...  0\n","23  23  ...  0\n","24  24  ...  0\n","25  25  ...  0\n","26  26  ...  0\n","27  27  ...  0\n","28  28  ...  0\n","29  29  ...  1\n","30  30  ...  0\n","31  31  ...  0\n","32  32  ...  0\n","33  33  ...  0\n","34  34  ...  1\n","35  35  ...  0\n","36  36  ...  0\n","37  37  ...  0\n","38  38  ...  0\n","39  39  ...  0\n","40  40  ...  1\n","41  41  ...  0\n","42  42  ...  0\n","43  43  ...  0\n","44  44  ...  1\n","45  45  ...  0\n","46  46  ...  0\n","47  47  ...  0\n","48  48  ...  0\n","49  49  ...  0\n","\n","[50 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"GIfayevxR_Ie","colab_type":"text"},"source":["<b>Removing stopwords</b>"]},{"cell_type":"code","metadata":{"id":"aUb2bs7rSF_B","colab_type":"code","outputId":"c8f5d2de-0caa-49ab-a6ea-00c143bd33a8","executionInfo":{"status":"ok","timestamp":1581511131312,"user_tz":0,"elapsed":113790,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"import nltk\n","stopwords = nltk.corpus.stopwords.words('english')\n","\n","def remove_stopwords(tokenized_list):\n","    text = [word for word in tokenized_list if word not in stopwords]\n","    return text\n","\n","train['text'] = train['text'].apply(lambda x: remove_stopwords(x))\n","\n","train.head(10)\"\"\""],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"import nltk\\nstopwords = nltk.corpus.stopwords.words('english')\\n\\ndef remove_stopwords(tokenized_list):\\n    text = [word for word in tokenized_list if word not in stopwords]\\n    return text\\n\\ntrain['text'] = train['text'].apply(lambda x: remove_stopwords(x))\\n\\ntrain.head(10)\""]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"6nbscFHKG-38","colab_type":"text"},"source":["<b> Lemmatizing text </b>"]},{"cell_type":"code","metadata":{"id":"RwqtAT10G9u5","colab_type":"code","outputId":"7bef971c-2cfd-4eb5-8853-2d59621436c9","executionInfo":{"status":"ok","timestamp":1581511131313,"user_tz":0,"elapsed":113772,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"import nltk\n","wn = nltk.WordNetLemmatizer()\n","def lemmatizing(tokenized_text):\n","    text = [wn.lemmatize(word) for word in tokenized_text]\n","    return text\n","\n","train['text'] = train['text'].apply( lambda x: lemmatizing(x))\n","data.head(10)\"\"\""],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"import nltk\\nwn = nltk.WordNetLemmatizer()\\ndef lemmatizing(tokenized_text):\\n    text = [wn.lemmatize(word) for word in tokenized_text]\\n    return text\\n\\ntrain['text'] = train['text'].apply( lambda x: lemmatizing(x))\\ndata.head(10)\""]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"BJhBOJp_xlDm","colab_type":"text"},"source":["**Removing Punctuation**"]},{"cell_type":"code","metadata":{"id":"aoZIR9IZxkx-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"f8da2673-7311-43d1-a38b-8e7bdb6c6fe0","executionInfo":{"status":"ok","timestamp":1581511131314,"user_tz":0,"elapsed":113767,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}}},"source":["\"\"\"\n","def remove_punct(text):\n","    \n","    #Return the charater as long as it's not punctuation\n","    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n","    return text_nopunct\n","\n","train['text'] = train['text'].apply(lambda x: remove_punct(x))\n","\"\"\""],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef remove_punct(text):\\n    \\n    #Return the charater as long as it\\'s not punctuation\\n    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\\n    return text_nopunct\\n\\ntrain[\\'text\\'] = train[\\'text\\'].apply(lambda x: remove_punct(x))\\n'"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"UhtlxArU2-yp","colab_type":"text"},"source":[" **In training, let's remove any tweets that have a length less than 10. They could skew our model**"]},{"cell_type":"code","metadata":{"id":"CVjM8yv8299W","colab_type":"code","outputId":"53ca6fee-8220-4b9d-f553-6452ba48f96c","executionInfo":{"status":"ok","timestamp":1581511131314,"user_tz":0,"elapsed":113747,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["length = len(train.index)\n","train = train[train['tweet'].apply(lambda x: len(x) > 10)]\n","#train = train[train['tweet'].apply(lambda x: len(x) < 300)]\n","print(length - len(train.index), \"tweets have been removed from the dataframe\\n\")\n","train.reset_index(drop = True, inplace = True)\n","train.id = train.index\n","train.info()"],"execution_count":20,"outputs":[{"output_type":"stream","text":["29 tweets have been removed from the dataframe\n","\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 8971 entries, 0 to 8970\n","Data columns (total 5 columns):\n","id       8971 non-null int64\n","tweet    8971 non-null object\n","label    8971 non-null int64\n","TR       8971 non-null int64\n","AG       8971 non-null int64\n","dtypes: int64(4), object(1)\n","memory usage: 350.6+ KB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6uMuDR0pqChY","colab_type":"text"},"source":["# Splitting data into train and dev. Also specifying label and text columns\n","\n","We store the name of the Data column containing the text we wish to classify and the name of the corresponding label column in global variables for ease of access down line and also so this code is generalizable. Label list is just a 0 or a 1 because the version of BERT we've created below only deals in binary classifcation and labels must be ints"]},{"cell_type":"code","metadata":{"id":"IuMOGwFui4it","colab_type":"code","outputId":"3fda4143-473b-4458-fb83-38599ccae71c","executionInfo":{"status":"ok","timestamp":1581511133841,"user_tz":0,"elapsed":116251,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["dev =  pd.read_csv('gs://csc3002/hateval2019/hateval2019_en_dev.csv', sep=',',  index_col = False, encoding = 'utf-8')\n","dev.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","\n","# Uncomment any you wish\n","dev['tweet'] = dev['tweet'].apply(emojiReplace) \n","dev['tweet'] = dev['tweet'].apply(preprocess)\n","dev['tweet'] = dev['tweet'].apply(hashtagSegment) \n","\n","DATA_COLUMN = 'tweet'\n","LABEL_COLUMN = 'label'\n","# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = [0, 1]\n","\n","print(\"Size of training data\", len(train.index))\n","print(\"Size of development data\", len(dev.index))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Size of training data 8971\n","Size of development data 1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V399W0rqNJ-Z","colab_type":"text"},"source":["#Data Preprocessing\n","We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n","\n","- `text_a` is the text we want to classify, which in this case, is the `tweet` field in our Dataframe. \n","- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n","- `label` is the label for our example, i.e. True, False"]},{"cell_type":"code","metadata":{"id":"p9gEt5SmM6i6","colab_type":"code","colab":{}},"source":["# Use the InputExample class from BERT's run_classifier code to create examples from the data\n","train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)\n","\n","dev_InputExamples = dev.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCZWZtKxObjh","colab_type":"text"},"source":["Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n","\n","\n","1. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n","2. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n","3. Map our words to indexes using a vocab file that BERT provides\n","4. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n","5. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n","\n","Happily, we don't have to worry about most of these details. It's automated with the below inbuilt functions\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Aws4Q_SXZENG","colab_type":"text"},"source":["Below is a way to retrieve desired BERT parameters, such as it's pre-trained checkpoints and it's vocab file, from my google storage bucket where I've downloaded the uncased LARGE version of bert."]},{"cell_type":"code","metadata":{"id":"UtZavIhEaWF5","colab_type":"code","outputId":"d38d689f-e1f8-4f6c-c36e-465cd4cbd795","executionInfo":{"status":"ok","timestamp":1581511135841,"user_tz":0,"elapsed":118232,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":110}},"source":["bucket_dir = 'gs://csc3002'\n","\n","bert_ckpt_dir = os.path.join(bucket_dir, bert_model_name) \n","\n","#For normal model\n","#bert_ckpt_file   = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n","#print(\"Using BERT checkpoint from:\", bert_ckpt_dir)\n","\n","#For further pretrained model\n","#bert_ckpt_file   = os.path.join(further_pretrained_model, \"bert_model.ckpt\")\n","bert_ckpt_file = tf.train.latest_checkpoint('gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model')\n","print(\"Using BERT checkpoint from:\", further_pretrained_model)\n","\n","bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")\n","vocab_file = os.path.join(bert_ckpt_dir, \"vocab.txt\")\n","\n","\n","tokenizer = bert.tokenization.FullTokenizer(vocab_file=vocab_file)\n","tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")\n","\n","print(\"Make sure that the function loads a checkpoint\")\n","assert bert_ckpt_file is not None, \"No BERT checkpoint file loaded\""],"execution_count":23,"outputs":[{"output_type":"stream","text":["Using BERT checkpoint from: wwm_uncased_L-24_H-1024_A-16/further_pretrained_model\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","Make sure that the function loads a checkpoint\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0OEzfFIt6GIc","colab_type":"text"},"source":["Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cm8RLoJ31WLa","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d85ffe3c-69cb-4640-bbc6-1e28494b810a","executionInfo":{"status":"ok","timestamp":1581511140542,"user_tz":0,"elapsed":122929,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}}},"source":["# BERT is limited to 512 tokens in length\n","MAX_SEQ_LENGTH = 256\n","# Convert our train and dev features to InputFeatures that BERT understands.\n","train_features =  bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","dev_features = bert.run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n","INFO:tensorflow:Writing example 0 of 8971\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] good morning to ; # mag ##a no da ##ca no da ##ca amnesty build the wall lock them all up america first not a bot rt forever we love you pot ##us [SEP]\n","INFO:tensorflow:input_ids: 101 2204 2851 2000 1025 1001 23848 2050 2053 4830 3540 2053 4830 3540 16154 3857 1996 2813 5843 2068 2035 2039 2637 2034 2025 1037 28516 19387 5091 2057 2293 2017 8962 2271 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] people deprived of the vital family support promised to them by the state , during the hardest time of their lives , thanks to . . . the state : home office delays + this country ##s inc ##oh ##ere ##nt , cruel , racist , un ##work ##able bureau ##cratic mess of an immigration system . [SEP]\n","INFO:tensorflow:input_ids: 101 2111 17676 1997 1996 8995 2155 2490 5763 2000 2068 2011 1996 2110 1010 2076 1996 18263 2051 1997 2037 3268 1010 4283 2000 1012 1012 1012 1996 2110 1024 2188 2436 14350 1009 2023 2406 2015 4297 11631 7869 3372 1010 10311 1010 16939 1010 4895 6198 3085 4879 17510 6752 1997 2019 7521 2291 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] in new orleans , . pp ##res ##enting results on immigrants and civic engagement project [SEP]\n","INFO:tensorflow:input_ids: 101 1999 2047 5979 1010 1012 4903 6072 26951 3463 2006 7489 1998 8388 8147 2622 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] face blowing a kiss face blowing a kiss down bitch . that is what women do . provide for her man and put his head towards god when he have doubts . i love you blue heart [SEP]\n","INFO:tensorflow:input_ids: 101 2227 11221 1037 3610 2227 11221 1037 3610 2091 7743 1012 2008 2003 2054 2308 2079 1012 3073 2005 2014 2158 1998 2404 2010 2132 2875 2643 2043 2002 2031 13579 1012 1045 2293 2017 2630 2540 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] it is women like kathy griffin that the word cu ##nt is becoming just as popular here as it is in the united kingdom and australia . [SEP]\n","INFO:tensorflow:input_ids: 101 2009 2003 2308 2066 14986 9258 2008 1996 2773 12731 3372 2003 3352 2074 2004 2759 2182 2004 2009 2003 1999 1996 2142 2983 1998 2660 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:Writing example 0 of 1000\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] i swear i am getting to places just in the nick of time ! its exhaust ##ing made your bed , now lie in the flea pit you created ' tourists go home , refugees welcome ' : why barcelona chose migrants over visitors [SEP]\n","INFO:tensorflow:input_ids: 101 1045 8415 1045 2572 2893 2000 3182 2074 1999 1996 4172 1997 2051 999 2049 15095 2075 2081 2115 2793 1010 2085 4682 1999 1996 26735 6770 2017 2580 1005 9045 2175 2188 1010 8711 6160 1005 1024 2339 7623 4900 16836 2058 5731 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] i am an immigrant and trump is right on immigration - , can you give you insights on this story ? [SEP]\n","INFO:tensorflow:input_ids: 101 1045 2572 2019 11560 1998 8398 2003 2157 2006 7521 1011 1010 2064 2017 2507 2017 20062 2006 2023 2466 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] illegal immigrants illegal aliens electoral system electoral college i am going to shock some people here : america is not a democracy america is a republic even more defined america is a representative republic . in a true . . . [SEP]\n","INFO:tensorflow:input_ids: 101 6206 7489 6206 12114 6092 2291 6092 2267 1045 2572 2183 2000 5213 2070 2111 2182 1024 2637 2003 2025 1037 7072 2637 2003 1037 3072 2130 2062 4225 2637 2003 1037 4387 3072 1012 1999 1037 2995 1012 1012 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] we have our own invasion issues with mexican ##s . build that wall [SEP]\n","INFO:tensorflow:input_ids: 101 2057 2031 2256 2219 5274 3314 2007 4916 2015 1012 3857 2008 2813 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] worker charged with sexually mole ##sti ##ng eight children at immigrant shelter [SEP]\n","INFO:tensorflow:input_ids: 101 7309 5338 2007 12581 16709 16643 3070 2809 2336 2012 11560 7713 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ccp5trMwRtmr","colab_type":"text"},"source":["#Creating a model\n","\n","Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. It loads the configs of the BERT model we specified earlier and it creates a single layer that will be trained to adapt BERT to our task (i.e. classifying whether a tweet is hate speech or not). This strategy of using a mostly trained model is called <i>fine-tuning</i>."]},{"cell_type":"code","metadata":{"id":"0_w7hsO8c2A_","colab_type":"code","colab":{}},"source":["def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n","                 labels, num_labels, use_one_hot_embeddings):\n","  \"\"\"Creates a classification model.\"\"\"\n","  model = modeling.BertModel(\n","      config=bert_config,\n","      is_training=is_training,\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      token_type_ids=segment_ids,\n","      use_one_hot_embeddings=use_one_hot_embeddings)\n","\n","  # In the demo, we are doing a simple classification task on the entire\n","  # segment.\n","  #\n","  # If you want to use the token-level output, use model.get_sequence_output()\n","  # instead.\n","  \n","  #output_layer = model.get_sequence_output()\n","  output_layer = model.get_pooled_output()\n","\n","  hidden_size = output_layer.shape[-1].value\n","\n","  output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","\n","  output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","  with tf.variable_scope(\"loss\"):\n","    if is_training:\n","      # I.e., 0.1 dropout\n","      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","\n","    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, output_bias)\n","    probabilities = tf.nn.softmax(logits, axis=-1)\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","    loss = tf.reduce_mean(per_example_loss)\n","\n","    return (loss, per_example_loss, logits, probabilities)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d8kL8ttxyIQB","colab_type":"text"},"source":["Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."]},{"cell_type":"code","metadata":{"id":"-gbOrzvodL6j","colab_type":"code","colab":{}},"source":["def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n","                     num_train_steps, num_warmup_steps, use_tpu,\n","                     use_one_hot_embeddings):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","\n","  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    tf.logging.info(\"*** Features ***\")\n","    for name in sorted(features.keys()):\n","      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n","\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","    is_real_example = None\n","    if \"is_real_example\" in features:\n","      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n","    else:\n","      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n","\n","    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n","\n","    (total_loss, per_example_loss, logits, probabilities) = create_model(\n","        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n","        num_labels, use_one_hot_embeddings)\n","\n","    tvars = tf.trainable_variables()\n","    initialized_variable_names = {}\n","    scaffold_fn = None\n","    if init_checkpoint:\n","      (assignment_map, initialized_variable_names\n","      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n","      if use_tpu:\n","\n","        def tpu_scaffold():\n","          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n","          return tf.train.Scaffold()\n","\n","        scaffold_fn = tpu_scaffold\n","      else:\n","        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n","\n","    \"\"\" tf.logging.info(\"**** Trainable Variables ****\")\n","    for var in tvars:\n","      init_string = \"\"\n","      if var.name in initialized_variable_names:\n","        init_string = \", *INIT_FROM_CKPT*\"\n","      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n","                      init_string)\"\"\"\n","\n","    output_spec = None\n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","\n","      train_op = optimization.create_optimizer(\n","          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n","\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          loss=total_loss,\n","          train_op=train_op,\n","          scaffold_fn=scaffold_fn)\n","    elif mode == tf.estimator.ModeKeys.EVAL:\n","\n","      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n","        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n","\n","        accuracy = tf.metrics.accuracy(labels=label_ids, predictions=predictions, weights=is_real_example)\n","        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n","        f1_score = tf.contrib.metrics.f1_score(label_ids, predictions)\n","        auc = tf.metrics.auc( label_ids, predictions)\n","        recall = tf.metrics.recall(label_ids, predictions)\n","        precision = tf.metrics.precision(label_ids, predictions)\n","        true_pos = tf.metrics.true_positives(label_ids, predictions)\n","        true_neg = tf.metrics.true_negatives(label_ids, predictions)\n","        false_pos = tf.metrics.false_positives(label_ids, predictions)  \n","        false_neg = tf.metrics.false_negatives(label_ids, predictions)\n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"eval_loss\": loss,\n","            \"F1_Score\": f1_score,\n","            \"auc\": auc,\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"true_positives\": true_pos,\n","            \"true_negatives\": true_neg,\n","            \"false_positives\": false_pos,\n","            \"false_negatives\": false_neg\n","        }\n","\n","      eval_metrics = (metric_fn, [per_example_loss, label_ids, logits, is_real_example])\n","\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          loss=total_loss,\n","          eval_metrics=eval_metrics,\n","          scaffold_fn=scaffold_fn)\n","    else:\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          predictions={\"probabilities\": probabilities},\n","          scaffold_fn=scaffold_fn)\n","    return output_spec\n","\n","  return model_fn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5gdp568cyB3M","colab_type":"text"},"source":["The Run config will be the same across all evaluation options below for running BERT"]},{"cell_type":"code","metadata":{"id":"QPYzNbvcx94h","colab_type":"code","colab":{}},"source":["# Model configs\n","SAVE_CHECKPOINTS_STEPS = 1000\n","run_config = tf.compat.v1.estimator.tpu.RunConfig(\n","    #I think the output file must be a sub-directory of the main BERT file\n","    model_dir=OUTPUT_DIR, \n","    cluster=cluster_resolver,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=100,    #Shows us summary metrics every 100 steps\n","        num_shards=8,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZY_71Lwmxwm1","colab_type":"text"},"source":["#Training and Evaluating BERT as normal"]},{"cell_type":"code","metadata":{"id":"OjwJ4bTeWXD8","colab_type":"code","outputId":"18264478-d648-410f-a4aa-36a5acfc060c","executionInfo":{"status":"ok","timestamp":1581511141280,"user_tz":0,"elapsed":123635,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":383}},"source":["# Compute train and warmup steps from batch size\n","TRAIN_BATCH_SIZE = 32 #recommended 16 or 32\n","EVAL_BATCH_SIZE = 8\n","PREDICT_BATCH_SIZE = 8\n","LEARNING_RATE = 2e-5 # Recommended 5e-5, 3e-5 or 2e-5\n","NUM_TRAIN_EPOCHS = 3.0 # Recommended 2, 3 or 4\n","MAX_SEQ_LENGTH = 256\n","# Warmup is a period of time where the learning rate \n","#is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","\n","# Compute # train and warmup steps from batch size\n","num_train_steps = int(len(train_features) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","print(\"The model will stop training when it reaches\", num_train_steps, \"as a checkpoint\")\n","print(\"\\nThe bert checkpoint directory is\", bert_ckpt_dir)\n","print(\"\\nThe output directory is\", OUTPUT_DIR)\n","\n","#This is the model function, which feeds in the bert configurations, the pretrained model itself and the parameters for the fine tuning of the model\n","model_fn = model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  use_one_hot_embeddings=True)\n","\n","#We use Tensorflow estimators to train, evaluate and test our model\n","estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","    use_tpu=True,\n","    model_fn=model_fn,\n","    config=run_config,\n","    train_batch_size=TRAIN_BATCH_SIZE,\n","    eval_batch_size=EVAL_BATCH_SIZE,\n","    predict_batch_size=PREDICT_BATCH_SIZE)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["The model will stop training when it reaches 841 as a checkpoint\n","\n","The bert checkpoint directory is gs://csc3002/wwm_uncased_L-24_H-1024_A-16\n","\n","The output directory is gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output\n","WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f8d822f82f0>) includes params argument, but params are not passed to Estimator.\n","INFO:tensorflow:Using config: {'_model_dir': 'gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.79.186.138:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8d85d47fd0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.79.186.138:8470', '_evaluation_master': 'grpc://10.79.186.138:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f8d8b688be0>}\n","INFO:tensorflow:_TPUContext: eval_on_tpu True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7siI74tc27eN","colab_type":"text"},"source":["Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator.\n","\n","This is a pretty standard design pattern for working with Tensorflow Estimators"]},{"cell_type":"code","metadata":{"id":"JUu_zpYV25z5","colab_type":"code","colab":{}},"source":["# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = bert.run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=True)\n","\n","# Input function for dev data, we feed in our previously created dev_features for this\n","test_input_fn = run_classifier.input_fn_builder(\n","    features=dev_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qp8UD9h13SvX","colab_type":"text"},"source":["<b>Now we train our BERT fine-tuned model"]},{"cell_type":"code","metadata":{"id":"vkNOJkVJ3SUh","colab_type":"code","outputId":"4eccceaf-94d3-4ebe-ab96-c768e810db5c","executionInfo":{"status":"ok","timestamp":1580813619530,"user_tz":0,"elapsed":498690,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["print(\"\\nThe model will stop training when it reaches\", num_train_steps, \"as a checkpoint\")\n","\n","print(f'Beginning Training!')\n","current_time = datetime.now()\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","train_time = datetime.now() - current_time\n","print(\"Training took time \", train_time)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","The model will stop training when it reaches 841 as a checkpoint\n","Beginning Training!\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.28.171.74:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 8278283442579639588)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15075832258415459024)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9235653837146825852)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 1552753277433411770)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 7926628980454224206)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7487291541762347249)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 9835192356092401906)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15341246576360441729)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 11817745622014086124)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 17020960504450462042)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5733577045996660008)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:751: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer Variable.assign which has equivalent behavior in 2.X.\n","INFO:tensorflow:Initialized dataset iterators in 1 seconds\n","INFO:tensorflow:Installing graceful shutdown hook.\n","INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 7 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Enqueue next (500) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (500) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:Outfeed finished for iteration (0, 310)\n","INFO:tensorflow:loss = 0.690946, step = 500\n","INFO:tensorflow:Enqueue next (341) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (341) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (1, 39)\n","INFO:tensorflow:loss = 0.6019056, step = 841 (79.208 sec)\n","INFO:tensorflow:global_step/sec: 4.30513\n","INFO:tensorflow:examples/sec: 137.764\n","INFO:tensorflow:Saving checkpoints for 841 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt.\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Loss for final step: 0.6019056.\n","INFO:tensorflow:training_loop marked as finished\n","Training took time  0:08:18.109756\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T_MPSFxy3gNG","colab_type":"text"},"source":["<b>And now we evaluate the performance of our model on the development data<b>"]},{"cell_type":"code","metadata":{"id":"uPLbCsbK3zHW","colab_type":"code","outputId":"c2b04581-31c0-46c8-e0f1-2b6bdc4ebfc4","executionInfo":{"status":"ok","timestamp":1580813843624,"user_tz":0,"elapsed":169584,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":983}},"source":["#You need to provide number of steps for a TPU\n","eval_steps = int(len(dev_InputExamples) / EVAL_BATCH_SIZE)\n","\n","#Eval will be slightly WRONG on the TPU because it will drop the last batch (drop_remainder = True).\n","estimator.evaluate(input_fn=test_input_fn, steps=eval_steps)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (1, 256)\n","INFO:tensorflow:  name = input_mask, shape = (1, 256)\n","INFO:tensorflow:  name = label_ids, shape = (1,)\n","INFO:tensorflow:  name = segment_ids, shape = (1, 256)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3322: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-02-04T10:54:43Z\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt-841\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 9 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Initialized dataset iterators in 0 seconds\n","INFO:tensorflow:Enqueue next (125) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (125) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:Evaluation [125/125]\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Finished evaluation at 2020-02-04-10:57:17\n","INFO:tensorflow:Saving dict for global step 841: F1_Score = 0.59845823, auc = 0.5, eval_accuracy = 0.573, eval_loss = 0.682442, false_negatives = 427.0, false_positives = 0.0, global_step = 841, loss = 0.69324154, precision = 0.0, recall = 0.0, true_negatives = 573.0, true_positives = 0.0\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 841: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt-841\n","INFO:tensorflow:evaluation_loop marked as finished\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'F1_Score': 0.59845823,\n"," 'auc': 0.5,\n"," 'eval_accuracy': 0.573,\n"," 'eval_loss': 0.682442,\n"," 'false_negatives': 427.0,\n"," 'false_positives': 0.0,\n"," 'global_step': 841,\n"," 'loss': 0.69324154,\n"," 'precision': 0.0,\n"," 'recall': 0.0,\n"," 'true_negatives': 573.0,\n"," 'true_positives': 0.0}"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"mDL1NYjW5_Gr","colab_type":"text"},"source":["Now that the pre-processing methods have been demonstrated. We can just load in data quickly using the function below. One can load one set or load in two and combine them - just like I'll demonstrate with the training set and the dev set\n","\n","<i>We'll combine these two sets for the cross-validation function below</i>"]},{"cell_type":"code","metadata":{"id":"Jrm4IWXuzudy","colab_type":"code","outputId":"25637e00-384d-4c28-a239-9e3466a351c5","executionInfo":{"status":"ok","timestamp":1581511272629,"user_tz":0,"elapsed":17777,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def loadData(data1, data2 = None, replaceEmoji = True, \\\n","             segmentHashtag = True, removeSmall = True): #Function caller can optionally load two dataframes and combine them\n","\n","  if data2 is not None:\n","    frames = [data1,data2]\n","    data = pd.concat(frames)\n","  else:\n","    data = data1\n","  \n","\n","  data.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","  cols = ['TR', 'AG']\n","  data.drop(cols, inplace = True, axis = 1)\n","  data = data.sample(frac=1, random_state= 3060)\n","\n","  if replaceEmoji == True:\n","    data['tweet'] = data['tweet'].apply(emojiReplace)\n","\n","  data['tweet'] = data['tweet'].apply(preprocess)\n","\n","  if segmentHashtag == True:\n","    data['tweet'] = data['tweet'].apply(hashtagSegment)\n","\n","  if removeSmall == True:\n","    data = data[data['tweet'].apply(lambda x: len(x) > 10)] \n","\n","  data.dropna(inplace = True)\n","  data.reset_index(drop = True, inplace = True)\n","\n","  data.id = data.index\n","  return data\n","\n","train = pd.read_csv('gs://csc3002/hateval2019/hateval2019_en_train.csv', sep=',',  index_col = False, encoding = 'utf-8')\n","dev =  pd.read_csv('gs://csc3002/hateval2019/hateval2019_en_dev.csv', sep=',',  index_col = False, encoding = 'utf-8')\n","\n","data = loadData(train, dev)\n","print(\"Out of {} tweets in this database, {} are not hate, {} are hate\\n\".format(len(data.index), \n","                                                    len(data[data['label']==0]),\n","                                                    len(data[data['label']==1])))\n","data[:50]"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Out of 9968 tweets in this database, 5765 are not hate, 4203 are hate\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>drake- i love you, we should date rihanna the savage-</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>an honour to meet so many dedicated aid workers here. this is mitun from whod recently attended training on child protection provided by . building up specialist refugee protection expertise within national ngos is part of our work here. aid works rohingya</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>despite boko haram threat, some displaced nigerians return: via</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>rage of women of trump in the white house made them individually fearless to turn on weinstein! blame 1 man not all men.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>french police officers injured by 'drunk' uk-bound migrants in calais via !</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>1/ I am reminded of an interaction that fuels me. soon after returning to finish my undergrad, an uncle came to visit. he is a greek immigrant..very greek. he visited my parents' restaurant, where i was working.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>rip twitter bitch cunt</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>yorkshire woman who taught a man consent while he was rape -ing her launches 1st real anti - rape</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>election fraud true the vote voter id illegal immigration voter fraud no election fraud? i do not think so. if the left is so interested in true elections then we should true the vote. dead people...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>watch: british tourist slaps immigration officer over $4,000 fine</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>cringemuch .when women get panicked&amp;start getting hysterical when having a standard delivery. everyone knows save energy,focus&amp;push.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>introvert level expert</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>nutting inside a bitch is rape ???</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>italy's new interior minister tells migrants 'pack your bags!' as he orders mass deportation - free speech time -</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>in modern life, sleep is becoming an increasing issue. so, to support you and your healthy nights rest, we have some top tips to aid a restful night: thursday thoughts</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>no, millions of americans have lost jobs to illegal immigrants for 3 reasons...they work for less, often for cash and are more reliable. sought out by employers. asylum seekers are trying to excape violence, want to assim</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>unblock my account 'aryanfascist101'. I am a nazi/fascist human supporter. (this is from backup account)</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>immigration will not make america great again</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>what a wonderful woman! this is the kind of immigrant we need here! two hearts flag: united states flag: united states flag: united states flexed biceps: light skin tone</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>I am beside myself watching these protests in california double exclamation mark unbelievable face with symbols on mouth pouting face a bunch of unamericans gathered together in order to spout off bullshit double exclamation mark gt fo of our country prohibited trash litter in bin sign batshit crazy undocumented is criminal send them back america first build the wall flag: united states</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>for more on the findings discussed in this article, including state-by-state data, check out:</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>in saudi &amp;; dubai if a man rape a woman with condom than semen is not found in vagina than how do sharia law tackle? i tell you punish the woman</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>thems the rules</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>mike i do not know you from a hole in the wall you are a businessman either way you can not lose, but</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>it is unfair to immigrants apply and wait for a visa to reside,work or go to school, here in the you.s., that illegal aliens are allowed to stay without documentation. let them go home and apply for a visa like all the legal immigrants have. what makes illegals exempt?</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>us vp arrives in ecuador, meets venezuelan migrants</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>so I am working but a friend of mine is telling me that corey booker and kamala harris are fighting for who is the more hysterical woman in the kavanaugh hearings is that true?#kavanaugh kavanaugh confirmation</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>not to mention that book claire did not stay in the kitchen either so the women clearly does not know what she is talking about. however yes so much more to women than doing the cooking, cleaning etc.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>1,500 migrants have died in mediterranean in 2018</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>frost is a stupid bitch.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>30</td>\n","      <td>I am so excited to announce my partnership with . I will be teaching 10 people my secrets to the kitchen in an intimate cooking class in la with air bnb experiences all proceeds will be donated to the americans for immigrant justice foundation ad</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>31</td>\n","      <td>cctv: shocking servo attack on pregnant woman via christopher josevski you are a fucking vile cunt angry face</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>32</td>\n","      <td>us senate calls on julian assange to testify</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>33</td>\n","      <td>oh btw just to offend the femalesss cunt cunt cunt whore cunt whore slut bitchslut feministshit</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>34</td>\n","      <td>leave us a alon damn migrants go home</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>35</td>\n","      <td>do mep,s visit ,greece to see ,the refugee reductions already trying to justify the turkey deal ,afraid its not working</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>36</td>\n","      <td>protecting refugee children is a test of our humanity</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>37</td>\n","      <td>we need a moratorium on all immigration, legal or otherwise, esp from non-european countries</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>38</td>\n","      <td>i do not think this fat whore could even get sex from rape.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>39</td>\n","      <td>italian boat returning migrants to libya may have broken law, un agency says</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>40</td>\n","      <td>I have seen you break down in tears and when the audience turned on you so let us not talk about hysterical women.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>41</td>\n","      <td>the situation here is already tense, imagine after this! yesterday, hundreds of refugees protested in mytilene in memory of an afghani who died due to lack of care in moria. riots and violent protests have already happened for less in the</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>42</td>\n","      <td>obituary: victoria reilly, inspirational teacher who spent her early life as a refugee</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>43</td>\n","      <td>so do we! check out our account for upcoming events and meetings male dominance</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>44</td>\n","      <td>build the wall now before the midterm elections maga make dc listen no amnesty no daca deal mandate e verify end chain migration</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>45</td>\n","      <td>filth no daca depor them all going to go turn on the phone now that you have to babysit that stupid gang behavior and idiotic graffiti. berkeley ca deserves better</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>46</td>\n","      <td>my sexting be so fuckin hot that i turn my damn self on</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>47</td>\n","      <td>urgh. tmj pain at full pelt this morning. all of the pain, very little sleep &amp; I am off to a separated parents information programme as the court do not seem to buy that i can not communicate with my ex because hes an abuser but that I am a hysterical woman who needs teaching.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>48</td>\n","      <td>thank god trump won!!!#maga america first build the wall no daca no chain immigration no anchor babies no sanctuary cities no sanctuary states term limits drain the swamp god bless america god bless ourpotustrump2020</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>49</td>\n","      <td>if illegals do not want to be separated from their children at the border,this can easily be prevented. do not illegally cross the order. why does anyone expect them to be rewarded for committing a crime? send them back build the wal no daca trump</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  ... label\n","0   0   ...  0   \n","1   1   ...  0   \n","2   2   ...  0   \n","3   3   ...  0   \n","4   4   ...  0   \n","5   5   ...  0   \n","6   6   ...  0   \n","7   7   ...  0   \n","8   8   ...  0   \n","9   9   ...  0   \n","10  10  ...  1   \n","11  11  ...  0   \n","12  12  ...  1   \n","13  13  ...  1   \n","14  14  ...  0   \n","15  15  ...  1   \n","16  16  ...  0   \n","17  17  ...  1   \n","18  18  ...  0   \n","19  19  ...  1   \n","20  20  ...  0   \n","21  21  ...  1   \n","22  22  ...  0   \n","23  23  ...  0   \n","24  24  ...  1   \n","25  25  ...  0   \n","26  26  ...  1   \n","27  27  ...  1   \n","28  28  ...  0   \n","29  29  ...  0   \n","30  30  ...  0   \n","31  31  ...  0   \n","32  32  ...  0   \n","33  33  ...  1   \n","34  34  ...  1   \n","35  35  ...  0   \n","36  36  ...  0   \n","37  37  ...  0   \n","38  38  ...  1   \n","39  39  ...  0   \n","40  40  ...  0   \n","41  41  ...  0   \n","42  42  ...  0   \n","43  43  ...  1   \n","44  44  ...  1   \n","45  45  ...  1   \n","46  46  ...  0   \n","47  47  ...  0   \n","48  48  ...  1   \n","49  49  ...  1   \n","\n","[50 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"k-AaQFsSb9xB","colab_type":"text"},"source":["Function to get metrics from an event file:"]},{"cell_type":"code","metadata":{"id":"39xFx83Gb8MT","colab_type":"code","colab":{}},"source":[" def get_metrics(OUTPUT_DIR, train_time, k=0):\n","\n","   os.environ[\"GCLOUD_PROJECT\"] = \"csc3002\"\n","   client = storage.Client()\n","   bucket = client.bucket('csc3002')\n","\n","   blobs = list(bucket.list_blobs(prefix=os.path.join(OUTPUT_DIR[13:], 'eval')))\n","   for blob in blobs:\n","     if 'events' in blob.name:\n","       eval_file = os.path.join('gs://csc3002/', blob.name)\n","          \n","   for e in tf.train.summary_iterator(eval_file):\n","     for v in e.summary.value:\n","\n","       if v.tag == 'F1_Score':\n","         fscore = v.simple_value\n","\n","       if v.tag == 'auc':\n","         auc = v.simple_value\n","\n","       if v.tag == 'eval_accuracy':\n","         accuracy = v.simple_value\n","\n","       if v.tag == 'recall':\n","         recall = v.simple_value\n","\n","       if v.tag == 'precision':\n","         precision = v.simple_value\n","\n","       if v.tag == 'false_positives':\n","         false_positives = v.simple_value\n","\n","       if v.tag == 'false_negatives':\n","         false_negatives = v.simple_value\n","\n","       if v.tag == 'true_positives':\n","         true_positives = v.simple_value\n","\n","       if v.tag == 'true_negatives':\n","         true_negatives = v.simple_value\n","    \n","   row = pd.Series({'F1 Score': fscore, 'auc': auc, 'Accuracy': accuracy,'Precision': precision,'Recall': recall,\\\n","                                    'False Negatives': false_negatives,'False Positives': false_positives,\\\n","                    'True Negatives':true_negatives ,'True Positives': true_positives, 'Training Time': train_time })\n","   if k != 0:\n","      row = pd.Series(row, name = 'Fold ' + str(k))\n","   else:\n","     row1 = pd.Series({'Batch Size': 32, 'Learn Rate': 2e-5, 'epochs': 3})\n","     row = pd.concat([row1, row], axis= 0, sort =False)\n","    \n","   return row"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VaGhttd75vOv","colab_type":"text"},"source":["# Cross Validation evaluation\n","\n","Does not provide in depth tensorflow logging but it does provide evaluation at the end. As mentioned above, we combine the provided training and dev files\n","\n"]},{"cell_type":"code","metadata":{"id":"Hvxh57Zh5zto","colab_type":"code","colab":{}},"source":["def bertCV(train_batch_size = 32, learn_rate = 2e-5,\\\n","           num_epochs =3.0, folds = 5,  gridSearch = False):\n","\n","  #Filter out all log messages so console isn't consumed with memory\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","  train = pd.read_csv('gs://csc3002/hateval2019/hateval2019_en_train.csv', sep=',',  index_col = False, encoding = 'utf-8')\n","  dev =  pd.read_csv('gs://csc3002/hateval2019/hateval2019_en_dev.csv', sep=',',  index_col = False, encoding = 'utf-8')\n","  \n","  loadData(train, dev)\n","\n","  \"\"\"FIXED MODEL PARAMS\"\"\"\n","  EVAL_BATCH_SIZE = 8\n","  PREDICT_BATCH_SIZE = 8\n","  MAX_SEQ_LENGTH = 256\n","  # Warmup is a period of time where the learning rate \n","  #is small and gradually increases--usually helps training.\n","  WARMUP_PROPORTION = 0.1\n","\n","  #Dataframe where grid search results will be stored. Empty to begin with\n","  eval_df = pd.DataFrame(columns = ['F1 Score', 'auc', 'Accuracy'] )\n","  \n","  k = 1 # Fold counter\n","  cv = KFold(n_splits=folds, shuffle=False)\n","  for train_index, dev_index in cv.split(data.index): # Sticking within the training dataset for evaluation. Train is the combination of the provided train and dev sets\n","    \n","    training  = data.iloc[train_index]\n","    develop = data.iloc[dev_index]\n","    \n","    \"\"\"Unlike before where I only one test set and one training set, this time I have K different sets of training and testing.\n","    Therefore, in each fold I need to get a new set of data and convert it to features each time.\"\"\"\n","    \n","    # Use the InputExample class from BERT's run_classifier code to create examples from the data\n","\n","    train_InputExamples = training.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                      text_a = x[DATA_COLUMN], \n","                                                                      text_b = None, \n","                                                                      label = x[LABEL_COLUMN]), axis = 1)\n","\n","    dev_InputExamples = develop.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                      text_a = x[DATA_COLUMN], \n","                                                                      text_b = None, \n","                                                                      label = x[LABEL_COLUMN]), axis = 1)\n","    \n","    #Convert these examples to features that BERT can interpret\n","    train_features =  bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","    dev_features = bert.run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","\n","    #Delete prior model graph, checkpoints and eval files to make room for new model each loop\n","    try:\n","      tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","    except:\n","    # Doesn't matter if the directory didn't exist\n","      pass\n","    tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","    # Compute # train and warmup steps from batch size\n","    num_train_steps = int(len(train_features) / train_batch_size * NUM_TRAIN_EPOCHS)\n","    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","    # Model configs\n","    model_fn = model_fn_builder(\n","    bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","    num_labels=len(label_list),\n","    init_checkpoint=bert_ckpt_file,\n","    learning_rate=LEARNING_RATE,\n","    num_train_steps=num_train_steps,\n","    num_warmup_steps=num_warmup_steps,\n","    use_tpu=True,\n","    use_one_hot_embeddings=True)\n","\n","    estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","      use_tpu=True,\n","      model_fn=model_fn,\n","      config=run_config,\n","      train_batch_size=TRAIN_BATCH_SIZE,\n","      eval_batch_size=EVAL_BATCH_SIZE,\n","      predict_batch_size=PREDICT_BATCH_SIZE)\n","    \n","    # Create an input function for training. drop_remainder = True for using TPUs.\n","    train_input_fn = bert.run_classifier.input_fn_builder(\n","        features=train_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=True,\n","        drop_remainder=True)\n","\n","    #input function for dev data, we feed in our previously created dev_features for this\n","    dev_input_fn = run_classifier.input_fn_builder(\n","        features=dev_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=False,\n","        drop_remainder=True)\n","    \n","    current_time = datetime.now()\n","    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","    train_time = datetime.now() - current_time\n","    \n","\n","    #You need to provide number of steps for a TPU\n","    eval_steps = int(len(dev_InputExamples) / EVAL_BATCH_SIZE)\n","\n","    #Eval will be slightly WRONG on the TPU because it will truncate the last batch.\n","    estimator.evaluate(input_fn=dev_input_fn, steps=eval_steps)\n","\n","   \n","    row = get_metrics(OUTPUT_DIR, train_time, k)\n","    eval_df = eval_df.append(row)\n","    print(\"Fold \" + str(k) + \":\\tF-Score:\", eval_df[\"F1 Score\"][k-1])\n","    print(\"Training took time \", train_time)\n","    print('---------------------------------------------------------------------------------------------------------\\n')\n","    k = k + 1 #Increment on fold counter\n","\n","  row = eval_df.mean(axis = 0)\n","  row = pd.Series(row, name = 'CV Average')\n","  eval_df = eval_df.append(row)\n","  \n","  if gridSearch == False:\n","    print(\"\\nTraining Batch Size:\", train_batch_size,  \"\\tLearn Rate: \",learn_rate, \"\\tNum Epochs: \", num_epochs )\n","    display(eval_df)\n","\n","  return row # Also return row of CV-Average"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GwCunxCw6tmT","colab_type":"text"},"source":["# HyperParameter Grid Search\n","Can also just be used to train and evaluate on single parameters if one wishes. \n","\n","Does not provide extra info such as average loss at each checkpoint like normal training of BERT will do because I set the tensorflow verbosity to ERROR so as to not overload the console with information and thus overload memory.\n","\n","However the metrics for each unique hyperparameter pairing are displayed upon completion of the function"]},{"cell_type":"code","metadata":{"id":"Lv4puqUe6tWK","colab_type":"code","colab":{}},"source":["def bertGridSearch(lr_values, num_epochs, train_batch_size = [32], CV = False):\n","\n","  #Filter out all log messages so console isn't consumed with memory\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","  \"\"\"FIXED MODEL PARAMS\"\"\"\n","  EVAL_BATCH_SIZE = 8\n","  PREDICT_BATCH_SIZE = 8\n","  MAX_SEQ_LENGTH = 256\n","  # Warmup is a period of time where the learning rate \n","  #is small and gradually increases--usually helps training.\n","  WARMUP_PROPORTION = 0.1\n","\n","  #Dataframe where grid search results will be stored. Empty to begin with\n","  eval_df = pd.DataFrame(columns = ['F1 Score', 'Precision', 'false_positives'] )\n","\n","  for TRAIN_BATCH_SIZE, NUM_TRAIN_EPOCHS, LEARNING_RATE in product(train_batch_size, num_epochs, lr_values):\n","\n","    if CV == True:\n","      row = bertCV(train_batch_size = TRAIN_BATCH_SIZE, learn_rate = LEARNING_RATE,\\\n","                   num_epochs =NUM_TRAIN_EPOCHS, gridSearch = True)\n","      \n","    else:\n","      #Delete prior model graph, checkpoints and eval files to make room for new model each loop\n","      try:\n","        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","      except:\n","      # Doesn't matter if the directory didn't exist\n","        pass\n","      tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","\n","      # Compute # train and warmup steps from batch size\n","      num_train_steps = int(len(train_features) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","      num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","      # Model configs\n","      model_fn = model_fn_builder(\n","      bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","      num_labels=len(label_list),\n","      init_checkpoint=bert_ckpt_file,\n","      learning_rate=LEARNING_RATE,\n","      num_train_steps=num_train_steps,\n","      num_warmup_steps=num_warmup_steps,\n","      use_tpu=True,\n","      use_one_hot_embeddings=True)\n","\n","      estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","        use_tpu=True,\n","        model_fn=model_fn,\n","        config=run_config,\n","        train_batch_size=TRAIN_BATCH_SIZE,\n","        eval_batch_size=EVAL_BATCH_SIZE,\n","        predict_batch_size=PREDICT_BATCH_SIZE)\n","      \n","      # Create an input function for training. drop_remainder = True for using TPUs.\n","      train_input_fn = bert.run_classifier.input_fn_builder(\n","          features=train_features,\n","          seq_length=MAX_SEQ_LENGTH,\n","          is_training=True,\n","          drop_remainder=True)\n","\n","      #input function for test data, we feed in our previously created dev_features for this\n","      dev_input_fn = run_classifier.input_fn_builder(\n","          features=dev_features,\n","          seq_length=MAX_SEQ_LENGTH,\n","          is_training=False,\n","          drop_remainder=True)\n","      \n","      current_time = datetime.now()\n","      estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","      train_time = datetime.now() - current_time\n","      print(\"Training took time \", train_time)\n","\n","      #You need to provide number of steps for a TPU\n","      eval_steps = int(len(dev_InputExamples) / EVAL_BATCH_SIZE)\n","\n","      #Eval will be slightly WRONG on the TPU because it will truncate the last batch.\n","      estimator.evaluate(input_fn=dev_input_fn, steps=eval_steps)\n","\n","      row = get_metrics(OUTPUT_DIR, train_time)\n","      \n","\n","    eval_df = eval_df.append(row, ignore_index = True)\n","    print(\"\\nTraining Batch Size: \" + str(TRAIN_BATCH_SIZE), \\\n","          '\\nLearning Rate: ' + str(LEARNING_RATE),  '\\t\\tF-Score:', fscore,\n","          '\\nNumber of epochs: ' + str(NUM_TRAIN_EPOCHS),\\\n","          '\\n-----------------------------------------------------------------------------------------------------------------------------\\n')\n","          \n","    \n","  #display(eval_df)\n","  idx = ['Batch Size', 'Learning Rate','Epochs']\n","  eval_df.set_index(idx, inplace = True)\n","  display(eval_df)  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jujNF1OEwaBD","colab_type":"text"},"source":["# You can run a grid search or cross-validation evaluation from here\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l9Y4Ql38-YQe","colab_type":"text"},"source":["<b> Cross-Validation"]},{"cell_type":"code","metadata":{"id":"h1ctK3c4LXVf","colab_type":"code","colab":{}},"source":["CV_Av = bertCV(learn_rate = 2.5e-5, num_epochs=3.0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Te2e53KNcrW","colab_type":"code","outputId":"2e477585-077c-445d-ec80-eff7c6c6a3d7","executionInfo":{"status":"ok","timestamp":1581426852771,"user_tz":0,"elapsed":465,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["e60 = CV_Av\n","print(e60)\n","e60 = pd.Series(e60, name = '60000')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["F1 Score           0.807454              \n","auc                0.832936              \n","Accuracy           0.833534              \n","False Negatives    144.2                 \n","False Positives    187.4                 \n","Precision          0.787785              \n","Recall             0.828457              \n","Training Time      0 days 00:08:12.122312\n","True Negatives     964.6                 \n","True Positives     695.8                 \n","Name: CV Average, dtype: object\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ksGfCY22fuIL","colab_type":"code","outputId":"16af8b02-723b-4b28-b808-2836321d317d","executionInfo":{"status":"ok","timestamp":1581426886354,"user_tz":0,"elapsed":1038,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":278}},"source":["eval_df = pd.read_csv('gs://csc3002/hateval2019/pretraining_eval_df.csv', sep=',',  index_col = 0, encoding = 'utf-8')\n","eval_df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1 Score</th>\n","      <th>auc</th>\n","      <th>Accuracy</th>\n","      <th>False Negatives</th>\n","      <th>False Positives</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Training Time</th>\n","      <th>True Negatives</th>\n","      <th>True Positives</th>\n","      <th>false_negatives</th>\n","      <th>false_positives</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>true_negatives</th>\n","      <th>true_positives</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>120000</th>\n","      <td>0.809541</td>\n","      <td>0.834597</td>\n","      <td>0.834337</td>\n","      <td>138.4</td>\n","      <td>191.6</td>\n","      <td>0.785437</td>\n","      <td>0.835473</td>\n","      <td>0 days 00:08:52.374760600</td>\n","      <td>960.4</td>\n","      <td>701.6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>0.796205</td>\n","      <td>0.823387</td>\n","      <td>0.825803</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0 days 00:09:23.425535</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>161.4</td>\n","      <td>185.6</td>\n","      <td>0.785145</td>\n","      <td>0.807766</td>\n","      <td>966.0</td>\n","      <td>679.0</td>\n","    </tr>\n","    <tr>\n","      <th>40000</th>\n","      <td>0.812154</td>\n","      <td>0.837227</td>\n","      <td>0.839056</td>\n","      <td>146.6</td>\n","      <td>174.0</td>\n","      <td>0.799342</td>\n","      <td>0.825520</td>\n","      <td>0 days 00:08:16.699617</td>\n","      <td>978.0</td>\n","      <td>693.4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>60000</th>\n","      <td>0.807454</td>\n","      <td>0.832936</td>\n","      <td>0.833534</td>\n","      <td>144.2</td>\n","      <td>187.4</td>\n","      <td>0.787785</td>\n","      <td>0.828457</td>\n","      <td>0 days 00:08:12.122312</td>\n","      <td>964.6</td>\n","      <td>695.8</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        F1 Score       auc  Accuracy  ...    recall  true_negatives  true_positives\n","120000  0.809541  0.834597  0.834337  ... NaN       NaN             NaN            \n","0       0.796205  0.823387  0.825803  ...  0.807766  966.0           679.0         \n","40000   0.812154  0.837227  0.839056  ... NaN       NaN             NaN            \n","60000   0.807454  0.832936  0.833534  ... NaN       NaN             NaN            \n","\n","[4 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":107}]},{"cell_type":"code","metadata":{"id":"1T8q6AvG0H91","colab_type":"code","outputId":"7ef331d2-ece4-471c-eea4-970dd9e30250","executionInfo":{"status":"ok","timestamp":1581426870740,"user_tz":0,"elapsed":428,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":278}},"source":["#eval_df = pd.DataFrame(columns = ['F1 Score', 'auc', 'Accuracy'] ) # Instantise\n","eval_df = eval_df.append(e60)\n","eval_df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1 Score</th>\n","      <th>auc</th>\n","      <th>Accuracy</th>\n","      <th>False Negatives</th>\n","      <th>False Positives</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Training Time</th>\n","      <th>True Negatives</th>\n","      <th>True Positives</th>\n","      <th>false_negatives</th>\n","      <th>false_positives</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>true_negatives</th>\n","      <th>true_positives</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>120000</th>\n","      <td>0.809541</td>\n","      <td>0.834597</td>\n","      <td>0.834337</td>\n","      <td>138.4</td>\n","      <td>191.6</td>\n","      <td>0.785437</td>\n","      <td>0.835473</td>\n","      <td>0 days 00:08:52.374760600</td>\n","      <td>960.4</td>\n","      <td>701.6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>0.796205</td>\n","      <td>0.823387</td>\n","      <td>0.825803</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0 days 00:09:23.425535</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>161.4</td>\n","      <td>185.6</td>\n","      <td>0.785145</td>\n","      <td>0.807766</td>\n","      <td>966.0</td>\n","      <td>679.0</td>\n","    </tr>\n","    <tr>\n","      <th>40000</th>\n","      <td>0.812154</td>\n","      <td>0.837227</td>\n","      <td>0.839056</td>\n","      <td>146.6</td>\n","      <td>174.0</td>\n","      <td>0.799342</td>\n","      <td>0.825520</td>\n","      <td>0 days 00:08:16.699617</td>\n","      <td>978.0</td>\n","      <td>693.4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>60000</th>\n","      <td>0.807454</td>\n","      <td>0.832936</td>\n","      <td>0.833534</td>\n","      <td>144.2</td>\n","      <td>187.4</td>\n","      <td>0.787785</td>\n","      <td>0.828457</td>\n","      <td>0 days 00:08:12.122312</td>\n","      <td>964.6</td>\n","      <td>695.8</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        F1 Score       auc  Accuracy  ...    recall  true_negatives  true_positives\n","120000  0.809541  0.834597  0.834337  ... NaN       NaN             NaN            \n","0       0.796205  0.823387  0.825803  ...  0.807766  966.0           679.0         \n","40000   0.812154  0.837227  0.839056  ... NaN       NaN             NaN            \n","60000   0.807454  0.832936  0.833534  ... NaN       NaN             NaN            \n","\n","[4 rows x 16 columns]"]},"metadata":{"tags":[]},"execution_count":105}]},{"cell_type":"code","metadata":{"id":"RQA09AG2foBH","colab_type":"code","colab":{}},"source":["eval_df.to_csv('gs://csc3002/hateval2019/pretraining_eval_df.csv', sep=',',  index = True, encoding = 'utf-8')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4xqjElk2IKTo","colab_type":"text"},"source":["<b> Grid Serch - with optional Cross-Validation"]},{"cell_type":"code","metadata":{"id":"NsIC3a2tIJtb","colab_type":"code","outputId":"847b2cb8-2d45-4de8-d4ba-6925f2baad43","executionInfo":{"status":"error","timestamp":1581170605825,"user_tz":0,"elapsed":3629718,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":682}},"source":["#0.796205\n","#Grid Search\n","bertGridSearch(num_epochs=[3.0], lr_values=[2e-5, 2.5e-5, 3e-5], CV =True) # Obviously takes much longer when CV = True"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Fold 1:\tF-Score: 0.8163264393806458\n","Training took time  0:09:02.120107\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 2:\tF-Score: 0.8084357380867004\n","Training took time  0:09:05.778452\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 3:\tF-Score: 0.7985479831695557\n","Training took time  0:09:14.959567\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 4:\tF-Score: 0.7967666983604431\n","Training took time  0:09:13.903270\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 5:\tF-Score: 0.8262107372283936\n","Training took time  0:09:07.563189\n","---------------------------------------------------------------------------------------------------------\n","\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-1388ad143a43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbertGridSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.5e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3e-5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCV\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Obviously takes much longer when CV = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-44-1a4269819a10>\u001b[0m in \u001b[0;36mbertGridSearch\u001b[0;34m(lr_values, num_epochs, train_batch_size, CV)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0meval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     print(\"\\nTraining Batch Size: \" + str(TRAIN_BATCH_SIZE),           '\\nLearning Rate: ' + str(LEARNING_RATE),  '\\t\\tF-Score:', fscore,\n\u001b[0m\u001b[1;32m     85\u001b[0m           \u001b[0;34m'\\nNumber of epochs: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_TRAIN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m           '\\n-----------------------------------------------------------------------------------------------------------------------------\\n')\n","\u001b[0;31mNameError\u001b[0m: name 'fscore' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"y4KnOByLTj9i","colab_type":"text"},"source":["## Adding in augmented back-translated hate speech tweets as extra data\n","\n","We have very few instances of hate speech labelled in this dataset. To remedy this I performed back_translation augmentation on this training set.\n","\n","Below I load in in the extra hate speech tweets I created via back-translation augmentation I performed in another colab notebook and I append it to the existing dataframe"]},{"cell_type":"code","metadata":{"id":"DHT16O8CTfpN","colab_type":"code","outputId":"8cd09f29-baf1-4fd0-fba9-e4ac17523842","executionInfo":{"status":"ok","timestamp":1580488889789,"user_tz":0,"elapsed":3685511,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"dat = '/content/drive/My Drive/hateval2019/backtranslated_hatEval.txt' \n","dat = pd.read_csv(dat, sep = '\\t', names = ['tweet'], header = None, encoding = 'utf-8')\n","pd.set_option('display.max_colwidth', -1)\n","dat = dat.astype(str)\n","dat.head(50)\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"dat = '/content/drive/My Drive/hateval2019/backtranslated_hatEval.txt' \\ndat = pd.read_csv(dat, sep = '\\t', names = ['tweet'], header = None, encoding = 'utf-8')\\npd.set_option('display.max_colwidth', -1)\\ndat = dat.astype(str)\\ndat.head(50)\""]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"X9jImOlRT2EO","colab_type":"text"},"source":["**See how the english is a little off?** \n","\n","That's because these are the hate speech tweets in the training set translated to french, then translated back again. This creates a whole new, yet similar set of hate speech tweets to train on. (Slightly augmented text)"]},{"cell_type":"code","metadata":{"id":"pe1Ndra6UAbb","colab_type":"code","outputId":"219efd82-0da1-46f3-fd7d-30f6077675a2","executionInfo":{"status":"ok","timestamp":1580488889789,"user_tz":0,"elapsed":3685501,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"print(\"There are\", len(dat.index), \"tweets\")\n","dat = dat[dat['tweet'].apply(lambda x: len(x) > 10)]\n","print(\"There are now\", len(dat.index), \"tweets\")\n","dat.head()\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'print(\"There are\", len(dat.index), \"tweets\")\\ndat = dat[dat[\\'tweet\\'].apply(lambda x: len(x) > 10)]\\nprint(\"There are now\", len(dat.index), \"tweets\")\\ndat.head()'"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"vNeswnQeUBY2","colab_type":"text"},"source":["<b>Rather than creating 3768 extra tweets, 19630 extra have been created. The tweets have been incorrectly parsed. Removing some tweets with a smaller length may mitigate this effect somewhat by removing tweets that were cut in half</b>\n","\n","Let's see if it helps by adding it to the original training set and testing it against our dev data"]},{"cell_type":"code","metadata":{"id":"apxp79BtUyf5","colab_type":"code","outputId":"cee28152-5561-4062-e3e2-12c5fb663e9f","executionInfo":{"status":"ok","timestamp":1580488889790,"user_tz":0,"elapsed":3685491,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"dat['label'] = 1\n","dat['id'] = 80000\n","frames = [dat,data]\n","data = pd.concat(frames)\n","print(data.info())\n","data.head()\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"dat['label'] = 1\\ndat['id'] = 80000\\nframes = [dat,data]\\ndata = pd.concat(frames)\\nprint(data.info())\\ndata.head()\""]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"lHT94i8VU4r7","colab_type":"text"},"source":["We'll shuffle the dataframe to make sure there's no funny business with the training of the model and we'll then reset the id field to make it unique and sequential for each row"]},{"cell_type":"code","metadata":{"id":"w8rPNri1VAiE","colab_type":"code","outputId":"1cbdbd13-3813-401a-d883-c5995c7c1158","executionInfo":{"status":"ok","timestamp":1580488889791,"user_tz":0,"elapsed":3685476,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"data = data.sample(frac=1)\n","data.reset_index(drop = True, inplace = True)\n","\n","data['id'] = data.reset_index().index + 1\n","print(data.label.value_counts(), \"\\n\")\n","print(data.info())\n","length = len(data.index)\n","print(\"\\nNow there are\", length , \"tweets total in this database\")\n","data.tail(10)\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'data = data.sample(frac=1)\\ndata.reset_index(drop = True, inplace = True)\\n\\ndata[\\'id\\'] = data.reset_index().index + 1\\nprint(data.label.value_counts(), \"\\n\")\\nprint(data.info())\\nlength = len(data.index)\\nprint(\"\\nNow there are\", length , \"tweets total in this database\")\\ndata.tail(10)'"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"SMgEcg_v4l7m","colab_type":"text"},"source":["# Training with both dev and training set. Then Testing with test set\n","<b>Loading in train and test data..."]},{"cell_type":"code","metadata":{"id":"c3KKifBM4kx5","colab_type":"code","outputId":"a510009e-035b-41fb-f954-a22031c84b84","executionInfo":{"status":"ok","timestamp":1580815063570,"user_tz":0,"elapsed":10294,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":145}},"source":["train = pd.read_csv('gs://csc3002/hateval2019/hateval2019_en_train.csv', sep=',',  index_col = False, encoding = 'utf-8')\n","dev =  pd.read_csv('gs://csc3002/hateval2019/hateval2019_en_dev.csv', sep=',',  index_col = False, encoding = 'utf-8')\n","loadData(train, dev)\n","\n","loadData(test)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Labels:\n"," 0    1739\n","1    1259\n","Name: label, dtype: int64 \n","\n","\n","There are 2998 tweets total in the training database\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZcNSY0kA8N4N","colab_type":"text"},"source":["<b>Function to get predictions on test data </b>"]},{"cell_type":"code","metadata":{"id":"mvpPwfj08PmW","colab_type":"code","colab":{}},"source":["def getPrediction(in_sentences):\n","\n","  labels = [0, 1]\n","  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n","  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n","  predictions = list(estimator.predict(predict_input_fn))\n","\n","  #Initialise empty predicted labels array\n","  predicted_classes = [None] * len(predictions)\n","\n","  #Use a for loop to iterate through probabilities and for each prediction assign a label\n","  #corresponding to which label has the highest probability\n","  for i in range(0, len(predictions)):\n","    if predictions[i]['probabilities'][0] > predictions[i]['probabilities'][1]:\n","      predicted_classes[i] = 0\n","    else:\n","      predicted_classes[i] = 1\n","  return predicted_classes"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rt4d5H3A6Iqu","colab_type":"text"},"source":["<b> Converting to features.... </b>"]},{"cell_type":"code","metadata":{"id":"Xf0Ofg3M6HBC","colab_type":"code","outputId":"11064084-4bb1-46b0-eced-8de966e53769","executionInfo":{"status":"ok","timestamp":1580816783695,"user_tz":0,"elapsed":881513,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 5\n","train_batch_size = 32\n","\n","SAVE_CHECKPOINTS_STEPS = 1000\n","run_config = tf.compat.v1.estimator.tpu.RunConfig(\n","    #I think the output file must be a sub-directory of the main BERT file\n","    model_dir=OUTPUT_DIR, \n","    cluster=cluster_resolver,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=100,\n","        num_shards=8,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","\n","train_InputExamples = data.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                      text_a = x[DATA_COLUMN], \n","                                                                      text_b = None, \n","                                                                      label = x[LABEL_COLUMN]), axis = 1)\n","\n","train_features =  bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","\n","\n","#Delete prior model graph, checkpoints and eval files to make room for new model each loop\n","try:\n","  tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","except:\n","# Doesn't matter if the directory didn't exist\n","  pass\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","# Compute # train and warmup steps from batch size\n","num_train_steps = int(len(train_features) / train_batch_size * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","# Model configs\n","model_fn = model_fn_builder(\n","bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","num_labels=len(label_list),\n","init_checkpoint=bert_ckpt_file,\n","learning_rate=LEARNING_RATE,\n","num_train_steps=num_train_steps,\n","num_warmup_steps=num_warmup_steps,\n","use_tpu=True,\n","use_one_hot_embeddings=True)\n","\n","estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","  use_tpu=True,\n","  model_fn=model_fn,\n","  config=run_config,\n","  train_batch_size=TRAIN_BATCH_SIZE,\n","  eval_batch_size=EVAL_BATCH_SIZE,\n","  predict_batch_size=PREDICT_BATCH_SIZE)\n","\n","# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = bert.run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=True)\n","\n","\n","\n","print(\"\\nThe model will stop training when it reaches\", num_train_steps, \"as a checkpoint\")\n","\n","print(f'Beginning Training!')\n","current_time = datetime.now()\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","train_time = datetime.now() - current_time\n","print(\"Training took time \", train_time)\n","\n","predictions = getPrediction(test.tweet)\n","test['predictions'] = predictions\n","\n","print(metrics.f1_score(test.label, test.predictions))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 9968\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] they are coming . and just because they made it here to america they think they have rights . des ##eased , une ##du ##cated and soon will be paid to be a loud ##mouth on the streets ! # build ##tha ##t ##wall america first [SEP]\n","INFO:tensorflow:input_ids: 101 2027 2024 2746 1012 1998 2074 2138 2027 2081 2009 2182 2000 2637 2027 2228 2027 2031 2916 1012 4078 25063 1010 16655 8566 12921 1998 2574 2097 2022 3825 2000 2022 1037 5189 14359 2006 1996 4534 999 1001 3857 8322 2102 9628 2637 2034 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] fuck you twice you pussy ass ni ##gga ##s [SEP]\n","INFO:tensorflow:input_ids: 101 6616 2017 3807 2017 22418 4632 9152 23033 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] want to be like you could be fucking working for him some day you fucking little sl ##ut [SEP]\n","INFO:tensorflow:input_ids: 101 2215 2000 2022 2066 2017 2071 2022 8239 2551 2005 2032 2070 2154 2017 8239 2210 22889 4904 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] i was going to ' like ' this comment then saw your profile pic of a bitch who attacked rape victims o [SEP]\n","INFO:tensorflow:input_ids: 101 1045 2001 2183 2000 1005 2066 1005 2023 7615 2059 2387 2115 6337 27263 1997 1037 7743 2040 4457 9040 5694 1051 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] want to speak down to my girl bitch then step up big nose ass bitch . saw that shit off . [SEP]\n","INFO:tensorflow:input_ids: 101 2215 2000 3713 2091 2000 2026 2611 7743 2059 3357 2039 2502 4451 4632 7743 1012 2387 2008 4485 2125 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f935319dd90>) includes params argument, but params are not passed to Estimator.\n","INFO:tensorflow:Using config: {'_model_dir': 'gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.28.171.74:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f934811e198>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.28.171.74:8470', '_evaluation_master': 'grpc://10.28.171.74:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f93ed61bfd0>}\n","INFO:tensorflow:_TPUContext: eval_on_tpu True\n","\n","The model will stop training when it reaches 1557 as a checkpoint\n","Beginning Training!\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.28.171.74:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 8278283442579639588)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15075832258415459024)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 9235653837146825852)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 1552753277433411770)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 7926628980454224206)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7487291541762347249)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 9835192356092401906)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15341246576360441729)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 11817745622014086124)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 17020960504450462042)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5733577045996660008)\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt.\n","INFO:tensorflow:Initialized dataset iterators in 2 seconds\n","INFO:tensorflow:Installing graceful shutdown hook.\n","INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 8 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Enqueue next (500) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (500) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:Outfeed finished for iteration (0, 310)\n","INFO:tensorflow:loss = 0.89259934, step = 500\n","INFO:tensorflow:Enqueue next (500) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (500) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (1, 40)\n","INFO:tensorflow:Outfeed finished for iteration (1, 350)\n","INFO:tensorflow:Saving checkpoints for 1000 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt.\n","INFO:tensorflow:loss = 0.6812393, step = 1000 (204.238 sec)\n","INFO:tensorflow:global_step/sec: 2.44811\n","INFO:tensorflow:examples/sec: 78.3396\n","INFO:tensorflow:Enqueue next (500) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (500) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (2, 0)\n","INFO:tensorflow:Outfeed finished for iteration (2, 310)\n","INFO:tensorflow:loss = 0.7263572, step = 1500 (101.714 sec)\n","INFO:tensorflow:global_step/sec: 4.91576\n","INFO:tensorflow:examples/sec: 157.304\n","INFO:tensorflow:Enqueue next (57) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (57) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 0.70303524, step = 1557 (15.098 sec)\n","INFO:tensorflow:global_step/sec: 3.77529\n","INFO:tensorflow:examples/sec: 120.809\n","INFO:tensorflow:Saving checkpoints for 1557 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt.\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Loss for final step: 0.70303524.\n","INFO:tensorflow:training_loop marked as finished\n","Training took time  0:12:18.750637\n","INFO:tensorflow:Writing example 0 of 2998\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: \n","INFO:tensorflow:tokens: [CLS] you & yo bitch some rat ##chet bonnie & clyde ho ##e [SEP]\n","INFO:tensorflow:input_ids: 101 2017 1004 10930 7743 2070 9350 20318 12220 1004 12085 7570 2063 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: \n","INFO:tensorflow:tokens: [CLS] you all ready to build that wall lock her up mag ##a red wave trump train . . . . . . . [SEP]\n","INFO:tensorflow:input_ids: 101 2017 2035 3201 2000 3857 2008 2813 5843 2014 2039 23848 2050 2417 4400 8398 3345 1012 1012 1012 1012 1012 1012 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: \n","INFO:tensorflow:tokens: [CLS] you are still stealing valor & credit from . without and his mini ##on leadership team ( you ) would have never reduced our taxes . the ac ##a has never been repealed as you promised , what is new ? bt ##w de ##port them all mag ##a for speaker . [SEP]\n","INFO:tensorflow:input_ids: 101 2017 2024 2145 11065 27314 1004 4923 2013 1012 2302 1998 2010 7163 2239 4105 2136 1006 2017 1007 2052 2031 2196 4359 2256 7773 1012 1996 9353 2050 2038 2196 2042 21492 2004 2017 5763 1010 2054 2003 2047 1029 18411 2860 2139 6442 2068 2035 23848 2050 2005 5882 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: \n","INFO:tensorflow:tokens: [CLS] ask this old whore to suicide or i can help this bitch die screaming . bb uk you guy ##z r shit to bring her back . [SEP]\n","INFO:tensorflow:input_ids: 101 3198 2023 2214 17219 2000 5920 2030 1045 2064 2393 2023 7743 3280 7491 1012 22861 2866 2017 3124 2480 1054 4485 2000 3288 2014 2067 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: \n","INFO:tensorflow:tokens: [CLS] plan to whore out my sexy bitch tomorrow night and look for hot tops . here is the fun ##nies ##t response i get face with tears of joy [SEP]\n","INFO:tensorflow:input_ids: 101 2933 2000 17219 2041 2026 7916 7743 4826 2305 1998 2298 2005 2980 13284 1012 2182 2003 1996 4569 15580 2102 3433 1045 2131 2227 2007 4000 1997 6569 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (1, 256)\n","INFO:tensorflow:  name = input_mask, shape = (1, 256)\n","INFO:tensorflow:  name = label_ids, shape = (1,)\n","INFO:tensorflow:  name = segment_ids, shape = (1, 256)\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt-1557\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 9 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Initialized dataset iterators in 0 seconds\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:prediction_loop marked as finished\n","INFO:tensorflow:prediction_loop marked as finished\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d83ZtdosKizK","colab_type":"code","outputId":"b794e86b-35b0-4636-f86e-32b52111f826","executionInfo":{"status":"ok","timestamp":1580823261489,"user_tz":0,"elapsed":11173,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["os.environ[\"GCLOUD_PROJECT\"] = \"csc3002\"\n","client = storage.Client()\n","bucket = client.bucket('csc3002')\n","\n","blobs = list(bucket.list_blobs(prefix=OUTPUT_DIR[13:])) # prefix needs to be the path of the dir you're trying to access in the bucket. Excluding 'gs://csc3002/'\n","for blob in blobs:\n","  if 'events' in blob.name:\n","    event_file = os.path.join('gs://csc3002/', blob.name)\n","  \n","lossList = []\n","for e in tf.train.summary_iterator(event_file):\n","    for v in e.summary.value:\n","      \n","      if v.tag == 'loss':\n","        print(v.simple_value)\n","        lossList.append(v.simple_value)\n","\n","print(OUTPUT_DIR)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fpDsa8ejf14-","colab_type":"text"},"source":["# Using Tensorboard to get deeper insight"]},{"cell_type":"code","metadata":{"id":"x3yg60YrO-Ub","colab_type":"code","outputId":"3dc15cb5-beb9-49bb-b6ee-56e53e91c5c5","executionInfo":{"status":"ok","timestamp":1580837354943,"user_tz":0,"elapsed":4206,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":254}},"source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip   #Downloads file to google drive"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-02-04 17:29:12--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 52.73.84.118, 52.3.157.51, 34.206.126.139, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|52.73.84.118|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13773305 (13M) [application/octet-stream]\n","Saving to: ‚Äòngrok-stable-linux-amd64.zip‚Äô\n","\n","ngrok-stable-linux- 100%[===================>]  13.13M  14.0MB/s    in 0.9s    \n","\n","2020-02-04 17:29:13 (14.0 MB/s) - ‚Äòngrok-stable-linux-amd64.zip‚Äô saved [13773305/13773305]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y97-_YvVXeIy","colab_type":"code","outputId":"153e7324-c089-485e-a5e1-1d82b734707d","executionInfo":{"status":"ok","timestamp":1580837361669,"user_tz":0,"elapsed":2209,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","def get_tensorboard(path_to_event_file = OUTPUT_DIR):\n","  get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 --reload_multifile=true &'\n",".format(path_to_event_file))\n","  \n","  get_ipython().system_raw('./ngrok http 6006 &')\n","\n","  !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","      \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n","\n","get_tensorboard(OUTPUT_DIR)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["http://2bc06d4c.ngrok.io\n"],"name":"stdout"}]}]}