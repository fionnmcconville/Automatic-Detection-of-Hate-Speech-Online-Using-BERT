{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HatEval_Tensorflow_TPU.ipynb","provenance":[{"file_id":"1EBH_dCvSIfTM-VlKeBfV2Em3exIJzkih","timestamp":1576806467871},{"file_id":"1JuDopUfWUBPOSoIOBuqs7S2tH2FJHokl","timestamp":1575402454368},{"file_id":"1VNukx0WgDZ6kdgs4gvDj6zZ5czaNCAO6","timestamp":1575299325999},{"file_id":"https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb","timestamp":1574879594425}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"j0a4mTk9o1Qg","colab_type":"code","colab":{}},"source":["# Copyright 2019 Google Inc.\n","\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Q4eUHbzUO-c","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/kpe/bert-for-tf2/blob/master/examples/tpu_movie_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"dCpvgG0vwXAZ","colab_type":"text"},"source":["#Detecting Hate speech Tweets with BERT"]},{"cell_type":"markdown","metadata":{"id":"xiYrZKaHwV81","colab_type":"text"},"source":["We are using bert-tensorflow for this classification task. At the moment I'm making sure it's tensorflow version 1.x because tensorflow version 2 gives issues with Bert at the moment. I believe Tensorflow hopes to have this issue resolved in tensorflow v 2.1\n","\n","We are using a TPU as a GPU does not have the required memory for Large BERT models- it can only cope with the base model. We'll see if there a TPU detected and we'll set it to a global environment variable so it can be accessed by our BERT functions later."]},{"cell_type":"code","metadata":{"id":"mYEcG2vlPumC","colab_type":"code","outputId":"a2cf83a3-ab70-4ddd-c678-9fae63f2da65","executionInfo":{"status":"ok","timestamp":1578955476222,"user_tz":0,"elapsed":16759,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":435}},"source":["%pip install bert-tensorflow\n","!pip install gcsfs\n","import pandas as pd\n","import numpy as np\n","from tensorflow.contrib.tpu.python.tpu import tpu_optimizer\n","\n","#Make sure to use tensorflow version 1.x, version 2 doesn't work with bert\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import os\n","\n","import html\n","import re\n","import json\n","import pprint\n","import random\n","import string\n","from datetime import datetime\n","\n","import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization\n","from bert import modeling\n","\n","assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n","TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","print('TPU address is', TPU_ADDRESS)\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","with tf.Session(TPU_ADDRESS) as session:\n","  # Upload credentials to TPU.\n","  with open('/content/adc.json', 'r') as f:\n","    auth_info = json.load(f)\n","  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n","\n","assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n","TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","print('TPU address is', TPU_ADDRESS)\n","USE_TPU=True\n","try:\n","  #tf.config.experimental_connect_to_host(TPU_WORKER)\n","  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n","    tpu=TPU_ADDRESS)\n","  tf.config.experimental_connect_to_cluster(cluster_resolver)\n","  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n","  tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n","except Exception as ex:\n","  print(ex)\n","  USE_TPU=False\n","\n","print(\"        USE_TPU:\", USE_TPU)\n","print(\"Eager Execution:\", tf.executing_eagerly())\n","\n","assert not tf.executing_eagerly(), \"Eager execution on TPUs have issues currently\"\n"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Requirement already satisfied: gcsfs in /usr/local/lib/python3.6/dist-packages (0.6.0)\n","Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.4.2)\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.21.0)\n","Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.7)\n","Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0.0)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.11.28)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n","TPU address is grpc://10.73.230.250:8470\n","TPU address is grpc://10.73.230.250:8470\n","        USE_TPU: True\n","Eager Execution: False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sRh9YYYfgUTE","colab_type":"text"},"source":["Setting a random seed for reproducability"]},{"cell_type":"code","metadata":{"id":"dgVteKeTgXrg","colab_type":"code","outputId":"a93f11fb-373c-431c-9d54-39bbeb6a5704","executionInfo":{"status":"ok","timestamp":1578955476225,"user_tz":0,"elapsed":16746,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tf.set_random_seed(3060)\n","print(\"Tensorflow Version:\", tf.__version__)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Tensorflow Version: 1.15.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KVB3eOcjxxm1","colab_type":"text"},"source":["Below we will set the directory where we will store our output model. To ensure the right variables are loaded in our run config function later, our ouput directory must be in the same directory as our pre-trained bert model directory.\n","\n","Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."]},{"cell_type":"code","metadata":{"id":"US_EAnICvP7f","colab_type":"code","cellView":"both","outputId":"1d766b7f-06ff-4a2d-f737-65b85c17e593","executionInfo":{"status":"ok","timestamp":1578955477894,"user_tz":0,"elapsed":18401,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#Choose which model you'd like -\n","#MUST be in GCP bucket\n","\n","#Bert uncased Large \n","#bert_model_name = 'uncased_L-24_H-1024_A-16' \n","\n","#Large whole word masking\n","bert_model_name = 'wwm_uncased_L-24_H-1024_A-16' \n","\n","#Adding further pretrained model\n","\"\"\"further_pretrained_model = \\\n","os.path.join(bert_model_name, 'further_pretrained_model')\n","\"\"\"\n","#Where we output the final, fine tuned model\n","output_dir = \\\n","os.path.join(bert_model_name, 'output')\n","\n","#@markdown Whether or not to clear/delete the directory and create a new one\n","DO_DELETE = True #@param {type:\"boolean\"}\n","#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","USE_BUCKET = True #@param {type:\"boolean\"}\n","BUCKET = 'csc3002' #@param {type:\"string\"}\n","\n","if USE_BUCKET:\n","  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, output_dir)\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","if DO_DELETE:\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","    # Doesn't matter if the directory didn't exist\n","    pass\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"],"execution_count":44,"outputs":[{"output_type":"stream","text":["***** Model output directory: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output *****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pmFYvkylMwXn","colab_type":"text"},"source":["#Data\n","I've stored the dataset in my google bucket for ease of access, authentication will have to be provided"]},{"cell_type":"code","metadata":{"id":"UvknR984WeDW","colab_type":"code","outputId":"10c209ad-4547-423f-a14b-abadab187bc4","executionInfo":{"status":"ok","timestamp":1578955493034,"user_tz":0,"elapsed":33526,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":234}},"source":["!gcloud config set project 'my-project-csc3002'\n","\n","data = 'gs://csc3002/hateval2019/hateval2019_en_train.csv'\n","data = pd.read_csv(data, sep=',',  index_col = False, encoding = 'utf-8')\n","data1 = 'gs://csc3002/hateval2019/hateval2019_en_dev.csv'\n","data1 = pd.read_csv(data1, sep=',',  index_col = False, encoding = 'utf-8')\n","frames = [data,data1]\n","data = pd.concat(frames)\n","data.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","cols = ['TR', 'AG']\n","data.drop(cols, inplace = True, axis = 1)\n","data = data.sample(frac=1)\n","data.reset_index(drop = True, inplace = True)\n","data.id = data.index\n","pd.set_option('display.max_colwidth', -1)\n","data.head()"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Updated property [core/project].\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>The woman who accused Nelly of rape last weekend is dropping her case. Should he get a public apology? https://t.co/iBw0SdxuPL</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Hysterical Hypocrite Liberals going after @ICEgov and Trump over 10 year old girl where were they when Obama deported 2 Million Hispanics ??</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Come work for @IRCEurope advocacting on behalf of refugees and migrants. Based in Tunis. DM for more details https://t.co/X3sEffRwf3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>#Kakuma Refugee Camp benefits from renewable sources of energy | Business Today  https://t.co/iRUrCSNzUH via @KTNNews @philkeits #WithRefugees @UNHCR_Kenya @refugeeaffairs @drckenya @NRC_HoA @tyrusmaina @imaana102</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Understanding the California Mind - American Greatness  https://t.co/ikZZPvG703#IllegalAlienInvasion#ProgressivePolicies #StopTheInvasion #UncheckedProgressivism</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  ... label\n","0  0   ...  0   \n","1  1   ...  0   \n","2  2   ...  0   \n","3  3   ...  0   \n","4  4   ...  1   \n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"sfRnHSz3iSXz","colab_type":"text"},"source":["We store the name of the Data column containing the text we wish to classify and the name of the corresponding label column in global variables for ease of access down line and also so this code is generalizable. Label list is just a 0 or a 1 because the version of BERT we've created below only deals in binary classifcation and labels must be ints"]},{"cell_type":"code","metadata":{"id":"G_GsYauDVpTB","colab_type":"code","outputId":"d8f4f787-da7c-4704-8bb2-9417122786ff","executionInfo":{"status":"ok","timestamp":1578955493036,"user_tz":0,"elapsed":33511,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":126}},"source":["print(data.label.value_counts(), \"\\n\")\n","print(\"\\nThere are\", len(data.index), \"tweets total in this database\")"],"execution_count":46,"outputs":[{"output_type":"stream","text":["0    5790\n","1    4210\n","Name: label, dtype: int64 \n","\n","\n","There are 10000 tweets total in this database\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7ttrJG8RvOBZ","colab_type":"text"},"source":["# Adding in data from ICVSM 2017\n","Reliably annotated dataset, very useful because like HatEval it includes tweets with terms like n**ger and f-g that might be associated with hate speech, but these terms aren't used in a hate speech context. I believe this is where my model fails most as where it fails is it's rate of false positives comes from\n","\n","<b>Although it's important to note that this HatEval dataset is hate speech targetting immigrants and women. ICVSM 2017 has a whole range of targets, not just women and immigrants and so possibly may not be beneficial towards improving performance because of this."]},{"cell_type":"code","metadata":{"id":"7Yhyx-nLyo7z","colab_type":"code","outputId":"fcca4f26-1ae3-49e0-8f72-c0e8f2a8e653","executionInfo":{"status":"ok","timestamp":1578955493038,"user_tz":0,"elapsed":33501,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["#This harms performance for now, whether it's adding the whole set or just the HS tweets.\n","#Maybe try again when you've another fine tuning model. Although  \n","\"\"\"sometweets = 'gs://csc3002/Raw_Data/ICVSM_2017.csv'\n","sometweets = pd.read_csv(sometweets, sep=',',  index_col = False, encoding = 'utf-8')\n","cols = ['Unnamed: 0', 'count','offensive_language','neither']\n","sometweets.drop(cols, inplace = True, axis = 1)\n","sometweets.reset_index(drop = True, inplace = True)\n","\n","sometweets.loc[sometweets['class'] == 0, 'label'] = 1\n","sometweets.loc[sometweets['class'] != 0, 'label'] = 0\n","sometweets.drop(['class'], inplace = True, axis = 1)\n","print(sometweets.label.value_counts())\n","\n","#Further refining this dataset. As I've demonstrated in the notebook where I inspected this data;\n","#there needs to be an agreeent of 3 or more annotators for it to qualify as hate speech to fit my description\n","sometweets = sometweets[sometweets.label == 1]\n","sometweets.label = sometweets.label.astype(int)\n","sometweets = sometweets[sometweets['hate_speech'] > 2]\n","print(sometweets.label.value_counts())\n","sometweets.head()\"\"\""],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"sometweets = 'gs://csc3002/Raw_Data/ICVSM_2017.csv'\\nsometweets = pd.read_csv(sometweets, sep=',',  index_col = False, encoding = 'utf-8')\\ncols = ['Unnamed: 0', 'count','offensive_language','neither']\\nsometweets.drop(cols, inplace = True, axis = 1)\\nsometweets.reset_index(drop = True, inplace = True)\\n\\nsometweets.loc[sometweets['class'] == 0, 'label'] = 1\\nsometweets.loc[sometweets['class'] != 0, 'label'] = 0\\nsometweets.drop(['class'], inplace = True, axis = 1)\\nprint(sometweets.label.value_counts())\\n\\n#Further refining this dataset. As I've demonstrated in the notebook where I inspected this data;\\n#there needs to be an agreeent of 3 or more annotators for it to qualify as hate speech to fit my description\\nsometweets = sometweets[sometweets.label == 1]\\nsometweets.label = sometweets.label.astype(int)\\nsometweets = sometweets[sometweets['hate_speech'] > 2]\\nprint(sometweets.label.value_counts())\\nsometweets.head()\""]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"InI98lGK0KTH","colab_type":"code","outputId":"a44ba8b0-4f65-4a97-95e8-b01890f4b3d5","executionInfo":{"status":"ok","timestamp":1578955493038,"user_tz":0,"elapsed":33452,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\"\"\"frames = [data,sometweets]\n","data = pd.concat(frames)\n","data = data.sample(frac=1)\n","data.reset_index(drop = True, inplace = True)\n","data.info()\"\"\""],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'frames = [data,sometweets]\\ndata = pd.concat(frames)\\ndata = data.sample(frac=1)\\ndata.reset_index(drop = True, inplace = True)\\ndata.info()'"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"Bb-voAQFxDSZ","colab_type":"text"},"source":["# Adding in augmented back-translated hate speech tweets as extra data\n","\n","We have very few instances of hate speech labelled in this dataset. To remedy this I performed back_translation augmentation on this training set.\n","\n","Below I load in in the extra hate speech tweets I created via back-translation augmentation I performed in another colab notebook and I append it to the existing dataframe"]},{"cell_type":"code","metadata":{"id":"DrHgPEFEyAPi","colab_type":"code","outputId":"273b0096-a7ad-4aeb-92e6-f30a2d5d52c4","executionInfo":{"status":"ok","timestamp":1578955493039,"user_tz":0,"elapsed":33442,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"dat = '/content/drive/My Drive/hateval2019/backtranslated_hatEval.txt' \n","dat = pd.read_csv(dat, sep = '\\t', names = ['tweet'], header = None, encoding = 'utf-8')\n","pd.set_option('display.max_colwidth', -1)\n","dat = dat.astype(str)\n","dat.head(50)\"\"\""],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"dat = '/content/drive/My Drive/hateval2019/backtranslated_hatEval.txt' \\ndat = pd.read_csv(dat, sep = '\\t', names = ['tweet'], header = None, encoding = 'utf-8')\\npd.set_option('display.max_colwidth', -1)\\ndat = dat.astype(str)\\ndat.head(50)\""]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"ykDag42wiRyc","colab_type":"text"},"source":["**Rather than creating 3768 extra tweets, 19630 extra have been created. The tweets have been incorrectly parsed. Removing some tweets with a smaller length may mitigate this effect somewhat by removing tweets that were cut in half**"]},{"cell_type":"code","metadata":{"id":"IuLgQwa7otjY","colab_type":"code","outputId":"0a8cbc02-6758-407c-fbfc-3dc23a23a1de","executionInfo":{"status":"ok","timestamp":1578955493039,"user_tz":0,"elapsed":33431,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\"\"\"print(\"There are\", len(dat.index), \"tweets\")\n","dat = dat[dat['tweet'].apply(lambda x: len(x) > 10)]\n","print(\"There are now\", len(dat.index), \"tweets\")\"\"\""],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'print(\"There are\", len(dat.index), \"tweets\")\\ndat = dat[dat[\\'tweet\\'].apply(lambda x: len(x) > 10)]\\nprint(\"There are now\", len(dat.index), \"tweets\")'"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"uddU6otlowF9","colab_type":"text"},"source":["ok didn't help much. Let's see if it improves our validation anyways"]},{"cell_type":"markdown","metadata":{"id":"Tv33kW_by4Ey","colab_type":"text"},"source":["**See how the english is a little off?** \n","\n","That's because these are the hate speech tweets in the training set translated to french, then translated back again. This creates a whole new, yet similar set of hate speech tweets to train on. (Slightly augmented text)"]},{"cell_type":"code","metadata":{"id":"kdKh1mw7zZTn","colab_type":"code","outputId":"553c635e-3e5a-41a1-c6f6-94a811bea288","executionInfo":{"status":"ok","timestamp":1578955493040,"user_tz":0,"elapsed":33422,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\"\"\"dat['label'] = 1\n","dat['id'] = 80000\n","frames = [dat,data]\n","data = pd.concat(frames)\n","print(data.info())\n","data.head()\"\"\""],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"dat['label'] = 1\\ndat['id'] = 80000\\nframes = [dat,data]\\ndata = pd.concat(frames)\\nprint(data.info())\\ndata.head()\""]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"Kenyh_Ci1LDZ","colab_type":"text"},"source":["We'll shuffle the dataframe to make sure there's no funny business with the training of the model and we'll then reset the id field to make it unique and sequential for each row"]},{"cell_type":"code","metadata":{"id":"6LUYVIBI1Jus","colab_type":"code","outputId":"493caf06-85d6-47ec-a18d-34946a2edc77","executionInfo":{"status":"ok","timestamp":1578955493040,"user_tz":0,"elapsed":33410,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"data = data.sample(frac=1)\n","data.reset_index(drop = True, inplace = True)\n","\n","data['id'] = data.reset_index().index + 1\n","print(data.label.value_counts(), \"\\n\")\n","print(data.info())\n","length = len(data.index)\n","print(\"\\nNow there are\", length , \"tweets total in this database\")\n","data.tail(10)\"\"\""],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'data = data.sample(frac=1)\\ndata.reset_index(drop = True, inplace = True)\\n\\ndata[\\'id\\'] = data.reset_index().index + 1\\nprint(data.label.value_counts(), \"\\n\")\\nprint(data.info())\\nlength = len(data.index)\\nprint(\"\\nNow there are\", length , \"tweets total in this database\")\\ndata.tail(10)'"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"v7jzuGG3dSJq","colab_type":"text"},"source":["# Text Pre-Processing\n","Below is our custom preprocess function which performs simple text preprocessing of the tweets. It's functions are explained in the method"]},{"cell_type":"code","metadata":{"id":"dXj3fJZLlM0q","colab_type":"code","outputId":"0d360ee6-d7f8-4e7f-b7db-96b47b9ea5bf","executionInfo":{"status":"ok","timestamp":1578955493041,"user_tz":0,"elapsed":33399,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["#The below function translates emojis to text\n","\"\"\"\n","%cd '/content/drive/My Drive'\n","%pip install demoji-0.1.5-py3-none-any.whl\n","\n","import demoji\n","demoji.download_codes()\n","def emojiReplace(text_string):\n","    \n","    emoji_dict = demoji.findall(text_string)\n","    for emoji in emoji_dict.keys():\n","        text_string = text_string.replace(emoji, ' '+  emoji_dict[emoji])\n","    \n","    return text_string\n","\n","#demoji.replace(preprocess(testtweet1), repl = )\n","testtweet1 = data.loc[4521]\n","testtweet2 = data.loc[4549]\n","\n","print(\"\\nOriginal:\", testtweet1['tweet'])\n","print('Label:', testtweet1['label'])\n","print(\"\\nPreprocessed:\", emojiReplace(testtweet1['tweet']))\n","\n","print(\"\\nOriginal:\", testtweet2['tweet'])\n","print('Label:', testtweet2['label'])\n","print(\"\\nPreprocessed:\", emojiReplace(testtweet2['tweet']))\"\"\""],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n%cd \\'/content/drive/My Drive\\'\\n%pip install demoji-0.1.5-py3-none-any.whl\\n\\nimport demoji\\ndemoji.download_codes()\\ndef emojiReplace(text_string):\\n    \\n    emoji_dict = demoji.findall(text_string)\\n    for emoji in emoji_dict.keys():\\n        text_string = text_string.replace(emoji, \\' \\'+  emoji_dict[emoji])\\n    \\n    return text_string\\n\\n#demoji.replace(preprocess(testtweet1), repl = )\\ntesttweet1 = data.loc[4521]\\ntesttweet2 = data.loc[4549]\\n\\nprint(\"\\nOriginal:\", testtweet1[\\'tweet\\'])\\nprint(\\'Label:\\', testtweet1[\\'label\\'])\\nprint(\"\\nPreprocessed:\", emojiReplace(testtweet1[\\'tweet\\']))\\n\\nprint(\"\\nOriginal:\", testtweet2[\\'tweet\\'])\\nprint(\\'Label:\\', testtweet2[\\'label\\'])\\nprint(\"\\nPreprocessed:\", emojiReplace(testtweet2[\\'tweet\\']))'"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"7cd_P1rZH3tb","colab_type":"text"},"source":["The contraction mapping below is not perfect. There are many ambigious contractions which are impossible to definitively resolve (e.g. he's - he has or he is).\n","\n","The mappings below are unambigious but they are mapped to the most likely contractions, we specifically choose flashtext as our library of choice for text replacement purposes because of it’s execution speed."]},{"cell_type":"code","metadata":{"id":"ZAoLqorwH3bE","colab_type":"code","outputId":"c3afaf39-58b4-44c0-f890-e348c189fa03","executionInfo":{"status":"ok","timestamp":1578955495579,"user_tz":0,"elapsed":35923,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!pip install flashtext\n","\n","from flashtext import KeywordProcessor\n","contraction_mapping = { \n","\"ain't\": \"is not\",\"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\",\n","\"'cause\": \"because\",\"cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n","\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\n","\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n","\"haven't\": \"have not\", \"he'd\": \"he would\",\"he'd've\": \"he would have\", \"he'll\": \"he will\",\n","\"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'll\": \"how will\",\n","\"how's\": \"how is \", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n","\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n","\"it'd\": \"it had\", \"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\",\n","\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\"mayn't\": \"may not\",\n","\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n","\"must've\": \"must have\", \"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n","\"needn't\": \"need not\", \"needn't've\": \"need not have\",\"oughtn't\": \"ought not\",\n","\"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n","\"shan't've\": \"shall not have\",\"she'd\": \"she had\",\"she'd've\": \"she would have\",\n","\"she'll\": \"she will\", \"she'll've\": \"she will have\",\"she's\": \"she is\",\n","\"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n","\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\n","\"that's\": \"that is\", \"there'd\": \"there had\",\"there'd've\": \"there would have\",\n","\"there's\": \"there is\",\"they'd\": \"they had\", \"they'd've\": \"they would have\",\n","\"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n","\"they've\": \"they have\", \"to've\": \"to have\",\"wasn't\": \"was not\",\n","\"we'd\": \"we would\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n","\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\n","\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\n","\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\n","\"where's\": \"where has\",\"where've\": \"where have\",\"who'll\": \"who will\",\n","\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n","\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n","\"would've\": \"would have\",\"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n","\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n","\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\n","\"you'd've\": \"you would have\",\"you'll\": \"you will\", \"you'll've\": \"you will have\",\n","\"you're\": \"you are\", \"you've\": \"you have\"}\n","\n","keyword_processor = KeywordProcessor() #initialise\n","for k, v in contraction_mapping.items():\n","  keyword_processor.add_keyword(k,v)"],"execution_count":54,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: flashtext in /usr/local/lib/python3.6/dist-packages (2.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U7Lh1I1qdR20","colab_type":"code","colab":{}},"source":["def preprocess(text_string):\n","    \"\"\"\n","    Accepts a text string and:\n","    1) Removes URLS\n","    2) lots of whitespace with one instance\n","    3) Removes mentions\n","    4) Uses the html.unescape() method to convert unicode to text counterpart\n","    5) Replace & with and\n","    6) Remove the fact the tweet is a retweet if it is - knowing the tweet is \n","       a retweet does not help towards our classification task.\n","    This allows us to get standardized counts of urls and mentions\n","    Without caring about specific people mentioned\n","    \"\"\"\n","    space_pattern = '\\s+'\n","    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[#$-_@.&+]|'\n","        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n","    mention_regex = '@[\\w\\-]+:'\n","    mention_regex1 = '@[\\w\\-]+'\n","    RT_regex = '(RT|rt)[ ]*@[ ]*[\\S]+'\n","    \n","    # Replaces urls with URL\n","    parsed_text = re.sub(giant_url_regex, '', text_string)\n","    parsed_text = re.sub('URL', '', parsed_text)\n","    \n","    # Remove the fact the tweet is a retweet. \n","    # (we're only interested in the language of the tweet here)\n","    parsed_text = re.sub(RT_regex, ' ', parsed_text) \n","    \n","    # Removes mentions as they're redundant information\n","    parsed_text = re.sub(mention_regex, '',  parsed_text)\n","    #including ones with semi-colons after - this seems to come up often\n","    parsed_text = re.sub(mention_regex1, '',  parsed_text)  \n","\n","    #Remove unicode\n","    parsed_text = re.sub(r'[^\\x00-\\x7F]','', parsed_text) \n","    parsed_text = re.sub(r'&#[0-9]+;', '', parsed_text)  \n","\n","    # Convert unicode missed by regex to text\n","   #parsed_text = html.unescape(parsed_text)\n","\n","    #Remove excess whitespace at the end\n","    parsed_text = re.sub(space_pattern, ' ', parsed_text) \n","    \n","    #Set text to lowercase and strip\n","    parsed_text = parsed_text.lower()\n","    parsed_text = parsed_text.strip()\n","    #parsed_text = keyword_processor.replace_keywords(parsed_text)\n","    \n","    return parsed_text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pVOEaAtad7Nh","colab_type":"text"},"source":["**Let's see what it looks like**"]},{"cell_type":"code","metadata":{"id":"gzRWRFtHeAcp","colab_type":"code","outputId":"5b7a6418-7509-49eb-a140-d0a3ae64bcd2","executionInfo":{"status":"ok","timestamp":1578955495755,"user_tz":0,"elapsed":36077,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":237}},"source":["testtweet = data.loc[2100]\n","print(\"Original:\", testtweet['tweet'])\n","print(\"Preprocessed:\", preprocess(testtweet['tweet']))\n","print('Label:', testtweet['label'])\n","\n","testtweet1 = data.loc[4521]\n","print(\"\\nOriginal:\", testtweet1['tweet'])\n","print(\"Preprocessed:\", preprocess(testtweet1['tweet']))\n","print('Label:', testtweet1['label'])\n","\n","testtweet2 = data.loc[4549]\n","print(\"\\nOriginal:\", testtweet2['tweet'])\n","print(\"Preprocessed:\", preprocess(testtweet2['tweet']))\n","print('Label:', testtweet2['label'])\n","\n","data['tweet'] = data['tweet'].apply(preprocess)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["Original: Get over yourselves ladies... You're wrong.... all the time!\n","Preprocessed: get over yourselves ladies... you're wrong.... all the time!\n","Label: 1\n","\n","Original: They keep attacking us because we're not allowed 2 stop them. We let them take over regions here & bring sharia to some of our cities. Mosque's were unheard of & we never had to bow to Muslims before 9/11. That's what I'll #NeverForgetKick them the hell out & #BUILDTHATWALL\n","Preprocessed: they keep attacking us because we're not allowed 2 stop them. we let them take over regions here & bring sharia to some of our cities. mosque's were unheard of & we never had to bow to muslims before 9/11. that's what i'll #neverforgetkick them the hell out & #buildthatwall\n","Label: 1\n","\n","Original: SHUT. IT. DOWN.#BorderWall #BuildThatWall #KAG\n","Preprocessed: shut. it. down.#borderwall #buildthatwall #kag\n","Label: 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UhtlxArU2-yp","colab_type":"text"},"source":[" **In training, let's remove any tweets that have a length less than 10. They could skew our model**"]},{"cell_type":"code","metadata":{"id":"CVjM8yv8299W","colab_type":"code","outputId":"e57c4ba8-c1f4-4a05-9eae-d0e854316f42","executionInfo":{"status":"ok","timestamp":1578955495756,"user_tz":0,"elapsed":36065,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"source":["length = len(data.index)\n","data = data[data['tweet'].apply(lambda x: len(x) > 10)]\n","#data = data[data['tweet'].apply(lambda x: len(x) < 300)]\n","print(length - len(data.index), \"tweets have been reomved from the dataframe\\n\")\n","data = data.sample(frac=1)\n","data.reset_index(drop = True, inplace = True)\n","data.id = data.index\n","data.info()"],"execution_count":57,"outputs":[{"output_type":"stream","text":["49 tweets have been reomved from the dataframe\n","\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 9951 entries, 0 to 9950\n","Data columns (total 3 columns):\n","id       9951 non-null int64\n","tweet    9951 non-null object\n","label    9951 non-null int64\n","dtypes: int64(2), object(1)\n","memory usage: 233.4+ KB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6uMuDR0pqChY","colab_type":"text"},"source":["# Splitting data into train and dev. Also specifying label and text columns"]},{"cell_type":"code","metadata":{"id":"IuMOGwFui4it","colab_type":"code","colab":{}},"source":["#The test data provided is supervised. So we can use the evaluate function of estimator\n","#to get a real approximation of our true f score with respect to the competition\n","train = data\n","test = 'gs://csc3002/hateval2019/hateval2019_en_test.csv'\n","test =  pd.read_csv(test, sep=',',  index_col = False, encoding = 'utf-8')\n","test.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","test['tweet'] = test['tweet'].apply(preprocess)\n","\n","DATA_COLUMN = 'tweet'\n","LABEL_COLUMN = 'label'\n","# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = [0, 1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V399W0rqNJ-Z","colab_type":"text"},"source":["#Data Preprocessing\n","We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n","\n","- `text_a` is the text we want to classify, which in this case, is the `tweet` field in our Dataframe. \n","- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n","- `label` is the label for our example, i.e. True, False"]},{"cell_type":"code","metadata":{"id":"p9gEt5SmM6i6","colab_type":"code","colab":{}},"source":["# Use the InputExample class from BERT's run_classifier code to create examples from the data\n","train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)\n","\n","test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCZWZtKxObjh","colab_type":"text"},"source":["Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n","\n","\n","1. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n","2. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n","3. Map our words to indexes using a vocab file that BERT provides\n","4. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n","5. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n","\n","Happily, we don't have to worry about most of these details. It's automated with the below inbuilt functions\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Aws4Q_SXZENG","colab_type":"text"},"source":["Below is a way to retrieve desired BERT parameters, such as it's pre-trained checkpoints and it's vocab file, from my google storage bucket where I've downloaded the uncased LARGE version of bert."]},{"cell_type":"code","metadata":{"id":"UtZavIhEaWF5","colab_type":"code","outputId":"4d416f7c-7576-454b-be9a-85d2090594ad","executionInfo":{"status":"ok","timestamp":1578955497639,"user_tz":0,"elapsed":37916,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":254}},"source":["bucket_dir = 'gs://csc3002'\n","\n","bert_ckpt_dir = os.path.join(bucket_dir, bert_model_name) \n","\n","#For normal model\n","bert_ckpt_file   = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n","print(\"Using BERT checkpoint from:\", bert_ckpt_dir)\n","\n","#For further pretrained model\n","#bert_ckpt_file = tf.train.latest_checkpoint(further_pretrained_model)\n","#print(\"Using BERT checkpoint from:\", further_pretrained_model)\n","\n","bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")\n","vocab_file = os.path.join(bert_ckpt_dir, \"vocab.txt\")\n","\n","\n","tokenizer = bert.tokenization.FullTokenizer(vocab_file=vocab_file)\n","tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"],"execution_count":60,"outputs":[{"output_type":"stream","text":["Using BERT checkpoint from: gs://csc3002/wwm_uncased_L-24_H-1024_A-16\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['this',\n"," 'here',\n"," \"'\",\n"," 's',\n"," 'an',\n"," 'example',\n"," 'of',\n"," 'using',\n"," 'the',\n"," 'bert',\n"," 'token',\n"," '##izer']"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"0OEzfFIt6GIc","colab_type":"text"},"source":["Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."]},{"cell_type":"code","metadata":{"id":"LL5W8gEGRTAf","colab_type":"code","colab":{}},"source":["# BERT is limited to 512 tokens in length\n","MAX_SEQ_LENGTH = 256\n","# Convert our train and test features to InputFeatures that BERT understands.\n","train_features =  bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ccp5trMwRtmr","colab_type":"text"},"source":["#Creating a model\n","\n","Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. It loads the configs of the BERT model we specified earlier and it creates a single layer that will be trained to adapt BERT to our task (i.e. classifying whether a tweet is hate speech or not). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."]},{"cell_type":"code","metadata":{"id":"0_w7hsO8c2A_","colab_type":"code","colab":{}},"source":["def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n","                 labels, num_labels, use_one_hot_embeddings):\n","  \"\"\"Creates a classification model.\"\"\"\n","  model = modeling.BertModel(\n","      config=bert_config,\n","      is_training=is_training,\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      token_type_ids=segment_ids,\n","      use_one_hot_embeddings=use_one_hot_embeddings)\n","\n","  # In the demo, we are doing a simple classification task on the entire\n","  # segment.\n","  #\n","  # If you want to use the token-level output, use model.get_sequence_output()\n","  # instead.\n","  output_layer = model.get_pooled_output()\n","\n","  hidden_size = output_layer.shape[-1].value\n","\n","  output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","\n","  output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","  with tf.variable_scope(\"loss\"):\n","    if is_training:\n","      # I.e., 0.1 dropout\n","      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","\n","    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, output_bias)\n","    probabilities = tf.nn.softmax(logits, axis=-1)\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","    loss = tf.reduce_mean(per_example_loss)\n","\n","    return (loss, per_example_loss, logits, probabilities)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-gbOrzvodL6j","colab_type":"code","colab":{}},"source":["def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n","                     num_train_steps, num_warmup_steps, use_tpu,\n","                     use_one_hot_embeddings):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","\n","  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    tf.logging.info(\"*** Features ***\")\n","    for name in sorted(features.keys()):\n","      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n","\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","    is_real_example = None\n","    if \"is_real_example\" in features:\n","      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n","    else:\n","      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n","\n","    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n","\n","    (total_loss, per_example_loss, logits, probabilities) = create_model(\n","        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n","        num_labels, use_one_hot_embeddings)\n","\n","    tvars = tf.trainable_variables()\n","    initialized_variable_names = {}\n","    scaffold_fn = None\n","    if init_checkpoint:\n","      (assignment_map, initialized_variable_names\n","      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n","      if use_tpu:\n","\n","        def tpu_scaffold():\n","          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n","          return tf.train.Scaffold()\n","\n","        scaffold_fn = tpu_scaffold\n","      else:\n","        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n","\n","    \"\"\" tf.logging.info(\"**** Trainable Variables ****\")\n","    for var in tvars:\n","      init_string = \"\"\n","      if var.name in initialized_variable_names:\n","        init_string = \", *INIT_FROM_CKPT*\"\n","      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n","                      init_string)\"\"\"\n","\n","    output_spec = None\n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","\n","      train_op = optimization.create_optimizer(\n","          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n","\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          loss=total_loss,\n","          train_op=train_op,\n","          scaffold_fn=scaffold_fn)\n","    elif mode == tf.estimator.ModeKeys.EVAL:\n","\n","      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n","        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n","\n","        accuracy = tf.metrics.accuracy(labels=label_ids, predictions=predictions, weights=is_real_example)\n","        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n","        f1_score = tf.contrib.metrics.f1_score(label_ids, predictions)\n","        auc = tf.metrics.auc( label_ids, predictions)\n","        recall = tf.metrics.recall(label_ids, predictions)\n","        precision = tf.metrics.precision(label_ids, predictions)\n","        true_pos = tf.metrics.true_positives(label_ids, predictions)\n","        true_neg = tf.metrics.true_negatives(label_ids, predictions)\n","        false_pos = tf.metrics.false_positives(label_ids, predictions)  \n","        false_neg = tf.metrics.false_negatives(label_ids, predictions)\n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"eval_loss\": loss,\n","            \"F1_Score\": f1_score,\n","            \"auc\": auc,\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"true_positives\": true_pos,\n","            \"true_negatives\": true_neg,\n","            \"false_positives\": false_pos,\n","            \"false_negatives\": false_neg\n","        }\n","\n","      eval_metrics = (metric_fn, [per_example_loss, label_ids, logits, is_real_example])\n","\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          loss=total_loss,\n","          eval_metrics=eval_metrics,\n","          scaffold_fn=scaffold_fn)\n","    else:\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          predictions={\"probabilities\": probabilities},\n","          scaffold_fn=scaffold_fn)\n","    return output_spec\n","\n","  return model_fn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qpE0ZIDOCQzE","colab_type":"text"},"source":["Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."]},{"cell_type":"markdown","metadata":{"id":"ZY_71Lwmxwm1","colab_type":"text"},"source":["#Model and Config Parameters"]},{"cell_type":"code","metadata":{"id":"OjwJ4bTeWXD8","colab_type":"code","colab":{}},"source":["# Compute train and warmup steps from batch size\n","# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n","TRAIN_BATCH_SIZE = 32 #recommended 16 or 32\n","EVAL_BATCH_SIZE = 8\n","PREDICT_BATCH_SIZE = 8\n","LEARNING_RATE = 2.5e-5 # Recommended 5e-5, 3e-5 or 2e-5\n","NUM_TRAIN_EPOCHS = 4.0 # Recommended 2, 3 or 4\n","MAX_SEQ_LENGTH = 256\n","# Warmup is a period of time where the learning rate \n","#is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","\n","# Compute # train and warmup steps from batch size\n","num_train_steps = int(len(train_features) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 1000\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"emHf9GhfWBZ_","colab_type":"code","outputId":"8be0ba0c-42b4-4f7c-e3fe-727e2cda5853","executionInfo":{"status":"ok","timestamp":1578955503461,"user_tz":0,"elapsed":43668,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":108}},"source":["print(\"The model will stop training when it reaches\", num_train_steps, \"as a checkpoint\")\n","print(\"\\nThe bert checkpoint directory is\", bert_ckpt_dir)\n","print(\"\\nThe output directory is\", OUTPUT_DIR)"],"execution_count":65,"outputs":[{"output_type":"stream","text":["The model will stop training when it reaches 1243 as a checkpoint\n","\n","The bert checkpoint directory is gs://csc3002/wwm_uncased_L-24_H-1024_A-16\n","\n","The output directory is gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"85uQCSdHafDN","colab_type":"text"},"source":["**Below are TPU model functions**"]},{"cell_type":"code","metadata":{"id":"q_WebpS1X97v","colab_type":"code","colab":{}},"source":["\n","#cluster = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n","\n","run_config = tf.compat.v1.estimator.tpu.RunConfig(\n","    #I think the output file must be a sub-directory of the main BERT file\n","    model_dir=OUTPUT_DIR, \n","    cluster=cluster_resolver,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=500,\n","        num_shards=8,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","\n","    \n","model_fn = model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  use_one_hot_embeddings=True)\n","\n","estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","    use_tpu=True,\n","    model_fn=model_fn,\n","    config=run_config,\n","    train_batch_size=TRAIN_BATCH_SIZE,\n","    eval_batch_size=EVAL_BATCH_SIZE,\n","    predict_batch_size=PREDICT_BATCH_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NOO3RfG1DYLo","colab_type":"text"},"source":["Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."]},{"cell_type":"code","metadata":{"id":"1Pv2bAlOX_-K","colab_type":"code","colab":{}},"source":["# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = bert.run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=True)\n","\n","#input function for test data, we feed in our previously created test_features for this\n","test_input_fn = run_classifier.input_fn_builder(\n","    features=test_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JhCEARCqh2AW","colab_type":"text"},"source":["# HyperParameter Grid Search\n","Can also just be used to train and evaluate on single parameters"]},{"cell_type":"code","metadata":{"id":"kpamy3TlcoOF","colab_type":"code","outputId":"26b0adb3-79f8-4329-b622-3907d4df1786","executionInfo":{"status":"error","timestamp":1577464245799,"user_tz":0,"elapsed":2404,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":435}},"source":["from itertools import product\n","from tensorflow.python.summary.summary_iterator import summary_iterator\n","from google.cloud import storage\n","\n","#Filter out all log messages so console isn't consumed with memory\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","\"\"\"FIXED MODEL PARAMS\"\"\"\n","TRAIN_BATCH_SIZE = 32 #recommended 16 or 32\n","EVAL_BATCH_SIZE = 8\n","PREDICT_BATCH_SIZE = 8\n","MAX_SEQ_LENGTH = 256\n","# Warmup is a period of time where the learning rate \n","#is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","\n","cluster = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n","\n","#Run config which will stay constant throughout\n","run_config = tf.compat.v1.estimator.tpu.RunConfig(\n","    #I think the output file must be a sub-directory of the main BERT file\n","    model_dir=OUTPUT_DIR, \n","    cluster=cluster,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=500,\n","        num_shards=8,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","\n","#Dataframe where grid search results will be stored. Empty to begin with\n","eval_df = pd.DataFrame(columns = ['F1 Score', 'precision', 'false_positives'] )\n","\n","#Param range\n","batch_size = [32]\n","lr_values = [1.75e-5, 2e-5]\n","num_epochs_values = [3.0, 5.0, 7.0]\n","for TRAIN_BATCH_SIZE, NUM_TRAIN_EPOCHS, LEARNING_RATE in product(batch_size, num_epochs_values, lr_values):\n","\n","  #Delete prior model graph, checkpoints and eval files to make room for new model each loop\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","  # Doesn't matter if the directory didn't exist\n","    pass\n","  tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","  # Compute # train and warmup steps from batch size\n","  num_train_steps = int(len(train_features) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","  num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","  # Model configs\n","  model_fn = model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  use_one_hot_embeddings=True)\n","\n","  estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","    use_tpu=True,\n","    model_fn=model_fn,\n","    config=run_config,\n","    train_batch_size=TRAIN_BATCH_SIZE,\n","    eval_batch_size=EVAL_BATCH_SIZE,\n","    predict_batch_size=PREDICT_BATCH_SIZE)\n","  \n","  # Create an input function for training. drop_remainder = True for using TPUs.\n","  train_input_fn = bert.run_classifier.input_fn_builder(\n","      features=train_features,\n","      seq_length=MAX_SEQ_LENGTH,\n","      is_training=True,\n","      drop_remainder=True)\n","\n","  #input function for test data, we feed in our previously created test_features for this\n","  test_input_fn = run_classifier.input_fn_builder(\n","      features=test_features,\n","      seq_length=MAX_SEQ_LENGTH,\n","      is_training=False,\n","      drop_remainder=True)\n","  \n","  print(\"\\nThe model will stop training when it reaches\", num_train_steps, \"as a checkpoint\")\n","\n","  print(f'Beginning Training!')\n","  current_time = datetime.now()\n","  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","  train_time = datetime.now() - current_time\n","  print(\"Training took time \", train_time)\n","\n","  #You need to provide number of steps for a TPU\n","  eval_steps = int(len(test_InputExamples) / EVAL_BATCH_SIZE)\n","\n","  #Eval will be slightly WRONG on the TPU because it will truncate the last batch.\n","  estimator.evaluate(input_fn=test_input_fn, steps=eval_steps)\n","\n","  os.environ[\"GCLOUD_PROJECT\"] = \"csc3002\"\n","  client = storage.Client()\n","  bucket = client.bucket('csc3002')\n","\n","  blobs = list(bucket.list_blobs(prefix='wwm_uncased_L-24_H-1024_A-16/output/eval'))\n","  for blob in blobs:\n","    if 'events' in blob.name:\n","      eval_dir = os.path.join('gs://csc3002/', blob.name)\n","        \n","  for e in tf.train.summary_iterator(eval_dir):\n","      for v in e.summary.value:\n","        if v.tag == 'F1_Score':\n","          fscore = v.simple_value\n","        \n","        if v.tag == 'auc':\n","          auc = v.simple_value\n","        \n","        if v.tag == 'eval_accuracy':\n","          accuracy = v.simple_value\n","        \n","        if v.tag == 'recall':\n","          recall = v.simple_value\n","\n","        if v.tag == 'precision':\n","          precision = v.simple_value\n","        \n","        if v.tag == 'false_positives':\n","          false_positives = v.simple_value\n","\n","        if v.tag == 'false_negatives':\n","          false_negatives = v.simple_value\n","\n","        if v.tag == 'true_positives':\n","          true_positives = v.simple_value\n","\n","        if v.tag == 'true_negatives':\n","          true_negatives = v.simple_value\n","\n","  ind = str(TRAIN_BATCH_SIZE) + '__' + str(LEARNING_RATE) + '__' + str(NUM_TRAIN_EPOCHS) #String representation denoting model configs\n","  row = pd.Series({'F1 Score': fscore, 'auc': auc, 'eval_accuracy': accuracy,'precision': precision,'recall': recall,\\\n","                                  'false_negatives': false_negatives,'false_positives': false_positives,\\\n","                   'true_negatives':true_negatives ,'true_positives': true_positives, 'Training Time': train_time }, name = ind)\n","  \n","  eval_df = eval_df.append(row)\n","  print(ind, \"F-Score:\", fscore)\n","#We'll save each time inside the loop, so if the program crashes after overuse of TPU\n","#we still have eval_results for many hyperparam combos\n","  eval_df.to_csv('gs://csc3002/hateval2019/eval_df.csv', sep=',', index = True)\n","#Loop ends. Save eval csv and print to console\n","eval_df.to_csv('gs://csc3002/hateval2019/eval_df.csv', sep=',', index = True)\n","eval_df"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","The model will stop training when it reaches 932 as a checkpoint\n","Beginning Training!\n","Training took time  0:09:13.011973\n","32__1.75e-05__3.0 F-Score: 0.6399791836738586\n","\n","The model will stop training when it reaches 932 as a checkpoint\n","Beginning Training!\n","Training took time  0:09:26.509635\n","32__2e-05__3.0 F-Score: 0.6249999403953552\n","\n","The model will stop training when it reaches 1554 as a checkpoint\n","Beginning Training!\n","Training took time  0:13:32.513474\n","32__1.75e-05__5.0 F-Score: 0.6331866383552551\n","\n","The model will stop training when it reaches 1554 as a checkpoint\n","Beginning Training!\n","Training took time  0:13:16.818867\n","32__2e-05__5.0 F-Score: 0.6365535259246826\n","\n","The model will stop training when it reaches 2176 as a checkpoint\n","Beginning Training!\n"],"name":"stdout"}]}]}