{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing for tweets\n",
    "\n",
    "<b>Below I will develop and demonstrate the functions I have created to preprocess the text in my tweets </b>\n",
    "\n",
    "<i>Importing and installing dependencies </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordsegment in c:\\users\\fionn\\anaconda3\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: demoji==0.1.5 from file:///C:/Users/fionn/Documents/CSC3002_Project/CSC3002_Detecting_Hate_Speech_On_Social_Media/csc3002_detecting_hate_speech/demoji-0.1.5-py3-none-any.whl in c:\\users\\fionn\\anaconda3\\lib\\site-packages (0.1.5)\n",
      "Requirement already satisfied: requests<3.0.0 in c:\\users\\fionn\\anaconda3\\lib\\site-packages (from demoji==0.1.5) (2.22.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\fionn\\anaconda3\\lib\\site-packages (from demoji==0.1.5) (44.0.0.post20200106)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\fionn\\anaconda3\\lib\\site-packages (from requests<3.0.0->demoji==0.1.5) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\fionn\\anaconda3\\lib\\site-packages (from requests<3.0.0->demoji==0.1.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fionn\\anaconda3\\lib\\site-packages (from requests<3.0.0->demoji==0.1.5) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\fionn\\anaconda3\\lib\\site-packages (from requests<3.0.0->demoji==0.1.5) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordsegment\n",
    "\n",
    "#Wheel file below is in path\n",
    "!pip install demoji-0.1.5-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordsegment as ws\n",
    "import demoji\n",
    "from wordsegment import load, segment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Loading in data </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 9000 tweets in this database, 5217 are not hate, 3783 are hate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @realDonaldTrump #LockThemUp #BuildTheWall #EndDACA #BoycottNFL #BoycottNike</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast majority of the ones escaping a war &amp;amp; not those who cannot fight like women, children, and the elderly?It's because the majority of the refugees are not actually refugees they are economic migrants trying to get into Europe.... https://t.co/Ks0SHbtYqn</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the border like Road Kill and Refuse to Unite! They Hope they get Amnesty, Free Education and Welfare Illegal #FamilesBelongTogether in their Country not on the Taxpayer Dime Its a SCAM #NoDACA #NoAmnesty #SendThe</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an Array of Problems' for Immigrants https://t.co/ACZKLhdMV9 https://t.co/CJAlSXCzR6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignoring the will of the people, they do not want migrants https://t.co/NeYFyqvYlX</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>206</td>\n",
       "      <td>@KurtSchlichter LEGAL is. Not illegal. #BuildThatWall</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>207</td>\n",
       "      <td>@RitaPanahi @826Maureen @RealCandaceO Antifa are just a pack of druggie misfits that no one loves, being the violent thugs they are is their cry for attention and their hit of self importance.#JuvenileDelinquents</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>208</td>\n",
       "      <td>Ex-Teacher Pleads Not guilty To Rape Charges https://t.co/D2mGu3VT5G</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>209</td>\n",
       "      <td>still places on our Bengali (Sylheti) class! it's London's 2nd language! know anyone interested @SBSisters @refugeecouncil @DocsNotCops https://t.co/sOx6shjvMx</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>210</td>\n",
       "      <td>DFID Africa Regional Profile: July 2018 https://t.co/npfZCriW0w</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  \\\n",
       "0  201   \n",
       "1  202   \n",
       "2  203   \n",
       "3  204   \n",
       "4  205   \n",
       "5  206   \n",
       "6  207   \n",
       "7  208   \n",
       "8  209   \n",
       "9  210   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                          tweet  \\\n",
       "0  Hurray, saving us $$$ in so many ways @potus @realDonaldTrump #LockThemUp #BuildTheWall #EndDACA #BoycottNFL #BoycottNike                                                                                                                                                                                      \n",
       "1  Why would young fighting age men be the vast majority of the ones escaping a war &amp; not those who cannot fight like women, children, and the elderly?It's because the majority of the refugees are not actually refugees they are economic migrants trying to get into Europe.... https://t.co/Ks0SHbtYqn   \n",
       "2  @KamalaHarris Illegals Dump their Kids at the border like Road Kill and Refuse to Unite! They Hope they get Amnesty, Free Education and Welfare Illegal #FamilesBelongTogether in their Country not on the Taxpayer Dime Its a SCAM #NoDACA #NoAmnesty #SendThe                                                \n",
       "3  NY Times: 'Nearly All White' States Pose 'an Array of Problems' for Immigrants https://t.co/ACZKLhdMV9 https://t.co/CJAlSXCzR6                                                                                                                                                                                 \n",
       "4  Orban in Brussels: European leaders are ignoring the will of the people, they do not want migrants https://t.co/NeYFyqvYlX                                                                                                                                                                                     \n",
       "5  @KurtSchlichter LEGAL is. Not illegal. #BuildThatWall                                                                                                                                                                                                                                                          \n",
       "6  @RitaPanahi @826Maureen @RealCandaceO Antifa are just a pack of druggie misfits that no one loves, being the violent thugs they are is their cry for attention and their hit of self importance.#JuvenileDelinquents                                                                                           \n",
       "7  Ex-Teacher Pleads Not guilty To Rape Charges https://t.co/D2mGu3VT5G                                                                                                                                                                                                                                           \n",
       "8  still places on our Bengali (Sylheti) class! it's London's 2nd language! know anyone interested @SBSisters @refugeecouncil @DocsNotCops https://t.co/sOx6shjvMx                                                                                                                                                \n",
       "9  DFID Africa Regional Profile: July 2018 https://t.co/npfZCriW0w                                                                                                                                                                                                                                                \n",
       "\n",
       "   label  TR  AG  \n",
       "0  1      0   0   \n",
       "1  1      0   0   \n",
       "2  1      0   0   \n",
       "3  0      0   0   \n",
       "4  0      0   0   \n",
       "5  1      0   0   \n",
       "6  0      0   0   \n",
       "7  0      0   0   \n",
       "8  0      0   0   \n",
       "9  0      0   0   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Raw_Data/hateval2019/hateval2019_en_train.csv', sep=',',  index_col = False, encoding = 'utf-8')\n",
    "train.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(\"Out of {} tweets in this database, {} are not hate, {} are hate\".format(len(train.index), \n",
    "                                                      len(train[train['label']==0]),\n",
    "                                                      len(train[train['label']==1])))\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic tweet text pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and:\n",
    "    1) Removes URLS\n",
    "    2) lots of whitespace with one instance\n",
    "    3) Removes mentions\n",
    "    4) Uses the html.unescape() method to convert unicode to text counterpart\n",
    "    5) Replace & with and\n",
    "    6) Remove the fact the tweet is a retweet if it is - knowing the tweet is \n",
    "       a retweet does not help towards our classification task.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[#$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+:'\n",
    "    mention_regex1 = '@[\\w\\-]+'\n",
    "    RT_regex = '(RT|rt)[ ]*@[ ]*[\\S]+'\n",
    "    \n",
    "    # Replaces urls with URL\n",
    "    parsed_text = re.sub(giant_url_regex, '', text_string)\n",
    "    parsed_text = re.sub('URL', '', parsed_text)\n",
    "    \n",
    "    # Remove the fact the tweet is a retweet. \n",
    "    # (we're only interested in the language of the tweet here)\n",
    "    parsed_text = re.sub(RT_regex, ' ', parsed_text) \n",
    "    \n",
    "    # Removes mentions as they're redundant information\n",
    "    parsed_text = re.sub(mention_regex, '',  parsed_text)\n",
    "    #...including mentions with colons after - this seems to come up often\n",
    "    parsed_text = re.sub(mention_regex1, '',  parsed_text)  \n",
    "\n",
    "    #Replace &amp; with and\n",
    "    parsed_text = re.sub('&amp;', 'and', parsed_text)\n",
    "\n",
    "    # Remove unicode\n",
    "    parsed_text = re.sub(r'[^\\x00-\\x7F]','', parsed_text) \n",
    "    parsed_text = re.sub(r'&#[0-9]+;', '', parsed_text)  \n",
    "\n",
    "    # Convert unicode missed by above regex to text\n",
    "    parsed_text = html.unescape(parsed_text)\n",
    "    \n",
    "    # Remove excess whitespace at the end\n",
    "    parsed_text = re.sub(space_pattern, ' ', parsed_text) \n",
    "    \n",
    "    # Set text to lowercase and strip\n",
    "    parsed_text = parsed_text.lower()\n",
    "    parsed_text = parsed_text.strip()\n",
    "    \n",
    "    return parsed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating basic tweet text preprocessing\n",
    "\n",
    "On Mentions and urls:\n",
    "\n",
    "Removes <i>@CanBorder @rcmpgrcpolice </i> as it's redundant information\n",
    "\n",
    "Also removes URL at the end as it is not only redundant information, but would be harmful if our tokenizer were to parse it as t would give many useless tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " @CanBorder @rcmpgrcpolice And meanwhile as they bicker our lettuce strainer of a border sees well dressed and outfitted \"refugees\" (code for country shoppers), other illegals and undocumented crossing the border daily... https://t.co/ghVM5IZviU\n",
      "\n",
      "Preprocessed:\n",
      " and meanwhile as they bicker our lettuce strainer of a border sees well dressed and outfitted \"refugees\" (code for country shoppers), other illegals and undocumented crossing the border daily...\n"
     ]
    }
   ],
   "source": [
    "testtweet = train['tweet'][1234]\n",
    "\n",
    "print(\"Original:\\n\", testtweet)\n",
    "print(\"\\nPreprocessed:\\n\", preprocess(testtweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of unicode demostrated below <i>‘˜big ideas’ </i> to normal <i>big ideas</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " Why we need to protect refugees from the ‘˜big ideas’ designed to save them https://t.co/nvvpIGyr2f @Refugees @RCKDirector @UNHCR_Kenya @NRC_HoA @drchorn_africaY @tyrusmaina @AmnestyKenya\n",
      "\n",
      "Preprocessed:\n",
      " why we need to protect refugees from the big ideas designed to save them\n"
     ]
    }
   ],
   "source": [
    "testtweet = train['tweet'][310]\n",
    "\n",
    "print(\"Original:\\n\", testtweet)\n",
    "print(\"\\nPreprocessed:\\n\", preprocess(testtweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Removal of unicode can result in removal of emojis though. which could be vital in understanding context of the tweet </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " Chi-town is a 💩⚫ https://t.co/X2QMidmAH9\n",
      "\n",
      "Preprocessed:\n",
      " chi-town is a\n"
     ]
    }
   ],
   "source": [
    "testtweet = train['tweet'][317]\n",
    "\n",
    "print(\"Original:\\n\", testtweet)\n",
    "print(\"\\nPreprocessed:\\n\", preprocess(testtweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji Translation\n",
    "\n",
    "If this function is used in the text preprocessing pipeline, it must be used before the basic tweet pre-processing demonstrated above. The function above will wipe out any unicode which in turn wipes out all emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 0.67 seconds)\n",
      "\u001b[33mWriting emoji data to C:\\Users\\fionn\\.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "demoji.download_codes()# Download most recent emoji codes\n",
    "\n",
    "def emojiReplace(text_string):\n",
    "    emoji_dict = demoji.findall(text_string)\n",
    "    for emoji in emoji_dict.keys():\n",
    "        text_string = text_string.replace(emoji, ' '+  emoji_dict[emoji])\n",
    "    return text_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demostrating emojiReplace\n",
    "\n",
    "In the sequnces below, we obtain perhaps vital context to what is being said in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " Chi-town is a 💩⚫ https://t.co/X2QMidmAH9\n",
      "\n",
      "Replacing emojis:\n",
      " Chi-town is a  pile of poo black circle https://t.co/X2QMidmAH9\n"
     ]
    }
   ],
   "source": [
    "testtweet = train['tweet'][317]\n",
    "\n",
    "print(\"Original:\\n\", testtweet)\n",
    "print(\"\\nReplacing emojis:\\n\", emojiReplace(testtweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " No seriously. It has 😂🤣 https://t.co/4k4jlLTDUj\n",
      "\n",
      "Replacing Emojis:\n",
      " No seriously. It has  face with tears of joy rolling on the floor laughing https://t.co/4k4jlLTDUj\n"
     ]
    }
   ],
   "source": [
    "testtweet = train['tweet'][300]\n",
    "\n",
    "print(\"Original:\\n\", testtweet)\n",
    "print(\"\\nReplacing Emojis:\\n\", emojiReplace(testtweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we perform the basic pre-processing function before this function, it will eliminate the excess whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " No seriously. It has 😂🤣 https://t.co/4k4jlLTDUj\n",
      "\n",
      "Replacing Emojis and Basic Preprocessing:\n",
      " no seriously. it has face with tears of joy rolling on the floor laughing\n"
     ]
    }
   ],
   "source": [
    "testtweet = train['tweet'][300]\n",
    "\n",
    "print(\"Original:\\n\", testtweet)\n",
    "print(\"\\nReplacing Emojis and Basic Preprocessing:\\n\", preprocess(emojiReplace(testtweet)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " Same. We really are soulmates... Dumb AF but soulmates nonetheless 🙃🙃🙃🙃🙃🙃🙃🙃🙃🙃🙃🙃🙃🙃 https://t.co/ZwXTny02jj\n",
      "\n",
      "Replacing emojis:\n",
      " Same. We really are soulmates... Dumb AF but soulmates nonetheless  upside-down face upside-down face upside-down face upside-down face upside-down face upside-down face upside-down face upside-down face upside-down face upside-down face upside-down face upside-down face upside-down face upside-down face https://t.co/ZwXTny02jj\n"
     ]
    }
   ],
   "source": [
    "testtweet1 = train['tweet'][7436]\n",
    "\n",
    "print(\"Original:\\n\", testtweet1)\n",
    "print(\"\\nReplacing emojis:\\n\", emojiReplace(testtweet1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmojiReplace_v2\n",
    "\n",
    "The above `emojiReplace` function is quite useful for converting emojis into interpretable text that has words that have been seen before by the pre-trained BERT model and so already have contextually informed weights.\n",
    "\n",
    "Whilst this may often lead to a relatively accurate portrayal of sentiment, perhaps it can have it's drawbacks. What about when there are several consecutive emojis, often of the same type? This can often lead to unneccessarily large sequence lengths. Also it can detract importance from the rest of the sequence which may have the important information that better tells us whether a tweet is hate speech or not.\n",
    "\n",
    "Furthermore, giving each of these emojis it's own singular token which it can be identified by could be beneficial for our classifier. We can replace the \"[unusedX]\" tokens, which have already randomly initialized weights. These weights will be updated in the fine-tuning stage, thus giving contextuall representation to these words.\n",
    "\n",
    "The altering of the BERT vocab file is done in the notebook `New_Vocab_File_for_Emojis.ipynb`. Below is demostrated the fucntion to convert emojis into unique tokens to work with the new vocab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emojiReplace_v2(text_string):\n",
    "    emoji_dict = demoji.findall(text_string)    \n",
    "    for emoji in emoji_dict.keys():\n",
    "        #Making the connecting token between words a normal letter 'w' because BERT's tokenizer\n",
    "        #splits on special tokens like '%' and '$'\n",
    "        emoji_token = 'x'.join(re.split('\\W+', emoji_dict[emoji])) + ' '\n",
    "        text_string = text_string.replace(emoji, emoji_token)\n",
    "        \n",
    "        #Controlling for multiple emojis in a row\n",
    "        pattern = '(' + emoji_token + ')' + '{2,}'\n",
    "        text_string = re.sub(pattern, 'mult' + emoji_token + ' ', text_string)\n",
    "    return text_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demostrating emojiReplace_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " No seriously. It has 😂🤣 https://t.co/4k4jlLTDUj\n",
      "\n",
      "Replacing emojis:\n",
      " No seriously. It has facexwithxtearsxofxjoy rollingxonxthexfloorxlaughing  https://t.co/4k4jlLTDUj\n"
     ]
    }
   ],
   "source": [
    "testtweet = train['tweet'][300]\n",
    "\n",
    "print(\"Original:\\n\", testtweet)\n",
    "print(\"\\nReplacing emojis:\\n\", emojiReplace_v2(testtweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " Same. We really are soulmates... Dumb AF but soulmates nonetheless 🙃🙃🙃🙃🙃🙃🙃🙃🙃🙃🙃🙃🙃🙃 https://t.co/ZwXTny02jj\n",
      "\n",
      "Replacing emojis:\n",
      " Same. We really are soulmates... Dumb AF but soulmates nonetheless multupsidexdownxface   https://t.co/ZwXTny02jj\n"
     ]
    }
   ],
   "source": [
    "testtweet1 = train['tweet'][7436]\n",
    "\n",
    "print(\"Original:\\n\", testtweet1)\n",
    "print(\"\\nReplacing emojis:\\n\", emojiReplace_v2(testtweet1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "load() # Loading wordsegment\n",
    "\n",
    "#The values below of the bigrams reflect the amount of search results on google that come up\n",
    "ws.BIGRAMS['alt right'] = 1.17e8 # update wordsegment dict so \n",
    "                                #it recognises altright as \"alt right\" rather than salt right\n",
    "ws.BIGRAMS['white supremacists'] = 3.86e6\n",
    "ws.BIGRAMS['tweets'] = 6.26e10\n",
    "ws.BIGRAMS['independece day'] = 6.21e7\n",
    "\n",
    "def hashtagSegment(text_string):\n",
    "    \n",
    "    #We target hashtags so that we only segment the hashtag strings.\n",
    "    #Otherwise the segment function may operate on misspelled words also; which\n",
    "    #often appear in hate speech tweets owing to the ill education of those spewing it\n",
    "    temp_str = []\n",
    "    for word in text_string.split(' '):\n",
    "        if word.startswith('#') == False:\n",
    "            temp_str.append(word)\n",
    "        else:\n",
    "            temp_str = temp_str + segment(word)\n",
    "            \n",
    "    text_string = ' '.join(temp_str)       \n",
    "\n",
    "    return text_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demostrating Hashtag Segmentation\n",
    "\n",
    "</b>Converts hashtags like #lockthemup and #enddaca into interpreatble words which can be converted into features by BERT<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " hurray, saving us $$$ in so many ways #lockthemup #buildthewall #enddaca #boycottnfl #boycottnike\n",
      "\n",
      "Hashtag Segmented:\n",
      " hurray, saving us $$$ in so many ways lock them up build the wall end daca boycott nfl boycott nike\n"
     ]
    }
   ],
   "source": [
    "testtweet = preprocess(train['tweet'][0])\n",
    "\n",
    "print(\"Original:\\n\", testtweet)\n",
    "print(\"\\nHashtag Segmented:\\n\", hashtagSegment(testtweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " great speec mr. president. i would suggest change to keep america great after u #buildthatwall that u promised and to make mexico pay for it. please keep the #promisesmade and #dowhatyousaid.\n",
      "\n",
      "Hashtag Segmented:\n",
      " great speec mr. president. i would suggest change to keep america great after u build that wall that u promised and to make mexico pay for it. please keep the promises made and do what you said\n"
     ]
    }
   ],
   "source": [
    "testtweet = preprocess(train['tweet'][1029])\n",
    "\n",
    "print(\"Original:\\n\", testtweet)\n",
    "print(\"\\nHashtag Segmented:\\n\", hashtagSegment(testtweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing punctuation\n",
    "\n",
    "Effects are self-explanatory, although one must be careful to do this after hashtage segmentation otherwise hashtags will be removed.\n",
    "\n",
    "When BERT tokenizes sequences, it treats punctuation as a separate token. Which could prove harmful to the model possibly as it's taking into account useless informtation here.\n",
    "\n",
    "Perhaps not though, BERT will have already seen punctuation in it's pre-training stage and already provided appropriate vectorized weights to these punctuation symbols. We shall test the effect of this technique anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    \n",
    "    #Return the charater as long as it's not punctuation\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Removal of Punctuation\n",
    "\n",
    "Below we remove full stops from this tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " we have to demand justice and mercy for immigrants. it is unacceptable that anyone would die in detention. congress has to act now with welcome and compassion.\n",
      "\n",
      "Removing Punctuation:\n",
      " we have to demand justice and mercy for immigrants it is unacceptable that anyone would die in detention congress has to act now with welcome and compassion\n"
     ]
    }
   ],
   "source": [
    "testtweet = preprocess(train['tweet'][921])\n",
    "\n",
    "print(\"Original:\\n\", testtweet)\n",
    "print(\"\\nRemoving Punctuation:\\n\", remove_punct(testtweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we remove commas, question marks, dashes and even hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " why #nodaca ?illegals est. between 11 - 30 millionillegal vote 80% dem or 600,000 dem vote advantage / 1,000,000 illegal voters70,000 votes gave 2016 electionwhy do you think dems pushing no walls, no borders, no voter ids?#potus #maga #kag #trump #news #votered\n",
      "\n",
      "Removing Punctuation:\n",
      " why nodaca illegals est between 11  30 millionillegal vote 80 dem or 600000 dem vote advantage  1000000 illegal voters70000 votes gave 2016 electionwhy do you think dems pushing no walls no borders no voter idspotus maga kag trump news votered\n"
     ]
    }
   ],
   "source": [
    "testtweet = preprocess(train['tweet'][910])\n",
    "\n",
    "print(\"Original:\\n\", testtweet)\n",
    "print(\"\\nRemoving Punctuation:\\n\", remove_punct(testtweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing text\n",
    "\n",
    "Very like stemming but more complex, the difference is that it's slower, because unlike stemming - which just heuristically chops of the word without taking into account the context in which it is used, lemmatizing returns words that are actually in the dictionary.\n",
    "\n",
    "It doesn't just cut off -ing or -ed because it sees it. The resulting word would have to be a real word with also a similar meaning (synonym) to the original word it was cutting off.\n",
    "\n",
    "Can be useful to our model because it helps us reduce the corpus of words that the model is exposed to by correlating words with similar meaning.\n",
    "\n",
    "Can be problematic with abbreviations in words, which is common on twitter so caution is advised with this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fionn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(text):\n",
    "    word_list = re.split('\\W+', text)\n",
    "    text = \" \".join([wn.lemmatize(word) for word in word_list])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Lemmatizing\n",
    "\n",
    "Small changes: replaces atrocities with atrocity and families with family. The sentence does not lose much of it's meaning with these changes, unlike stemming which would result in much meaning lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " hitler left a stain on germany for the atrocities he committed against the jews. trump will leave a stain on america for the atrocities he's commiting against these immigrant families. i only hope there is a reenactment of the nuremberg trials at the end of his reign. #inhumane\n",
      "\n",
      "Lemmatized:\n",
      " hitler left a stain on germany for the atrocity he committed against the jew trump will leave a stain on america for the atrocity he s commiting against these immigrant family i only hope there is a reenactment of the nuremberg trial at the end of his reign inhumane\n"
     ]
    }
   ],
   "source": [
    "testtweet = preprocess(train['tweet'][21])\n",
    "\n",
    "print(\"Original:\\n\", testtweet)\n",
    "print(\"\\nLemmatized:\\n\", lemmatizing(testtweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords\n",
    "Stopwords are words which have been deemed to not give very much useful information. Words like 'the' and 'with' which are sentiment neutral words and words which don't tell us a lot about the intent of a sentence\n",
    "\n",
    "Caution is advised with this pre-processing yet again though as removing stopwords can completely transform a sentence. This technique has proven benefits with dealing with basic NLP tasks like sentiment clssifiers and spam detection. Howvere hate speech detection is a whole different matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fionn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    word_list = re.split('\\W+', text)\n",
    "    text = \" \".join([word for word in word_list if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating removing stopwords\n",
    "\n",
    "As it's shown below, words like <b><i>'and', 'to', 'them' </i></b> and so on are removed. This sort of text-preprocessing is a blunt tool and should again be treated with caution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " angry italian officials refuse to let this italian commercial ship disembark 66 refugees and migrants because they think it should have let libyan coastguards intercept them and return them to inhumane detention centers instead\n",
      "\n",
      "Removing stopwords:\n",
      " angry italian officials refuse let italian commercial ship disembark 66 refugees migrants think let libyan coastguards intercept return inhumane detention centers instead\n"
     ]
    }
   ],
   "source": [
    "testtweet = preprocess(train['tweet'][291])\n",
    "\n",
    "print(\"Original:\\n\", testtweet)\n",
    "print(\"\\nRemoving stopwords:\\n\", remove_stopwords(testtweet))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
