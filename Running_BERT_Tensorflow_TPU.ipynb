{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Running_BERT_Tensorflow_TPU.ipynb","provenance":[{"file_id":"1EBH_dCvSIfTM-VlKeBfV2Em3exIJzkih","timestamp":1576806467871},{"file_id":"1JuDopUfWUBPOSoIOBuqs7S2tH2FJHokl","timestamp":1575402454368},{"file_id":"1VNukx0WgDZ6kdgs4gvDj6zZ5czaNCAO6","timestamp":1575299325999},{"file_id":"https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb","timestamp":1574879594425}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"j0a4mTk9o1Qg","colab_type":"code","colab":{}},"source":["# Copyright 2019 Google Inc.\n","\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Q4eUHbzUO-c","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/kpe/bert-for-tf2/blob/master/examples/tpu_movie_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"dCpvgG0vwXAZ","colab_type":"text"},"source":["#Detecting Hate speech Tweets with BERT"]},{"cell_type":"markdown","metadata":{"id":"xiYrZKaHwV81","colab_type":"text"},"source":["We are using bert-tensorflow for this classification task. At the moment I'm making sure it's tensorflow version 1.x because tensorflow version 2 gives issues with Bert at the moment. I believe Tensorflow hopes to have this issue resolved in tensorflow v 2.1\n","\n","We are using a TPU as a GPU does not have the required memory for Large BERT models- it can only cope with the base model. We'll see if there a TPU detected and we'll set it to a global environment variable so it can be accessed by our BERT functions later."]},{"cell_type":"code","metadata":{"id":"mYEcG2vlPumC","colab_type":"code","outputId":"dffb42d9-5c79-4c58-9410-53024c420816","executionInfo":{"status":"ok","timestamp":1584115221998,"user_tz":0,"elapsed":34071,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":887}},"source":["!pip install gcsfs \n","import pandas as pd\n","import numpy as np\n","\n","#Make sure to use tensorflow version 1.x, version 2 doesn't work with bert\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import os\n","\n","#For cross-validation and grid search\n","from itertools import product\n","from tensorflow.python.summary.summary_iterator import summary_iterator\n","from google.cloud import storage\n","import ipywidgets as widgets\n","from IPython.display import display\n","\n","import sklearn\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from sklearn import metrics\n","\n","\n","import html\n","import re\n","import json\n","import pprint\n","import random\n","import string\n","import nltk\n","from datetime import datetime\n","import time\n","\n","\n","assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n","TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","print('TPU address is', TPU_ADDRESS)\n","\n","#Below we give ourselves as well as the TPU access to our private GCS bucket\n","from google.colab import auth\n","auth.authenticate_user()\n","tf.reset_default_graph()  \n","with tf.Session(TPU_ADDRESS) as session:\n","  # Upload credentials to TPU.\n","  with open('/content/adc.json', 'r') as f:\n","    auth_info = json.load(f)\n","  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n","\n","USE_TPU=True\n","try:\n","  #tf.config.experimental_connect_to_host(TPU_ADDRESS)\n","  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n","  tf.config.experimental_connect_to_cluster(cluster_resolver)\n","  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n","  tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n","except Exception as ex:\n","  print(ex)\n","  USE_TPU=False\n","\n","print(\"        USE_TPU:\", USE_TPU)\n","print(\"Eager Execution:\", tf.executing_eagerly())\n","\n","assert not tf.executing_eagerly(), \"Eager execution on TPUs have issues currently\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting gcsfs\n","  Downloading https://files.pythonhosted.org/packages/3e/9f/864a9ff497ed4ba12502c4037db8c66fde0049d9dd0388bd55b67e5c4249/gcsfs-0.6.0-py2.py3-none-any.whl\n","Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.7.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.1)\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.6.2)\n","Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.21.0)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (3.1.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (45.2.0)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.11.28)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n","Installing collected packages: gcsfs\n","Successfully installed gcsfs-0.6.0\n","TPU address is grpc://10.1.124.234:8470\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","INFO:tensorflow:Initializing the TPU system: 10.1.124.234:8470\n","INFO:tensorflow:Finished initializing TPU system.\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.1.124.234:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4710141087512758117)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 12274108799380103254)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5018382452545410607)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5341975069421973992)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 13553801097406412008)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 4506931240684566645)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 3394888996658467550)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 9440640357204891578)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 1593121549171023204)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 207858147323404956)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 17863398238325349030)\n","        USE_TPU: True\n","Eager Execution: False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sRh9YYYfgUTE","colab_type":"text"},"source":["Setting a random seed for reproducability of results and checking version of tensorflow"]},{"cell_type":"code","metadata":{"id":"dgVteKeTgXrg","colab_type":"code","outputId":"050981fb-d5b9-47a2-a5f1-0de9312436f9","executionInfo":{"status":"ok","timestamp":1584115221999,"user_tz":0,"elapsed":34061,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Setting the graph-level random seed for the default graph. Different than operation level seed\n","SEED = 3060\n","tf.reset_default_graph()\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","tf.set_random_seed(SEED) \n","random.seed(SEED)\n","np.random.seed(SEED)\n","print(\"Tensorflow Version:\", tf.__version__)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Tensorflow Version: 1.15.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KVB3eOcjxxm1","colab_type":"text"},"source":["Below we will set the directory where we will store our output model. To ensure the right variables are loaded in our run config function later, our ouput directory must be in the same directory as our pre-trained bert model directory.\n","\n","Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."]},{"cell_type":"code","metadata":{"id":"US_EAnICvP7f","colab_type":"code","cellView":"both","outputId":"a42ddb8f-5a2a-4a7b-b2d3-c72b1b5fc1aa","executionInfo":{"status":"ok","timestamp":1584115226482,"user_tz":0,"elapsed":38534,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["#Large whole word masking BERt pre-trained weights\n","bert_model_name = 'wwm_uncased_L-24_H-1024_A-16' \n","\n","#Where we output the fine tuned model\n","output_dir = os.path.join(bert_model_name, 'output1')\n","\n","DATASET = 'HatEval' #@param [\"HatEval\", \"AnalyticsVidhya\"]\n","\n","#@markdown Whether or not to use the further pretrained model\n","FURTHER_PRETRAINED = True #@param {type:\"boolean\"}\n","if FURTHER_PRETRAINED == True:\n","\n","  further_pretrained_model = os.path.join(bert_model_name, 'further_pretrained_model1')\n","  output_dir = os.path.join(further_pretrained_model, 'output')\n","\n","#@markdown Whether or not to clear/delete the directory and create a new one\n","DO_DELETE = True #@param {type:\"boolean\"}\n","#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","USE_BUCKET = True #@param {type:\"boolean\"}\n","BUCKET = 'csc3002' #@param {type:\"string\"}\n","os.environ[\"GCLOUD_PROJECT\"] = \"csc3002\"\n","\n","if USE_BUCKET:\n","  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, output_dir)\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","if DO_DELETE:\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","    # Doesn't matter if the directory didn't exist\n","    pass\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["***** Model output directory: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output *****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yDFCBYMB-m3v","colab_type":"text"},"source":["<b> If you're not connected to a TPU environment but still want to access GCS bucket - run below: </b>"]},{"cell_type":"code","metadata":{"id":"65aCrle2Bmmi","colab_type":"code","outputId":"e65427b1-f3c3-491b-93f9-8ac2f640d0dc","executionInfo":{"status":"ok","timestamp":1584115226483,"user_tz":0,"elapsed":38525,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["\"\"\"from google.colab import drive\n","drive.mount('/content/drive')\n","!gcloud auth activate-service-account --key-file '/content/drive/My Drive/storageCreds.json'\n","\"\"\""],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"from google.colab import drive\\ndrive.mount('/content/drive')\\n!gcloud auth activate-service-account --key-file '/content/drive/My Drive/storageCreds.json'\\n\""]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"UmeFZJIvTy33","colab_type":"text"},"source":["#### Setting up Data Based Upon Choice of DATASET\n"]},{"cell_type":"code","metadata":{"id":"jhIIdbOETxC4","colab_type":"code","colab":{}},"source":["if DATASET == 'HatEval':\n","  dirc = 'gs://csc3002/hateval2019'\n","\n","  rawTrain = pd.read_csv(os.path.join(dirc, 'hateval2019_en_train.csv'), sep=',',  index_col = False, encoding = 'utf-8')\n","  rawTrain.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","  rawTrain.drop(['TR', 'AG'], inplace = True, axis = 1)\n","\n","  rawDev = pd.read_csv(os.path.join(dirc, 'hateval2019_en_dev.csv'), sep=',',  index_col = False, encoding = 'utf-8')\n","  rawDev.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","  rawDev.drop(['TR', 'AG'], inplace = True, axis = 1)\n","\n","  rawTest = pd.read_csv(os.path.join(dirc, 'hateval2019_en_test.csv'), sep=',',  index_col = False, encoding = 'utf-8')\n","  rawTest.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","  rawTest.drop(['TR', 'AG'], inplace = True, axis = 1)\n","\n","  imbalanced = False\n","\n","elif DATASET == \"AnalyticsVidhya\":\n","  dirc = 'gs://csc3002/trial'\n","\n","  rawTrain= pd.read_csv(os.path.join(dirc, 'train_E6oV3lV.csv'),  sep=',',  index_col = False, encoding = 'utf-8')\n","  \n","  rawTrain, rawDev = train_test_split(rawTrain, test_size=0.20, random_state=SEED)\n","  \n","  rawTest = pd.read_csv(os.path.join(dirc, 'test_tweets_anuFYb8.csv'), sep=',',  index_col = False, encoding = 'utf-8')\n","  \n","  imbalanced = True\n","\n","else:\n","  raise ValueError('No Valid DATASET chosen')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pmFYvkylMwXn","colab_type":"text"},"source":["#Training Data\n","I've stored all of the data, (train, dev and test),  in my google bucket for ease of access, authentication will have to be provided"]},{"cell_type":"code","metadata":{"id":"UvknR984WeDW","colab_type":"code","outputId":"48ff7e3f-9f24-462a-9566-a3f3fee6e751","executionInfo":{"status":"ok","timestamp":1584115231916,"user_tz":0,"elapsed":43943,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!gcloud config set project 'my-project-csc3002'\n","\n","train = rawTrain.sample(frac=1, random_state = SEED) #Shuffling really helps model performance\n","train.reset_index(drop = True, inplace = True)\n","train.id = train.index\n","pd.set_option('display.max_colwidth', -1)\n","print(\"Out of {} tweets in this database, {} are not hate, {} are hate\".format(len(train.index), \n","                                                      len(train[train['label']==0]),\n","                                                      len(train[train['label']==1])))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Updated property [core/project].\n","Out of 9000 tweets in this database, 5217 are not hate, 3783 are hate\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"57uQmKX8yQGq","colab_type":"text"},"source":["<b>Original Dataset </b>"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"dd24ef68-4370-474a-fcc2-e3ff3f39729a","executionInfo":{"status":"ok","timestamp":1584115232109,"user_tz":0,"elapsed":44126,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"id":"TblezF_V2DrA","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train.head(30)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>How come Allah is not helping you it is up to Christian countries to  protect you feed you ,The countries hit by violence from islam take refugees in feed them etcPlease no more explaining about your hard times we are doing our best for uYes there is good and bad every where</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>With todays #JalalabadAttack &amp;amp; other vicious attacks claimed by ISIS, I smell a spillover of refugees in Pak again. This time we should not open borders for them. We cant afford terrorists taking undue advantage. Let Americans deal with it.#Afghanistan #Jalalabad</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>https://t.co/i9LJDjtGz7Migration greatest threat‚Äô to Austrian security, says top military figure.EU and Europe bitterly divided√∞≈∏‚Äò‚Ä∞major confrontations between the two.Nothing more counterproductive than ‚Äúcenters‚Äù on European territory or euro bribes for migrants.#Visegrad #V4 https://t.co/VnPCTe7opC</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>When all your friends are out hoe'in and you're stuck at home in a shitty relationship https://t.co/X9oz1Tx7TC</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>I wonder if rick will make another deal with those crazy ass women ü§î and if that crazy ass nigga will actually hoe Daryl again üòê</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>Worker Charged With Sexually Molesting Eight Children at Immigrant Shelter https://t.co/D6HcH03nGL via @CitizenTruth_ #realDonaldTrump do something about this disgrace and stop separating children from their parents.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>UN seeks new funding pledges for Palestinian refugees... https://t.co/SNJhD1PWxT https://t.co/DlHQ8fc5N6</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>If you really wanna know what someone you're fucking thinks about you, make them show you how you're stored in their phone...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>Going to make Du'a at the shrine of Imam Reza(AS) for the refugees in Athens.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>Poor kid. Someone wise must have told him, \"When the world gives you lemons, make lemonade.\" He listened. His lemonade should now be offered with ICE in abundance. #BuildTheWall #SendThemBack https://t.co/8AM7fgo9ph</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>@TVRav You got reply in before mine, but what I'm pointing out is women work their lives around being safe from *some men*, not all men</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>Her- remember when you said you loved me? Me- https://t.co/hv9WiaqXTu</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>I stand with the victims of sexual harassment or rape. No matter if they're a woman or a man. A rape is a rape what‚Ä¶ https://t.co/PpiL6mBtqV</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>Canada's @JustinTrudeau appointed to his cabinet immigrants from such cultures. One is in charge of immigration &amp;amp; refugee affairs - license to flood Canada with own kind. The other, Education Minister, tried to sneak into new Education code female genital mutilation as a right!</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>@graysonslays marcos you skank hannah montana is my thinf</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>@_miidnightr Wanna speak down to my girl bitch then step up big nose ass bitch. Saw that shit off.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>Philippines - Christians are facing starvation as they continue to hide from jihadists.  An estimate 1500 remain... https://t.co/P04dSj0nbs</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>Thank you so much to @WValderrama &amp;amp; @BoomEventsLA for hosting such an amazing event this past weekend! No child should face immigration court w/o an attorney by their side &amp;amp; this event helped raise critical awareness for these immigrant &amp;amp; refugee children #KINDLA #GivingKINDness https://t.co/v3bxbNMsYx</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>Could you open up your home to refugees in need? https://t.co/0F2rzmYCSw</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>It's unfortunate that such racist tactics are used against migrant communities. Remember those who are spreading fear are those who cut the funding of vital services for migrants such as healthcare and education!  https://t.co/82hkXRW6L1 via @theage</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>bitch what the fuck do you mean you cant find the fucking page cunt i need to vote for my boys dont you understand https://t.co/xbdlkyXah7</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>World Refugee Day was this past Wednesday, but there is still time to make a difference for the 20,000 refugees who will arrive in the U.S. this year. Rally the support of your friends and family by becoming a fundraiser for refugees. Get started here √¢≈æ_x009d_ https://t.co/QzFjSqWwJ8 https://t.co/vS9TqvpVm6</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>US Senate Calls On Julian Assange To Testify https://t.co/Qqy6GO6jm7 https://t.co/arrxfZVTdb</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>When you find a bag of drugs on the ground https://t.co/j8JcFgLCHP</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>When you're flexin on these bitches https://t.co/7fR6wozeU4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>@SenKamalaHarris So now the word hysterical is a completely sexist term? So let's get this straight nobody can say the word monkey anymore when they are describing anyting and the word hysterical can no longer be used to describe a hysterical woman?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>VIDEO: Immigrant activist who climbed Statue of Liberty has a new song: \"America, you motherfuckers,... https://t.co/YSjmys3MPK</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>@Scouse_ma hi üòò</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>Picture: the couple Engelbert and Spera, both Jews. Engelbert is now elected in the Austrian parliament for the √É‚ÄìVP. He says in the newspaper Ha√É¬†retz the danger for Jews doesnt come from FP√É‚ÄìNazis, more from Islamic refugees, which import antisemitism to Austria. https://t.co/PhsxnDXWxP</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>* Croatia √∞≈∏‚Ä°¬≠√∞≈∏‚Ä°¬∑:  The Croatian authorities, surprisingly, reject the EU‚Äôs bid of 6,000 euros per migrant.  √∞≈∏‚Äò_x008f_ https://t.co/hccQmsQ9Vl #v4 #visegrad https://t.co/1x1MXOQm2t</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  ... label\n","0   0   ...  0   \n","1   1   ...  1   \n","2   2   ...  0   \n","3   3   ...  0   \n","4   4   ...  1   \n","5   5   ...  0   \n","6   6   ...  0   \n","7   7   ...  0   \n","8   8   ...  0   \n","9   9   ...  0   \n","10  10  ...  0   \n","11  11  ...  0   \n","12  12  ...  0   \n","13  13  ...  1   \n","14  14  ...  0   \n","15  15  ...  1   \n","16  16  ...  0   \n","17  17  ...  0   \n","18  18  ...  0   \n","19  19  ...  0   \n","20  20  ...  0   \n","21  21  ...  0   \n","22  22  ...  0   \n","23  23  ...  0   \n","24  24  ...  0   \n","25  25  ...  0   \n","26  26  ...  0   \n","27  27  ...  0   \n","28  28  ...  0   \n","29  29  ...  1   \n","\n","[30 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"aRLD57l1z631","colab_type":"text"},"source":["# Text Preprocessing\n","\n","The text pre-processing for this project is detailed in the notebook `Text_Preprocessing.ipynb` in the github repo. Below is an import of the repo into the google colab workspace so I can retrieve and use these functions at convenience\n","\n","Also below is a function which loads whichever dataset I choose to load from my GCS bucket or local system. This will be useful later when I want to quickly load in data without the messy, long-winded code to go along with it.\n","\n","<i>We'll put in the option for the function to load and combine two datasets, as later we'll use this when we combine training sets and dev sets for the cross-validation sets.</i>"]},{"cell_type":"code","metadata":{"id":"sxrBp7dg0bp-","colab_type":"code","outputId":"4a410fac-59c8-43b0-be70-eaf62ca23841","executionInfo":{"status":"ok","timestamp":1584115260367,"user_tz":0,"elapsed":72374,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["!git clone https://github.com/fionnmcconville/Automatic-Detection-of-Hate-Speech-Online-Using-BERT.git\n","%cd Automatic-Detection-of-Hate-Speech-Online-Using-BERT \n","#!ls\n","import preprocessing as pre\n","#Return to original workspace\n","%cd ..\n","\n","\n","params = {'replaceEmoji_v2': False, 'replaceEmoji': True, 'segmentHashtag': True,\n","             'remove_stop': False, 'lemmatize': False, 'remove_punct': False}\n","\n","#Function caller can optionally load two dataframes and combine them\n","def loadData(data1, data2 = None, params_dict = params):\n","\n","  if data2 is not None:\n","    frames = [data1,data2]\n","    data = pd.concat(frames)\n","  else:\n","    data = data1\n","  \n","\n","  #Don't have both replaceEmoji functions as true. Otherwise they'll cancel each other out.\n","  #We throw an error here if this is the case\n","  assert not (params_dict['replaceEmoji'] == True and params_dict['replaceEmoji_v2'] == True), \"You can't have two emojiReplace functions selected at the same time\"\n","\n","  #Replace emoji must be done before basic preprocess otherwise unicode will be wiped out\n","  #And this function will be ineffective\n","  if params_dict['replaceEmoji'] == True:\n","    data['tweet'] = data['tweet'].apply(pre.emojiReplace)\n","\n","  if params_dict['replaceEmoji_v2'] == True:\n","    data['tweet'] = data['tweet'].apply(pre.emojiReplace_v2)\n","\n","  #Must be performed after emoji translation\n","  data['tweet'] = data['tweet'].apply(pre.preprocess)\n","\n","  if params_dict['segmentHashtag'] == True:\n","    data['tweet'] = data['tweet'].apply(pre.hashtagSegment)\n","\n","  if params_dict['remove_punct'] == True:\n","    data['tweet'] = data['tweet'].apply(lambda x: pre.remove_punct(x))\n","\n","  if params_dict['remove_stop'] == True:\n","    data['tweet'] = data['tweet'].apply(lambda x: pre.remove_stopwords(x))\n","\n","  if params_dict['lemmatize'] == True:\n","    data['tweet'] = data['tweet'].apply(lambda x: pre.lemmatizing(x))\n","\n","\n","  #data = data[data['tweet'].apply(lambda x: len(x) > 10)] \n","  data = data.sample(frac = 1, random_state=SEED)\n","  data.dropna(inplace = True)\n","  data.reset_index(drop = True, inplace = True)\n","\n","  data.id = data.index\n","  return data\n","\n","#Testing function\n","train = loadData(rawTrain, params_dict = params)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Cloning into 'Automatic-Detection-of-Hate-Speech-Online-Using-BERT'...\n","remote: Enumerating objects: 61, done.\u001b[K\n","remote: Counting objects: 100% (61/61), done.\u001b[K\n","remote: Compressing objects: 100% (45/45), done.\u001b[K\n","remote: Total 101 (delta 28), reused 46 (delta 15), pack-reused 40\u001b[K\n","Receiving objects: 100% (101/101), 86.99 MiB | 33.46 MiB/s, done.\n","Resolving deltas: 100% (32/32), done.\n","Checking out files: 100% (49/49), done.\n","/content/Automatic-Detection-of-Hate-Speech-Online-Using-BERT\n","\u001b[33mDownloading emoji data ...\u001b[0m\n","\u001b[92m... OK\u001b[0m (Got response in 0.11 seconds)\n","\u001b[33mWriting emoji data to /root/.demoji/codes.json ...\u001b[0m\n","\u001b[92m... OK\u001b[0m\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g020ACV22uFG","colab_type":"text"},"source":["**Cleaned tweet text dataset**"]},{"cell_type":"code","metadata":{"id":"lBP7J70v2ueo","colab_type":"code","outputId":"07a33d03-f05d-4a35-d68f-dbaec1959aec","executionInfo":{"status":"ok","timestamp":1584115260367,"user_tz":0,"elapsed":72364,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train[:30]"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>how come allah is not helping you it is up to christian countries to protect you feed you ,the countries hit by violence from islam take refugees in feed them etcplease no more explaining about your hard times we are doing our best for uyes there is good and bad every where</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>with todays jalalabad attack and other vicious attacks claimed by isis, i smell a spillover of refugees in pak again. this time we should not open borders for them. we cant afford terrorists taking undue advantage. let americans deal with it.#afghanistan jalalabad</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>greatest threat to austrian security, says top military figure.eu and europe bitterly dividedmajor confrontations between the two.nothing more counterproductive than centers on european territory or euro bribes for migrants.#visegrad v4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>when all your friends are out hoe'in and you're stuck at home in a shitty relationship</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>i wonder if rick will make another deal with those crazy ass women thinking face and if that crazy ass nigga will actually hoe daryl again neutral face</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>worker charged with sexually molesting eight children at immigrant shelter via real donald trump do something about this disgrace and stop separating children from their parents.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>un seeks new funding pledges for palestinian refugees...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>if you really wanna know what someone you're fucking thinks about you, make them show you how you're stored in their phone...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>going to make du'a at the shrine of imam reza(as) for the refugees in athens.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>poor kid. someone wise must have told him, \"when the world gives you lemons, make lemonade.\" he listened. his lemonade should now be offered with ice in abundance. build the wall send them back</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>you got reply in before mine, but what i'm pointing out is women work their lives around being safe from *some men*, not all men</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>her- remember when you said you loved me? me-</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>i stand with the victims of sexual harassment or rape. no matter if they're a woman or a man. a rape is a rape what</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>canada's appointed to his cabinet immigrants from such cultures. one is in charge of immigration and refugee affairs - license to flood canada with own kind. the other, education minister, tried to sneak into new education code female genital mutilation as a right!</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>marcos you skank hannah montana is my thinf</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>wanna speak down to my girl bitch then step up big nose ass bitch. saw that shit off.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>philippines - christians are facing starvation as they continue to hide from jihadists. an estimate 1500 remain...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>thank you so much to and for hosting such an amazing event this past weekend! no child should face immigration court w/o an attorney by their side and this event helped raise critical awareness for these immigrant and refugee children kind la giving kindness</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>could you open up your home to refugees in need?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>it's unfortunate that such racist tactics are used against migrant communities. remember those who are spreading fear are those who cut the funding of vital services for migrants such as healthcare and education! via</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>bitch what the fuck do you mean you cant find the fucking page cunt i need to vote for my boys dont you understand</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>world refugee day was this past wednesday, but there is still time to make a difference for the 20,000 refugees who will arrive in the u.s. this year. rally the support of your friends and family by becoming a fundraiser for refugees. get started here _x009d_</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>us senate calls on julian assange to testify</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>when you find a bag of drugs on the ground</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>when you're flexin on these bitches</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>so now the word hysterical is a completely sexist term? so let's get this straight nobody can say the word monkey anymore when they are describing anyting and the word hysterical can no longer be used to describe a hysterical woman?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>video: immigrant activist who climbed statue of liberty has a new song: \"america, you motherfuckers,...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>hi face blowing a kiss</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>picture: the couple engelbert and spera, both jews. engelbert is now elected in the austrian parliament for the vp. he says in the newspaper haretz the danger for jews doesnt come from fpnazis, more from islamic refugees, which import antisemitism to austria.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>* croatia : the croatian authorities, surprisingly, reject the eus bid of 6,000 euros per migrant. _x008f_ v4 visegrad</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  ... label\n","0   0   ...  0   \n","1   1   ...  1   \n","2   2   ...  0   \n","3   3   ...  0   \n","4   4   ...  1   \n","5   5   ...  0   \n","6   6   ...  0   \n","7   7   ...  0   \n","8   8   ...  0   \n","9   9   ...  0   \n","10  10  ...  0   \n","11  11  ...  0   \n","12  12  ...  0   \n","13  13  ...  1   \n","14  14  ...  0   \n","15  15  ...  1   \n","16  16  ...  0   \n","17  17  ...  0   \n","18  18  ...  0   \n","19  19  ...  0   \n","20  20  ...  0   \n","21  21  ...  0   \n","22  22  ...  0   \n","23  23  ...  0   \n","24  24  ...  0   \n","25  25  ...  0   \n","26  26  ...  0   \n","27  27  ...  0   \n","28  28  ...  0   \n","29  29  ...  1   \n","\n","[30 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"6uMuDR0pqChY","colab_type":"text"},"source":["# Loading in development data. Also specifying label and text columns\n","\n","We store the name of the Data column containing the text we wish to classify and the name of the corresponding label column in global variables for ease of access down line and also so this code is generalizable.\n","\n","Label list is just a 0 or a 1 because the version of BERT we've created below only deals in binary classifcation and labels must be ints"]},{"cell_type":"code","metadata":{"id":"IuMOGwFui4it","colab_type":"code","outputId":"b42387f6-a056-4070-8b0c-fc2484ce2bca","executionInfo":{"status":"ok","timestamp":1584115262386,"user_tz":0,"elapsed":74373,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["dev = loadData(rawDev)\n","\n","DATA_COLUMN = 'tweet'\n","LABEL_COLUMN = 'label'\n","# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = [0, 1]\n","\n","print(\"Size of training data\", len(train.index))\n","print(\"Size of development data\", len(dev.index), '\\n')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Size of training data 9000\n","Size of development data 1000 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HhIvI0kDXoUq","colab_type":"text"},"source":["## Import Custom BERT Repositry\n","\n","Instead of importing functions from the official BERT repo I have opted to import these functions from my custom, forked BERT repositry.\n","\n","This allows for a more custom approach when building the BERT model and fine-tuning it. It also is a much more clean way of organising this project, rather than copying entire functions from the BERT repo in this notebook and modifying them here\n","\n","The`create_model` function in my BERT repo will be edited later, the default function from BERT simply creates a single layer that will be trained to adapt BERT to our task (i.e. classifying whether a tweet is hate speech or not). This strategy of using a mostly trained model is called <i>fine-tuning</i>.\n","\n","My hope is that later I will test more complex methods in fine-tuning, such as CNNs, RNNs and large scale LTSMs.\n","\n","Also the `model_fn` method in my BERT repo is less verbose than the default one, as well as providing far more detailed metrics than just accuracy and loss, such as F - Score, AUC, precision and recall - so I can better analyse the performance of different models"]},{"cell_type":"code","metadata":{"id":"ACDvt5FYXvvm","colab_type":"code","outputId":"f7e94b70-a662-4820-ca6e-ea2422397a11","executionInfo":{"status":"ok","timestamp":1584115264393,"user_tz":0,"elapsed":76370,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["!git clone https://github.com/fionnmcconville/bert.git\n","%cd bert\n","import run_classifier\n","import optimization\n","import tokenization\n","import modeling\n","#Return to original workspace\n","%cd .."],"execution_count":12,"outputs":[{"output_type":"stream","text":["Cloning into 'bert'...\n","remote: Enumerating objects: 360, done.\u001b[K\n","Receiving objects:   0% (1/360)   \rReceiving objects:   1% (4/360)   \rReceiving objects:   2% (8/360)   \rReceiving objects:   3% (11/360)   \rReceiving objects:   4% (15/360)   \rReceiving objects:   5% (18/360)   \rReceiving objects:   6% (22/360)   \rReceiving objects:   7% (26/360)   \rReceiving objects:   8% (29/360)   \rReceiving objects:   9% (33/360)   \rReceiving objects:  10% (36/360)   \rReceiving objects:  11% (40/360)   \rReceiving objects:  12% (44/360)   \rReceiving objects:  13% (47/360)   \rReceiving objects:  14% (51/360)   \rReceiving objects:  15% (54/360)   \rReceiving objects:  16% (58/360)   \rReceiving objects:  17% (62/360)   \rReceiving objects:  18% (65/360)   \rReceiving objects:  19% (69/360)   \rReceiving objects:  20% (72/360)   \rReceiving objects:  21% (76/360)   \rReceiving objects:  22% (80/360)   \rReceiving objects:  23% (83/360)   \rReceiving objects:  24% (87/360)   \rReceiving objects:  25% (90/360)   \rReceiving objects:  26% (94/360)   \rReceiving objects:  27% (98/360)   \rReceiving objects:  28% (101/360)   \rReceiving objects:  29% (105/360)   \rReceiving objects:  30% (108/360)   \rReceiving objects:  31% (112/360)   \rReceiving objects:  32% (116/360)   \rReceiving objects:  33% (119/360)   \rReceiving objects:  34% (123/360)   \rReceiving objects:  35% (126/360)   \rReceiving objects:  36% (130/360)   \rReceiving objects:  37% (134/360)   \rReceiving objects:  38% (137/360)   \rReceiving objects:  39% (141/360)   \rReceiving objects:  40% (144/360)   \rReceiving objects:  41% (148/360)   \rReceiving objects:  42% (152/360)   \rReceiving objects:  43% (155/360)   \rReceiving objects:  44% (159/360)   \rReceiving objects:  45% (162/360)   \rremote: Total 360 (delta 0), reused 0 (delta 0), pack-reused 360\u001b[K\n","Receiving objects: 100% (360/360), 304.34 KiB | 3.85 MiB/s, done.\n","Resolving deltas: 100% (199/199), done.\n","/content/bert\n","WARNING:tensorflow:From /content/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V399W0rqNJ-Z","colab_type":"text"},"source":["#Data Preprocessing\n","We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n","\n","- `text_a` is the text we want to classify, which in this case, is the `tweet` field in our Dataframe. \n","- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n","- `label` is the label for our example, i.e. True, False"]},{"cell_type":"code","metadata":{"id":"p9gEt5SmM6i6","colab_type":"code","colab":{}},"source":["# Use the InputExample class from BERT's run_classifier code to create examples from the data\n","train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for book-keeping, unused in this example\n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)\n","\n","dev_InputExamples = dev.apply(lambda x: run_classifier.InputExample(guid=None, \n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCZWZtKxObjh","colab_type":"text"},"source":["Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n","\n","\n","1. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n","2. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n","3. Map our words to indexes using a vocab file that BERT provides\n","4. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n","5. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n","\n","Happily, we don't have to worry about most of these details. It's automated with the below inbuilt functions\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Aws4Q_SXZENG","colab_type":"text"},"source":["Below is a way to retrieve desired BERT parameters, such as it's pre-trained checkpoints and it's vocab file, from my google storage bucket where I've downloaded the uncased LARGE version of bert."]},{"cell_type":"code","metadata":{"id":"UtZavIhEaWF5","colab_type":"code","outputId":"43c8d692-8f1d-46d3-e879-1f9c972c42f6","executionInfo":{"status":"ok","timestamp":1584115265369,"user_tz":0,"elapsed":77331,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":462}},"source":["bucket_dir = 'gs://csc3002'\n","bert_ckpt_dir = os.path.join(bucket_dir, bert_model_name)\n","\n","#For further pretrained model\n","if FURTHER_PRETRAINED:\n","  further_pretrained_model = os.path.join(bert_model_name, 'further_pretrained_model1')\n","  further_pretrained_model = os.path.join(bucket_dir, further_pretrained_model)\n","  bert_ckpt_file = tf.train.latest_checkpoint(further_pretrained_model)\n","  print(\"\\nUsing BERT checkpoint from directory:\", os.path.join(further_pretrained_model))\n","\n","else:\n","  bert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n","  print(\"\\nUsing BERT checkpoint from directory:\", bert_ckpt_dir)\n","\n","print(\"\\nBERT checkpoint file is:\", bert_ckpt_file)\n","\n","#Setting up BERT config, vocab file and tokenizer - all default from the BERT repo\n","bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")\n","vocab_file = os.path.join(bert_ckpt_dir, \"vocab.txt\")\n","\n","if params['replaceEmoji_v2'] == True:\n","  vocab_file = os.path.join(bert_ckpt_dir, \"vocab1.txt\")\n","  \n","tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file)\n","\n","\n","print(\"\\nMake sure that the function loads a checkpoint, if it doesn't an error will be thrown here\")\n","assert bert_ckpt_file is not None, \"No BERT checkpoint file loaded\"\n","\n","print(\"\\nUsing vocab file\\n\", vocab_file)\n","print(\"\\nBelow is an example of the BERT tokenizer in action\")\n","tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["\n","Using BERT checkpoint from directory: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1\n","\n","BERT checkpoint file is: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/model.ckpt-40000\n","WARNING:tensorflow:From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","\n","Make sure that the function loads a checkpoint, if it doesn't an error will be thrown here\n","\n","Using vocab file\n"," gs://csc3002/wwm_uncased_L-24_H-1024_A-16/vocab.txt\n","\n","Below is an example of the BERT tokenizer in action\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['this',\n"," 'here',\n"," \"'\",\n"," 's',\n"," 'an',\n"," 'example',\n"," 'of',\n"," 'using',\n"," 'the',\n"," 'bert',\n"," 'token',\n"," '##izer']"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"0OEzfFIt6GIc","colab_type":"text"},"source":["Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cm8RLoJ31WLa","outputId":"ef54735e-a143-46d6-9e4a-cf3afd045d5a","executionInfo":{"status":"ok","timestamp":1584115270369,"user_tz":0,"elapsed":82322,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# BERT is limited to 512 tokens in length\n","MAX_SEQ_LENGTH = 256\n","# Convert our train and dev features to InputFeatures that BERT understands.\n","train_features =  run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","dev_features = run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /content/bert/run_classifier.py:786: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n","INFO:tensorflow:Writing example 0 of 9000\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] how come allah is not helping you it is up to christian countries to protect you feed you , the countries hit by violence from islam take refugees in feed them etc ##ple ##ase no more explaining about your hard times we are doing our best for u ##yes there is good and bad every where [SEP]\n","INFO:tensorflow:input_ids: 101 2129 2272 16455 2003 2025 5094 2017 2009 2003 2039 2000 3017 3032 2000 4047 2017 5438 2017 1010 1996 3032 2718 2011 4808 2013 7025 2202 8711 1999 5438 2068 4385 10814 11022 2053 2062 9990 2055 2115 2524 2335 2057 2024 2725 2256 2190 2005 1057 23147 2045 2003 2204 1998 2919 2296 2073 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] with today ##s ja ##lal ##abad attack and other vicious attacks claimed by isis , i smell a spill ##over of refugees in pak again . this time we should not open borders for them . we can ##t afford terrorists taking und ##ue advantage . let americans deal with it . # afghanistan ja ##lal ##abad [SEP]\n","INFO:tensorflow:input_ids: 101 2007 2651 2015 14855 13837 10542 2886 1998 2060 13925 4491 3555 2011 18301 1010 1045 5437 1037 14437 7840 1997 8711 1999 22190 2153 1012 2023 2051 2057 2323 2025 2330 6645 2005 2068 1012 2057 2064 2102 8984 15554 2635 6151 5657 5056 1012 2292 4841 3066 2007 2009 1012 1001 7041 14855 13837 10542 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] greatest threat to austrian security , says top military figure . eu and europe bitterly divided ##ma ##jo ##r confrontation ##s between the two . nothing more counter ##pro ##ductive than centers on european territory or euro bribes for migrants . # vis ##eg ##rad v ##4 [SEP]\n","INFO:tensorflow:input_ids: 101 4602 5081 2000 6161 3036 1010 2758 2327 2510 3275 1012 7327 1998 2885 19248 4055 2863 5558 2099 13111 2015 2090 1996 2048 1012 2498 2062 4675 21572 26638 2084 6401 2006 2647 3700 2030 9944 29117 2005 16836 1012 1001 25292 13910 12173 1058 2549 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] when all your friends are out ho ##e ' in and you ' re stuck at home in a shitty relationship [SEP]\n","INFO:tensorflow:input_ids: 101 2043 2035 2115 2814 2024 2041 7570 2063 1005 1999 1998 2017 1005 2128 5881 2012 2188 1999 1037 28543 3276 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] i wonder if rick will make another deal with those crazy ass women thinking face and if that crazy ass ni ##gga will actually ho ##e daryl again neutral face [SEP]\n","INFO:tensorflow:input_ids: 101 1045 4687 2065 6174 2097 2191 2178 3066 2007 2216 4689 4632 2308 3241 2227 1998 2065 2008 4689 4632 9152 23033 2097 2941 7570 2063 22514 2153 8699 2227 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:Writing example 0 of 1000\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] fight russia now you pussy pot ##us . [SEP]\n","INFO:tensorflow:input_ids: 101 2954 3607 2085 2017 22418 8962 2271 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] hai ##nes city , fl ##a wil ##iba ##ld ##o garcia an illegal alien from mexico charged with sexual battery on an 8 y ##r old girl . garcia has an alias lorenzo sip ##rian ##o with prior criminal history . # ms ##ms ##ile ##nce # build ##tha ##t ##wall [SEP]\n","INFO:tensorflow:input_ids: 101 15030 5267 2103 1010 13109 2050 19863 18410 6392 2080 7439 2019 6206 7344 2013 3290 5338 2007 4424 6046 2006 2019 1022 1061 2099 2214 2611 1012 7439 2038 2019 14593 12484 10668 6862 2080 2007 3188 4735 2381 1012 1001 5796 5244 9463 5897 1001 3857 8322 2102 9628 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] study con ##fl ##ants illegal immigrants with legal immigrants to get a low crime number ##r trump mag ##a red nation rising red hen [SEP]\n","INFO:tensorflow:input_ids: 101 2817 9530 10258 11390 6206 7489 2007 3423 7489 2000 2131 1037 2659 4126 2193 2099 8398 23848 2050 2417 3842 4803 2417 21863 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] already won most hysterical woman with her 2 day twitter freak ##out over leg room on a plane . [SEP]\n","INFO:tensorflow:input_ids: 101 2525 2180 2087 25614 2450 2007 2014 1016 2154 10474 11576 5833 2058 4190 2282 2006 1037 4946 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] you ' re just a ska ##nk and everyone knows it otherwise you wouldn ' t be making excuses for not settling down whilst hanging out with multiple different guys . [SEP]\n","INFO:tensorflow:input_ids: 101 2017 1005 2128 2074 1037 24053 8950 1998 3071 4282 2009 4728 2017 2876 1005 1056 2022 2437 21917 2005 2025 9853 2091 5819 5689 2041 2007 3674 2367 4364 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5gdp568cyB3M","colab_type":"text"},"source":["The Run config will be the same across all evaluation options below for running BERT. In it we define the amount of summary steps, as well as how often we should checkpoint the model"]},{"cell_type":"code","metadata":{"id":"QPYzNbvcx94h","colab_type":"code","outputId":"8b4b1641-9a34-4926-b86a-50620b73fa15","executionInfo":{"status":"ok","timestamp":1584115270370,"user_tz":0,"elapsed":82312,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["# Model configs\n","SAVE_CHECKPOINTS_STEPS = 1000\n","run_config = tf.compat.v1.estimator.tpu.RunConfig(  \n","    #I think the output file must be a sub-directory of the main BERT file\n","    model_dir=OUTPUT_DIR, \n","    tf_random_seed=SEED,\n","    cluster=cluster_resolver,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=100,    #Shows us summary metrics every 100 steps\n","        num_shards=8,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","print(run_config.session_config)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.1.124.234:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZY_71Lwmxwm1","colab_type":"text"},"source":["### Setting up fine tuning model configurations and parameters"]},{"cell_type":"code","metadata":{"id":"OjwJ4bTeWXD8","colab_type":"code","outputId":"0e4cc305-86ab-476d-bda6-e41efe309c61","executionInfo":{"status":"ok","timestamp":1584115270371,"user_tz":0,"elapsed":82304,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["# Compute train and warmup steps from batch size\n","TRAIN_BATCH_SIZE = 32 #recommended 16 or 32\n","EVAL_BATCH_SIZE = 8\n","PREDICT_BATCH_SIZE = 8\n","LEARNING_RATE = 2e-5 # Recommended 5e-5, 3e-5 or 2e-5\n","NUM_TRAIN_EPOCHS = 3.0 # Recommended 2, 3 or 4\n","MAX_SEQ_LENGTH = 256\n","# Warmup is a period of time where the learning rate \n","#is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","\n","# Compute # train and warmup steps from batch size\n","num_train_steps = int(len(train_features) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","print(\"The model will stop training when it reaches\", num_train_steps, \"as a checkpoint\")\n","print(\"\\nThe bert checkpoint directory is\", bert_ckpt_dir)\n","print(\"\\nThe output directory is\", OUTPUT_DIR, '\\n')\n","\n","#This is the model function, which feeds in the bert configurations, the pretrained model itself and the parameters for the fine tuning of the model\n","model_fn = run_classifier.model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  use_one_hot_embeddings=True)\n","\n","#We use Tensorflow estimators to train, evaluate and test our model\n","#estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","estimator = tf.contrib.tpu.TPUEstimator(\n","    use_tpu=True,\n","    model_fn=model_fn,\n","    config=run_config,\n","    train_batch_size=TRAIN_BATCH_SIZE,\n","    eval_batch_size=EVAL_BATCH_SIZE,\n","    predict_batch_size=PREDICT_BATCH_SIZE)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["The model will stop training when it reaches 843 as a checkpoint\n","\n","The bert checkpoint directory is gs://csc3002/wwm_uncased_L-24_H-1024_A-16\n","\n","The output directory is gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output \n","\n","WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f1a7b9ac400>) includes params argument, but params are not passed to Estimator.\n","INFO:tensorflow:Using config: {'_model_dir': 'gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output', '_tf_random_seed': 3060, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.1.124.234:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1a76009cc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.1.124.234:8470', '_evaluation_master': 'grpc://10.1.124.234:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f1a87b59438>}\n","INFO:tensorflow:_TPUContext: eval_on_tpu True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7siI74tc27eN","colab_type":"text"},"source":["Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator.\n","\n","This is a pretty standard design pattern for working with Tensorflow Estimators"]},{"cell_type":"code","metadata":{"id":"JUu_zpYV25z5","colab_type":"code","colab":{}},"source":["# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=True,)\n","\n","# Input function for dev data, we feed in our previously created dev_features for this\n","test_input_fn = run_classifier.input_fn_builder(\n","    features=dev_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qp8UD9h13SvX","colab_type":"text"},"source":["## Simple Train and then Evaluate\n","<b>Now we train our BERT fine-tuned model"]},{"cell_type":"code","metadata":{"id":"vkNOJkVJ3SUh","colab_type":"code","outputId":"c5111c62-8125-4901-c4c2-153d17451355","executionInfo":{"status":"ok","timestamp":1582723737403,"user_tz":0,"elapsed":978409,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["print(\"\\nThe model will stop training when it reaches\", num_train_steps, \"as a checkpoint\")\n","print(f'Beginning Training!')\n","current_time = datetime.now()\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)#, hooks=[evaluator])\n","train_time = datetime.now() - current_time\n","print(\"Training took time \", train_time)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","The model will stop training when it reaches 840 as a checkpoint\n","Beginning Training!\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.8.208.162:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 15396571222743673123)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 3033160054236891001)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 16028321910086008709)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10458696249324074875)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 2330865355022013731)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 886758388565652479)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 7117416557114201466)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6917482831568383810)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7523899006997986303)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 64253279562885317)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9984127101897606442)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","INFO:tensorflow:Calling model_fn.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","WARNING:tensorflow:From /content/bert/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /content/bert/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From /content/bert/bert/run_classifier.py:651: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/run_classifier.py:660: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/run_classifier.py:661: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n","\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output/model.ckpt.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:751: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer Variable.assign which has equivalent behavior in 2.X.\n","INFO:tensorflow:Initialized dataset iterators in 1 seconds\n","INFO:tensorflow:Installing graceful shutdown hook.\n","INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 4 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:Saving checkpoints for 100 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output/model.ckpt.\n","INFO:tensorflow:loss = 0.06742204, step = 100\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (1, 0)\n","INFO:tensorflow:Saving checkpoints for 200 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output/model.ckpt.\n","INFO:tensorflow:loss = 1.086492, step = 200 (83.288 sec)\n","INFO:tensorflow:global_step/sec: 1.20066\n","INFO:tensorflow:examples/sec: 38.421\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (2, 0)\n","INFO:tensorflow:Saving checkpoints for 300 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output/model.ckpt.\n","INFO:tensorflow:loss = 0.01913736, step = 300 (76.492 sec)\n","INFO:tensorflow:global_step/sec: 1.30732\n","INFO:tensorflow:examples/sec: 41.8341\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (3, 0)\n","INFO:tensorflow:Saving checkpoints for 400 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output/model.ckpt.\n","INFO:tensorflow:loss = 1.4245027, step = 400 (77.259 sec)\n","INFO:tensorflow:global_step/sec: 1.29435\n","INFO:tensorflow:examples/sec: 41.4193\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (4, 0)\n","INFO:tensorflow:Saving checkpoints for 500 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output/model.ckpt.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","INFO:tensorflow:loss = 0.0111955125, step = 500 (72.308 sec)\n","INFO:tensorflow:global_step/sec: 1.38298\n","INFO:tensorflow:examples/sec: 44.2552\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (5, 0)\n","INFO:tensorflow:Saving checkpoints for 600 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output/model.ckpt.\n","INFO:tensorflow:loss = 0.0027012776, step = 600 (73.514 sec)\n","INFO:tensorflow:global_step/sec: 1.36027\n","INFO:tensorflow:examples/sec: 43.5288\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (6, 0)\n","INFO:tensorflow:Saving checkpoints for 700 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output/model.ckpt.\n","INFO:tensorflow:loss = 0.0019017966, step = 700 (74.162 sec)\n","INFO:tensorflow:global_step/sec: 1.34839\n","INFO:tensorflow:examples/sec: 43.1486\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (7, 0)\n","INFO:tensorflow:Saving checkpoints for 800 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output/model.ckpt.\n","INFO:tensorflow:loss = 0.0014956653, step = 800 (75.778 sec)\n","INFO:tensorflow:global_step/sec: 1.31966\n","INFO:tensorflow:examples/sec: 42.2291\n","INFO:tensorflow:Enqueue next (40) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (40) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (8, 0)\n","INFO:tensorflow:loss = 1.4809785, step = 840 (12.313 sec)\n","INFO:tensorflow:global_step/sec: 3.24852\n","INFO:tensorflow:examples/sec: 103.953\n","INFO:tensorflow:Saving checkpoints for 840 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output/model.ckpt.\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Loss for final step: 1.4809785.\n","INFO:tensorflow:training_loop marked as finished\n","Training took time  0:16:01.535748\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T_MPSFxy3gNG","colab_type":"text"},"source":["<b>And now we evaluate the performance of our model on the development data<b>"]},{"cell_type":"code","metadata":{"id":"uPLbCsbK3zHW","colab_type":"code","outputId":"d2b38dc6-bc3f-4a1c-f986-142d4fb56205","executionInfo":{"status":"ok","timestamp":1582724086499,"user_tz":0,"elapsed":236165,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#You need to provide number of steps for a TPU\n","eval_steps = int(len(dev_features) / EVAL_BATCH_SIZE)\n","\n","#Eval will be slightly WRONG on the TPU because it will drop the last batch (drop_remainder = True).\n","estimator.evaluate(input_fn=test_input_fn, steps=eval_steps)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (1, 256)\n","INFO:tensorflow:  name = input_mask, shape = (1, 256)\n","INFO:tensorflow:  name = label_ids, shape = (1,)\n","INFO:tensorflow:  name = segment_ids, shape = (1, 256)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3322: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","WARNING:tensorflow:From /content/bert/bert/run_classifier.py:683: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/run_classifier.py:684: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/run_classifier.py:686: The name tf.metrics.auc is deprecated. Please use tf.compat.v1.metrics.auc instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/run_classifier.py:687: The name tf.metrics.recall is deprecated. Please use tf.compat.v1.metrics.recall instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/run_classifier.py:688: The name tf.metrics.precision is deprecated. Please use tf.compat.v1.metrics.precision instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/run_classifier.py:689: The name tf.metrics.true_positives is deprecated. Please use tf.compat.v1.metrics.true_positives instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/run_classifier.py:690: The name tf.metrics.true_negatives is deprecated. Please use tf.compat.v1.metrics.true_negatives instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/run_classifier.py:691: The name tf.metrics.false_positives is deprecated. Please use tf.compat.v1.metrics.false_positives instead.\n","\n","WARNING:tensorflow:From /content/bert/bert/run_classifier.py:692: The name tf.metrics.false_negatives is deprecated. Please use tf.compat.v1.metrics.false_negatives instead.\n","\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-02-26T13:31:03Z\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output/model.ckpt-840\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 9 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Initialized dataset iterators in 0 seconds\n","INFO:tensorflow:Enqueue next (124) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (124) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:Evaluation [124/124]\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Finished evaluation at 2020-02-26-13:34:39\n","INFO:tensorflow:Saving dict for global step 840: F1_Score = 0.77377045, auc = 0.7965411, eval_accuracy = 0.79133064, eval_loss = 1.067598, false_negatives = 71.0, false_positives = 136.0, global_step = 840, loss = 1.1221331, precision = 0.722449, recall = 0.8329412, true_negatives = 431.0, true_positives = 354.0\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 840: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/output/model.ckpt-840\n","INFO:tensorflow:evaluation_loop marked as finished\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'F1_Score': 0.77377045,\n"," 'auc': 0.7965411,\n"," 'eval_accuracy': 0.79133064,\n"," 'eval_loss': 1.067598,\n"," 'false_negatives': 71.0,\n"," 'false_positives': 136.0,\n"," 'global_step': 840,\n"," 'loss': 1.1221331,\n"," 'precision': 0.722449,\n"," 'recall': 0.8329412,\n"," 'true_negatives': 431.0,\n"," 'true_positives': 354.0}"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"q8YcAb0YP6Ju","colab_type":"text"},"source":["## `tf.train_and_evaluate` function for tensorflow\n","\n","### Setting the Run Config for Tensorflow train_and evaluate\n","\n","Setting the TF_CONFIG environment variable so we can use the `train_and_evaluate` function. We need to set this because we need to explicitly define the roles of each node in our TPU cluster for this function so training and evaluation can run in concurrence. Otherwise the function will never evaluate the model as it will be too busy using all of the TPU resources to train the model unless told otherwise\n"]},{"cell_type":"code","metadata":{"id":"kOC6er-nFDC5","colab_type":"code","colab":{}},"source":["#cluster_resolver.cluster_spec().as_dict()  - shows the json for the current cluster as defined by the cluster resolver\n","\"\"\"def _cluster():\n","    return {'worker': [TPU_ADDRESS,TPU_ADDRESS, TPU_ADDRESS],\n","             'ps': [TPU_ADDRESS, TPU_ADDRESS],\n","             'chief': [TPU_ADDRESS]}\n","\n","def _set_tf_config():\n","    tf_config = {\n","            'cluster': _cluster(),\n","            'task': {'type': 'worker', 'index': 0}}\n","    os.environ['TF_CONFIG'] = json.dumps(tf_config)\n","\n","_set_tf_config()\n","print(os.environ['TF_CONFIG'])\n","\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 1000\n","run_config = tf.compat.v1.estimator.tpu.RunConfig(  \n","    #I think the output file must be a sub-directory of the main BERT file\n","    model_dir=OUTPUT_DIR, \n","    tf_random_seed=SEED,\n","    cluster=cluster_resolver,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=100,    #Shows us summary metrics every 100 steps\n","        num_shards=8,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","print(run_config.session_config)\n","\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivL-NEcjFc-r","colab_type":"text"},"source":["#### Evaluate whilst training classifier and export best performing model\n","\n","Requires a factory reset runtime after run. The hope is to create a function which exports only the best model. I believe that we need two machines to run training and evaluation in concurrence. I only have one TPU worker in the colab environment.\n","\n","The function may possibly be finagled into working, howevere many sources on the internet dispute it can work at all, including this medium article https://medium.com/tensorflow/how-to-write-a-custom-estimator-model-for-the-cloud-tpu-7d8bd9068c26\n","\n","Besides, the hard-coded train_and_evaluate function I have works, it my be slower but it is quicker to just use it, rather than try and find a solution for the the `tf.train_and_evaluate` function - which is not readily available at this time"]},{"cell_type":"code","metadata":{"id":"k8JuLvrdPySs","colab_type":"code","outputId":"46e6300c-955c-4521-f1a5-46cf1f64e204","executionInfo":{"status":"ok","timestamp":1583231145752,"user_tz":0,"elapsed":1446,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["\"\"\"#Below is the serving Input function which is to be used with the BestExporter function of the estimator.\n","#In the function we define what sort of inputs are expected when we predict the model\n","def serving_input_fn():\n","    with tf.compat.v1.variable_scope(\"foo\"):\n","      feature_spec = {\n","          \"input_ids\": tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n","          \"input_mask\": tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n","          \"segment_ids\": tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n","          \"label_ids\": tf.FixedLenFeature([], tf.int64),\n","        }\n","      serialized_tf_example = tf.placeholder(dtype=tf.int64,\n","                                             shape=[1, MAX_SEQ_LENGTH])\n","      receiver_tensors = {'input_ids': serialized_tf_example}\n","      features = tf.parse_example(serialized_tf_example, feature_spec)\n","      return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n","\n","def compare_fn(best_eval_result, current_eval_result):\n","\n","  #F-Score isn't available, besides AUC is a good metric too.\n","  default_key = metric_keys.MetricKeys.AUC\n","\n","  if not best_eval_result or default_key not in best_eval_result:\n","    raise ValueError(\n","        'best_eval_result cannot be empty or no loss is found in it.')\n","\n","  if not current_eval_result or default_key not in current_eval_result:\n","    raise ValueError(\n","        'current_eval_result cannot be empty or no loss is found in it.')\n","\n","  return best_eval_result[default_key] > current_eval_result[default_key]\n","  \n","\n","train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=1000)\n","estimator._export_to_tpu = False\n","best_exporter = tf.estimator.BestExporter(serving_input_receiver_fn=serving_input_fn, compare_fn = compare_fn, exports_to_keep=5)\n","exporters = [best_exporter]\n","eval_spec = tf.estimator.EvalSpec(input_fn=test_input_fn, start_delay_secs = 10, throttle_secs = 30, exporters=exporters)\n","#Can only be called once per run as not all gRPC servers from the first call have been closed\n","\n","#tf.estimator.train_and_evaluate does not seem to work properly with TPUs. \n","#It cannot handle the distributed TPU strategy seemingly, or perhaps there are other problems\n","model = tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'#Below is the serving Input function which is to be used with the BestExporter function of the estimator.\\n#In the function we define what sort of inputs are expected when we predict the model\\ndef serving_input_fn():\\n    with tf.compat.v1.variable_scope(\"foo\"):\\n      feature_spec = {\\n          \"input_ids\": tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\\n          \"input_mask\": tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\\n          \"segment_ids\": tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\\n          \"label_ids\": tf.FixedLenFeature([], tf.int64),\\n        }\\n      serialized_tf_example = tf.placeholder(dtype=tf.int64,\\n                                             shape=[1, MAX_SEQ_LENGTH])\\n      receiver_tensors = {\\'input_ids\\': serialized_tf_example}\\n      features = tf.parse_example(serialized_tf_example, feature_spec)\\n      return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\\n\\ndef compare_fn(best_eval_result, current_eval_result):\\n\\n  #F-Score isn\\'t available, besides AUC is a good metric too.\\n  default_key = metric_keys.MetricKeys.AUC\\n\\n  if not best_eval_result or default_key not in best_eval_result:\\n    raise ValueError(\\n        \\'best_eval_result cannot be empty or no loss is found in it.\\')\\n\\n  if not current_eval_result or default_key not in current_eval_result:\\n    raise ValueError(\\n        \\'current_eval_result cannot be empty or no loss is found in it.\\')\\n\\n  return best_eval_result[default_key] > current_eval_result[default_key]\\n  \\n\\ntrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=1000)\\nestimator._export_to_tpu = False\\nbest_exporter = tf.estimator.BestExporter(serving_input_receiver_fn=serving_input_fn, compare_fn = compare_fn, exports_to_keep=5)\\nexporters = [best_exporter]\\neval_spec = tf.estimator.EvalSpec(input_fn=test_input_fn, start_delay_secs = 10, throttle_secs = 30, exporters=exporters)\\n#Can only be called once per run as not all gRPC servers from the first call have been closed\\n\\n#tf.estimator.train_and_evaluate does not seem to work properly with TPUs. \\n#It cannot handle the distributed TPU strategy seemingly, or perhaps there are other problems\\nmodel = tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"SHqEs8G6C8X7","colab_type":"text"},"source":["## Running Evaluation Whilst Training - Hard-coded function\n","\n","Below is a custom function to run evaluation on the fine-tuned BERT model whilst training. A grid search is quite inefficient for obtaining the best model as there are so many different values of steps one can attempt. \n","\n","The function below solves this by identifying the optimum number of steps the fine-tuning should run for based upon evaluation metrics by testing the trained model to that point against the dev set\n","\n","Presently the problem is that every time we want to run an evaluation within training, tensorflow restores the parameters from the most recent checkpoint which has a lot of overhead as BERT large is a huge model. There is a way to overcome this in GPU training with the session hook InMemoryEvaluationHook in conjunction with estimator.train - however this does not work with TPUs. As the amount of data we train on is not that much, this is not a big problem"]},{"cell_type":"code","metadata":{"id":"fQsD0qy6DE5U","colab_type":"code","colab":{}},"source":["#We'll set a large value for train steps because we want to make this model run\n","#for as long as possible before it finds the optimimum model\n","hparams = {'train_steps': 3000, \n","            'train_batch_size': 32,\n","            'eval_batch_size': 8,\n","            'use_tpu': True,\n","            'num_train_features': len(train_features),\n","            'num_eval_features': len(dev_features),\n","           'learning_rate': 2e-5 \n","            }\n","if DATASET == 'AnalyticsVidhya':\n","  hparams['train_steps'] = 12000\n","          \n","def load_global_step_from_checkpoint_dir(checkpoint_dir):\n","  try:\n","    checkpoint_reader = tf.train.NewCheckpointReader(\n","        tf.train.latest_checkpoint(checkpoint_dir))\n","    return checkpoint_reader.get_tensor(tf.GraphKeys.GLOBAL_STEP)\n","  except:  \n","    return 0\n","\n","def train_and_evaluate(out_dir, hparams, steps_per_eval):\n","\n","#Delete prior model graph, checkpoints and eval files to enable consecutive runs, rather than resetting runtime\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","  # Doesn't matter if the directory didn't exist\n","    pass\n","  tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","  max_steps = hparams['train_steps']\n","  train_batch_size = hparams['train_batch_size']\n","  eval_batch_size = hparams['eval_batch_size']\n","  print('\\ntrain_batch_size={:d}  eval_batch_size={:d}  max_steps={:d}'.format(\n","                  train_batch_size,\n","                  eval_batch_size,\n","                  max_steps))\n","\n","  config = tf.contrib.tpu.RunConfig(\n","    cluster=cluster_resolver,\n","    model_dir=out_dir,\n","    save_checkpoints_steps=steps_per_eval,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","      iterations_per_loop=steps_per_eval,\n","      per_host_input_for_training=True))\n","\n","  model_fn = run_classifier.model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=hparams['learning_rate'],\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  use_one_hot_embeddings=True)\n","\n","  estimator = tf.contrib.tpu.TPUEstimator(  # TPU change 4\n","    model_fn=model_fn,\n","    config=config,\n","    params=hparams,\n","    model_dir=out_dir,\n","    train_batch_size=train_batch_size,\n","    eval_batch_size=eval_batch_size,\n","    use_tpu=True\n","  )\n"," # load last checkpoint and start from there\n","  current_step = load_global_step_from_checkpoint_dir(out_dir)\n","  steps_per_epoch = hparams['num_train_features'] // train_batch_size\n","  print('\\nTraining for {:d} steps ({:2f} epochs in total). Current'\n","                  ' step {:d}.'.format(\n","                  max_steps,\n","                  max_steps / steps_per_epoch,\n","                  current_step))\n","\n","  start_timestamp = time.time()  # This time will include compilation time\n","  best_score = 0\n","  best_model = 0\n","  while current_step < max_steps:\n","    # Train for up to steps_per_eval number of steps.\n","    # At the end of training, a checkpoint will be written to --model_dir.\n","    next_checkpoint = min(current_step + steps_per_eval, max_steps)\n","    estimator.train(input_fn=train_input_fn, max_steps=next_checkpoint)\n","    current_step = next_checkpoint\n","    print('\\nFinished training up to step {:d}. Elapsed seconds {:d}.\\n'.format(\n","                    next_checkpoint, int(time.time() - start_timestamp)))\n","\n","    print('\\nStarting to evaluate at step {:d} \\n'.format(next_checkpoint))\n","    eval_results = estimator.evaluate(\n","      input_fn=test_input_fn,\n","      steps=hparams['num_eval_features'] // eval_batch_size)\n","    print('\\nEval results at step {:d}: \\n'.format(next_checkpoint), eval_results)\n","    \n","    current_score = eval_results['F1_Score']\n","    if current_score > best_score:\n","      best_score = current_score \n","      best_model = current_step\n","      score_buffer = [] #Reset buffer\n","    else:\n","      score_buffer.append(current_score)\n","    #If 3 times in a row evaluation results haven't improved; we stop training\n","    if len(score_buffer) == 3:\n","      elapsed_time = int(time.time() - start_timestamp)\n","      \n","      print('\\nFinished training at step {:d} as there has been no improvement on the previous 3 iterations'.format(current_step),\n","      '\\nElapsed seconds {:d}. \\n'.format(elapsed_time), \n","      \"\\nBest model is at step {:d} with the best F-score {:d}\".format(best_model, best_score),\n","      \"\\nNow edit the protocol buffer file and set the most recent step to\", best_model,\n","            \"so this model checkpoint can be loaded using the tf.train.latest_checkpoint function\")\n","      \n","      return best_model\n","    \n","\n","  elapsed_time = int(time.time() - start_timestamp)\n","  print('\\nFinished training up to step {:d}. Elapsed seconds {:d}. \\n'.format(max_steps, elapsed_time))\n","  return best_model\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"keoF6rLIDqzl","colab_type":"text"},"source":["Now run the train_and_evaluate function. We can toggle the steps_pereval in the params to control how often we checkpoint and evaluate"]},{"cell_type":"code","metadata":{"id":"8YHGavBJDqhY","colab_type":"code","outputId":"a28f7513-7b48-4fe1-ca21-3ad1ce14d529","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["best_step = train_and_evaluate(OUTPUT_DIR, hparams, steps_per_eval=1000) # Will return the optimum step for the BERT model\n","print(\"\\nBest step for model is at\", best_step)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","train_batch_size=32  eval_batch_size=8  max_steps=12000\n","INFO:tensorflow:Using config: {'_model_dir': 'gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.103.143.2:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8a7f9e16a0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.103.143.2:8470', '_evaluation_master': 'grpc://10.103.143.2:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f8a91f59278>}\n","INFO:tensorflow:_TPUContext: eval_on_tpu True\n","\n","Training for 12000 steps (12.072435 epochs in total). Current step 0.\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.103.143.2:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6950939721587658405)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 7214882111008902385)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11723597538653886847)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10168099005517632449)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11977576496132178308)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 261289063090977959)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6461506172735637731)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 18118260143106526205)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 14835276859969354722)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 8603088287462879159)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10377542404254460937)\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output/model.ckpt.\n","INFO:tensorflow:Initialized dataset iterators in 3 seconds\n","INFO:tensorflow:Installing graceful shutdown hook.\n","INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 8 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VaGhttd75vOv","colab_type":"text"},"source":["# Cross Validation evaluation\n","\n","Does not provide in depth tensorflow logging but it does provide evaluation at the end. As mentioned above, we combine the provided training and dev files\n","\n"]},{"cell_type":"code","metadata":{"id":"Hvxh57Zh5zto","colab_type":"code","colab":{}},"source":["def bertCV(data, train_batch_size = 32, learn_rate = 2e-5,\\\n","           num_epochs =3.0, folds = 5):\n","\n","  #Filter out all log messages so console isn't consumed with memory\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","  \"\"\"FIXED MODEL PARAMS\"\"\"\n","  EVAL_BATCH_SIZE = 8\n","  PREDICT_BATCH_SIZE = 8\n","  MAX_SEQ_LENGTH = 256\n","\n","  # Warmup is a period of time where the learning rate \n","  #is small and gradually increases - usually helps training.\n","  WARMUP_PROPORTION = 0.1\n","\n","  #Dataframe where grid search results will be stored. Empty to begin with\n","  eval_df = pd.DataFrame(columns = ['F1 Score', 'auc', 'Accuracy'] )\n","  \n","  k = 1 # Fold counter\n","\n","  #Stratified K fold ensures the folds are made by preserving the percentage of samples for each class.\n","  cv = StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n","\n","  # Sticking within the training dataset for evaluation. Data is the combination of the provided train and dev sets\n","  for train_index, dev_index in cv.split(data.tweet, data.label): \n","    \n","    #Shuffling again because otherwise the StratifiedKFold function groups a lot of 0's at the start\n","    training  = data.iloc[train_index]\n","    training = training.sample(frac = 1, random_state=SEED)\n","    develop = data.iloc[dev_index]\n","    develop = develop.sample(frac = 1, random_state=SEED)\n","    \n","    \"\"\"Unlike before where I only one test set and one training set, this time I have K different sets of training and testing.\n","    Therefore, in each fold I need to get a new set of data and convert it to features each time.\"\"\"\n","    \n","    # Use the InputExample class from BERT's run_classifier code to create examples from the data\n","\n","    train_InputExamples = training.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                      text_a = x[DATA_COLUMN], \n","                                                                      text_b = None, \n","                                                                      label = x[LABEL_COLUMN]), axis = 1)\n","\n","    dev_InputExamples = develop.apply(lambda x: run_classifier.InputExample(guid=None, \n","                                                                      text_a = x[DATA_COLUMN], \n","                                                                      text_b = None, \n","                                                                      label = x[LABEL_COLUMN]), axis = 1)\n","    \n","    #Convert these examples to features that BERT can interpret\n","    train_features =  run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","    dev_features = run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","\n","    #Delete prior model graph, checkpoints and eval files to make room for new model each loop\n","    try:\n","      tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","    except:\n","    # Doesn't matter if the directory didn't exist\n","      pass\n","    tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","    # Compute # train and warmup steps from batch size\n","    num_train_steps = int(len(train_features) / train_batch_size * NUM_TRAIN_EPOCHS)\n","    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","    # Model configs\n","    model_fn = run_classifier.model_fn_builder(\n","    bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","    num_labels=len(label_list),\n","    init_checkpoint=bert_ckpt_file,\n","    learning_rate=LEARNING_RATE,\n","    num_train_steps=num_train_steps,\n","    num_warmup_steps=num_warmup_steps,\n","    use_tpu=True,\n","    use_one_hot_embeddings=True)\n","\n","    estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","      use_tpu=True,\n","      model_fn=model_fn,\n","      config=run_config,\n","      train_batch_size=TRAIN_BATCH_SIZE,\n","      eval_batch_size=EVAL_BATCH_SIZE,\n","      predict_batch_size=PREDICT_BATCH_SIZE)\n","    \n","    # Create an input function for training. drop_remainder = True for using TPUs.\n","    train_input_fn = run_classifier.input_fn_builder(\n","        features=train_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=True,\n","        drop_remainder=True)\n","\n","    #input function for dev data, we feed in our previously created dev_features for this\n","    dev_input_fn = run_classifier.input_fn_builder(\n","        features=dev_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=False,\n","        drop_remainder=True)\n","    \n","    current_time = datetime.now()\n","    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps )\n","    train_time = datetime.now() - current_time\n","    \n","\n","    #You need to provide number of steps for a TPU\n","    eval_steps = int(len(dev_InputExamples) / EVAL_BATCH_SIZE)\n","\n","    #Eval may be slightly WRONG on the TPU because it will truncate the last batch.\n","    eval_results = estimator.evaluate(input_fn=dev_input_fn, steps=eval_steps)\n","\n","    row = pd.Series({'F1 Score': eval_results['F1_Score'], 'auc': eval_results['auc'], 'Accuracy': eval_results['eval_accuracy'],'Precision': eval_results['precision'],'Recall': eval_results['recall'],\\\n","                                    'False Negatives': eval_results['false_negatives'],'False Positives': eval_results['false_positives'],\\\n","                    'True Negatives':eval_results['true_negatives'] ,'True Positives': eval_results['true_positives'], 'Training Time': train_time })\n","    #row = get_metrics(OUTPUT_DIR, train_time, k)\n","    row = pd.Series(row, name = 'Fold ' + str(k))\n","\n","    \"\"\"Below statement controls for whenever we get a bad fold which results in a model predicting only one class.\n","    This isn't truly representative of normal performance and can bring down CV score, so we omit model evaluation\n","    if the below statement is true\"\"\"\n","    if eval_results['false_negatives'] < 1 or eval_results['false_positives'] < 1: \n","      print(\"Classifier predicts one class. Thus not recording this metric as it will skew CV\\n\")\n","      #k = k + 1\n","      continue\n","\n","    eval_df = eval_df.append(row)\n","    print(\"Fold \" + str(k) + \":\\tF-Score:\", eval_df[\"F1 Score\"][k-1])\n","    print(\"Training took time \", train_time)\n","    print('---------------------------------------------------------------------------------------------------------\\n')\n","    k = k + 1 #Increment on fold counter\n","\n","  row = eval_df.mean(axis = 0)\n","  row = pd.Series(row, name = 'CV Average')\n","  eval_df = eval_df.append(row)\n","  print(\"\\nTraining Batch Size: \", train_batch_size, \"\\tLearn Rate: \", learn_rate, \"\\tNum Epochs: \", num_epochs)\n","  display(eval_df)\n","\n","  return row # Also return row of CV-Average"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l9Y4Ql38-YQe","colab_type":"text"},"source":["### Cross-Validation\n","Basic cross-validation can be performed here"]},{"cell_type":"code","metadata":{"id":"h1ctK3c4LXVf","colab_type":"code","outputId":"bdfe0f4c-9f17-414f-89df-7ba94d0988d7","executionInfo":{"status":"error","timestamp":1583231842508,"user_tz":0,"elapsed":1136,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":231}},"source":["data = loadData(rawTrain, rawDev, params)\n","\n","CV_Av = bertCV(data, learn_rate = 2e-5, num_epochs=4.0)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-738bb652a453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m data = loadData(train, dev, replaceEmoji = False, segmentHashtag = True,\n\u001b[0;32m----> 5\u001b[0;31m                 remove_stop = False, lemmatize= False, remove_punctuation = False)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mCV_Av\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbertCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: loadData() got an unexpected keyword argument 'replaceEmoji'"]}]},{"cell_type":"markdown","metadata":{"id":"TFVD9MdCM5nH","colab_type":"text"},"source":["### Cross-Validation of cross-validation\n","\n","Tensorflow 1.x is non-deterministic, which has resulted in the variability between each run to be greater than the difference in performance gained between introductions of different configurations and parameters. This makes it difficult to determine what is the best pre-trainig and text preprocessing pipeline to undertake.\n","\n","To better ensure the reliability of experiments my solution is to have a 5 fold cross-validation of a cross-validated sample of my data. My hope is that by doing this I can better identify what pre-processing pipeline works best in my program and also which further pre-trained checkpoint is best (i.e. after 40,000 steps or 120,000 steps)"]},{"cell_type":"code","metadata":{"id":"V_CA6NTiM4wQ","colab_type":"code","outputId":"3ef54d55-b884-4fdc-d7f4-f77426a8e809","executionInfo":{"status":"ok","timestamp":1583982904787,"user_tz":0,"elapsed":16437454,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["data = loadData(rawTrain, rawDev, params)\n","\n","#Stratified K fold ensures the folds are made by preserving the percentage of samples for each class.\n","folds = 5\n","cv = StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n","eval_df = pd.DataFrame(columns = ['F1 Score', 'auc', 'Accuracy'])\n","\n","#This will be a 5-fold CV so the sample each time will be a fifth of the data\n","i = 1\n","for __, data_index in cv.split(data.tweet, data.label):\n","  dat = data.iloc[data_index]\n","  CV_Av = bertCV(dat, learn_rate = 2e-5, num_epochs=4.0)\n","  CV_Av = pd.Series(CV_Av, name = 'CV Average' + str(i))\n","  eval_df = eval_df.append(CV_Av)\n","\n","row = eval_df.mean(axis = 0)\n","row = pd.Series(row, name = '40,000')\n","eval_df1 = pd.read_csv('gs://csc3002/hateval2019/pretraining_eval_df1.csv', sep=',',  index_col = 0, encoding = 'utf-8')\n","eval_df1 = eval_df1.append(row)\n","eval_df1.to_csv('gs://csc3002/hateval2019/pretraining_eval_df1.csv', sep=',',  index = True, encoding = 'utf-8')\n","eval_df1\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Fold 1:\tF-Score: 0.722741425037384\n","Training took time  0:08:53.766756\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 2:\tF-Score: 0.703601062297821\n","Training took time  0:05:09.879504\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 3:\tF-Score: 0.7641791105270386\n","Training took time  0:05:17.234892\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 4:\tF-Score: 0.7848836779594421\n","Training took time  0:05:19.039668\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 5:\tF-Score: 0.7942028045654297\n","Training took time  0:05:22.761278\n","---------------------------------------------------------------------------------------------------------\n","\n","\n","Training Batch Size:  32 \tLearn Rate:  2e-05 \tNum Epochs:  4.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1 Score</th>\n","      <th>auc</th>\n","      <th>Accuracy</th>\n","      <th>False Negatives</th>\n","      <th>False Positives</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Training Time</th>\n","      <th>True Negatives</th>\n","      <th>True Positives</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Fold 1</th>\n","      <td>0.722741</td>\n","      <td>0.765497</td>\n","      <td>0.7775</td>\n","      <td>52.0</td>\n","      <td>37.0</td>\n","      <td>0.758170</td>\n","      <td>0.690476</td>\n","      <td>00:08:53.766756</td>\n","      <td>195.0</td>\n","      <td>116.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 2</th>\n","      <td>0.703601</td>\n","      <td>0.735735</td>\n","      <td>0.7325</td>\n","      <td>41.0</td>\n","      <td>66.0</td>\n","      <td>0.658031</td>\n","      <td>0.755952</td>\n","      <td>00:05:09.879504</td>\n","      <td>166.0</td>\n","      <td>127.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 3</th>\n","      <td>0.764179</td>\n","      <td>0.796901</td>\n","      <td>0.8025</td>\n","      <td>40.0</td>\n","      <td>39.0</td>\n","      <td>0.766467</td>\n","      <td>0.761905</td>\n","      <td>00:05:17.234892</td>\n","      <td>193.0</td>\n","      <td>128.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 4</th>\n","      <td>0.784884</td>\n","      <td>0.812828</td>\n","      <td>0.8150</td>\n","      <td>34.0</td>\n","      <td>40.0</td>\n","      <td>0.771429</td>\n","      <td>0.798817</td>\n","      <td>00:05:19.039668</td>\n","      <td>191.0</td>\n","      <td>135.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 5</th>\n","      <td>0.794203</td>\n","      <td>0.820910</td>\n","      <td>0.8225</td>\n","      <td>32.0</td>\n","      <td>39.0</td>\n","      <td>0.778409</td>\n","      <td>0.810651</td>\n","      <td>00:05:22.761278</td>\n","      <td>192.0</td>\n","      <td>137.0</td>\n","    </tr>\n","    <tr>\n","      <th>CV Average</th>\n","      <td>0.753922</td>\n","      <td>0.786374</td>\n","      <td>0.7900</td>\n","      <td>39.8</td>\n","      <td>44.2</td>\n","      <td>0.746501</td>\n","      <td>0.763560</td>\n","      <td>00:06:00.536419</td>\n","      <td>187.4</td>\n","      <td>128.6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            F1 Score       auc  ...  True Negatives  True Positives\n","Fold 1      0.722741  0.765497  ...  195.0           116.0         \n","Fold 2      0.703601  0.735735  ...  166.0           127.0         \n","Fold 3      0.764179  0.796901  ...  193.0           128.0         \n","Fold 4      0.784884  0.812828  ...  191.0           135.0         \n","Fold 5      0.794203  0.820910  ...  192.0           137.0         \n","CV Average  0.753922  0.786374  ...  187.4           128.6         \n","\n","[6 rows x 10 columns]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Fold 1:\tF-Score: 0.7830985188484192\n","Training took time  0:05:22.738472\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 2:\tF-Score: 0.7797618508338928\n","Training took time  0:05:27.449404\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 3:\tF-Score: 0.7048192024230957\n","Training took time  0:05:20.619050\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 4:\tF-Score: 0.7485713362693787\n","Training took time  0:05:35.600859\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 5:\tF-Score: 0.7323076128959656\n","Training took time  0:05:31.675044\n","---------------------------------------------------------------------------------------------------------\n","\n","\n","Training Batch Size:  32 \tLearn Rate:  2e-05 \tNum Epochs:  4.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1 Score</th>\n","      <th>auc</th>\n","      <th>Accuracy</th>\n","      <th>False Negatives</th>\n","      <th>False Positives</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Training Time</th>\n","      <th>True Negatives</th>\n","      <th>True Positives</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Fold 1</th>\n","      <td>0.783099</td>\n","      <td>0.810242</td>\n","      <td>0.8075</td>\n","      <td>29.0</td>\n","      <td>48.0</td>\n","      <td>0.743316</td>\n","      <td>0.827381</td>\n","      <td>00:05:22.738472</td>\n","      <td>184.0</td>\n","      <td>139.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 2</th>\n","      <td>0.779762</td>\n","      <td>0.810140</td>\n","      <td>0.8150</td>\n","      <td>37.0</td>\n","      <td>37.0</td>\n","      <td>0.779762</td>\n","      <td>0.779762</td>\n","      <td>00:05:27.449404</td>\n","      <td>195.0</td>\n","      <td>131.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 3</th>\n","      <td>0.704819</td>\n","      <td>0.746921</td>\n","      <td>0.7550</td>\n","      <td>51.0</td>\n","      <td>47.0</td>\n","      <td>0.713415</td>\n","      <td>0.696429</td>\n","      <td>00:05:20.619050</td>\n","      <td>185.0</td>\n","      <td>117.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 4</th>\n","      <td>0.748571</td>\n","      <td>0.779349</td>\n","      <td>0.7800</td>\n","      <td>38.0</td>\n","      <td>50.0</td>\n","      <td>0.723757</td>\n","      <td>0.775148</td>\n","      <td>00:05:35.600859</td>\n","      <td>181.0</td>\n","      <td>131.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 5</th>\n","      <td>0.732308</td>\n","      <td>0.771984</td>\n","      <td>0.7825</td>\n","      <td>50.0</td>\n","      <td>37.0</td>\n","      <td>0.762821</td>\n","      <td>0.704142</td>\n","      <td>00:05:31.675044</td>\n","      <td>194.0</td>\n","      <td>119.0</td>\n","    </tr>\n","    <tr>\n","      <th>CV Average</th>\n","      <td>0.749712</td>\n","      <td>0.783727</td>\n","      <td>0.7880</td>\n","      <td>41.0</td>\n","      <td>43.8</td>\n","      <td>0.744614</td>\n","      <td>0.756572</td>\n","      <td>00:05:27.616565</td>\n","      <td>187.8</td>\n","      <td>127.4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            F1 Score       auc  ...  True Negatives  True Positives\n","Fold 1      0.783099  0.810242  ...  184.0           139.0         \n","Fold 2      0.779762  0.810140  ...  195.0           131.0         \n","Fold 3      0.704819  0.746921  ...  185.0           117.0         \n","Fold 4      0.748571  0.779349  ...  181.0           131.0         \n","Fold 5      0.732308  0.771984  ...  194.0           119.0         \n","CV Average  0.749712  0.783727  ...  187.8           127.4         \n","\n","[6 rows x 10 columns]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Fold 1:\tF-Score: 0.7607361078262329\n","Training took time  0:05:26.349457\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 2:\tF-Score: 0.786786675453186\n","Training took time  0:05:23.930032\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 3:\tF-Score: 0.768817126750946\n","Training took time  0:05:37.883686\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 4:\tF-Score: 0.7603304982185364\n","Training took time  0:05:31.956526\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 5:\tF-Score: 0.6624203324317932\n","Training took time  0:05:42.495866\n","---------------------------------------------------------------------------------------------------------\n","\n","\n","Training Batch Size:  32 \tLearn Rate:  2e-05 \tNum Epochs:  4.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1 Score</th>\n","      <th>auc</th>\n","      <th>Accuracy</th>\n","      <th>False Negatives</th>\n","      <th>False Positives</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Training Time</th>\n","      <th>True Negatives</th>\n","      <th>True Positives</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Fold 1</th>\n","      <td>0.760736</td>\n","      <td>0.795772</td>\n","      <td>0.8050</td>\n","      <td>44.0</td>\n","      <td>34.0</td>\n","      <td>0.784810</td>\n","      <td>0.738095</td>\n","      <td>00:05:26.349457</td>\n","      <td>198.0</td>\n","      <td>124.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 2</th>\n","      <td>0.786787</td>\n","      <td>0.816605</td>\n","      <td>0.8225</td>\n","      <td>37.0</td>\n","      <td>34.0</td>\n","      <td>0.793939</td>\n","      <td>0.779762</td>\n","      <td>00:05:23.930032</td>\n","      <td>198.0</td>\n","      <td>131.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 3</th>\n","      <td>0.768817</td>\n","      <td>0.794130</td>\n","      <td>0.7850</td>\n","      <td>25.0</td>\n","      <td>61.0</td>\n","      <td>0.700980</td>\n","      <td>0.851190</td>\n","      <td>00:05:37.883686</td>\n","      <td>171.0</td>\n","      <td>143.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 4</th>\n","      <td>0.760330</td>\n","      <td>0.787072</td>\n","      <td>0.7825</td>\n","      <td>31.0</td>\n","      <td>56.0</td>\n","      <td>0.711340</td>\n","      <td>0.816568</td>\n","      <td>00:05:31.956526</td>\n","      <td>175.0</td>\n","      <td>138.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 5</th>\n","      <td>0.662420</td>\n","      <td>0.718948</td>\n","      <td>0.7350</td>\n","      <td>65.0</td>\n","      <td>41.0</td>\n","      <td>0.717241</td>\n","      <td>0.615385</td>\n","      <td>00:05:42.495866</td>\n","      <td>190.0</td>\n","      <td>104.0</td>\n","    </tr>\n","    <tr>\n","      <th>CV Average</th>\n","      <td>0.747818</td>\n","      <td>0.782505</td>\n","      <td>0.7860</td>\n","      <td>40.4</td>\n","      <td>45.2</td>\n","      <td>0.741662</td>\n","      <td>0.760200</td>\n","      <td>00:05:32.523113</td>\n","      <td>186.4</td>\n","      <td>128.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            F1 Score       auc  ...  True Negatives  True Positives\n","Fold 1      0.760736  0.795772  ...  198.0           124.0         \n","Fold 2      0.786787  0.816605  ...  198.0           131.0         \n","Fold 3      0.768817  0.794130  ...  171.0           143.0         \n","Fold 4      0.760330  0.787072  ...  175.0           138.0         \n","Fold 5      0.662420  0.718948  ...  190.0           104.0         \n","CV Average  0.747818  0.782505  ...  186.4           128.0         \n","\n","[6 rows x 10 columns]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Classifier predicts one class. Thus not recording this metric as it will skew CV\n","\n","Fold 1:\tF-Score: 0.766570508480072\n","Training took time  0:05:31.500574\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 2:\tF-Score: 0.8192089200019836\n","Training took time  0:05:40.513690\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 3:\tF-Score: 0.7683284878730774\n","Training took time  0:05:43.482192\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 4:\tF-Score: 0.7546011805534363\n","Training took time  0:05:42.643961\n","---------------------------------------------------------------------------------------------------------\n","\n","\n","Training Batch Size:  32 \tLearn Rate:  2e-05 \tNum Epochs:  4.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1 Score</th>\n","      <th>auc</th>\n","      <th>Accuracy</th>\n","      <th>False Negatives</th>\n","      <th>False Positives</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Training Time</th>\n","      <th>True Negatives</th>\n","      <th>True Positives</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Fold 1</th>\n","      <td>0.766571</td>\n","      <td>0.796089</td>\n","      <td>0.7975</td>\n","      <td>36.00</td>\n","      <td>45.00</td>\n","      <td>0.747191</td>\n","      <td>0.786982</td>\n","      <td>00:05:31.500574</td>\n","      <td>186.0</td>\n","      <td>133.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 2</th>\n","      <td>0.819209</td>\n","      <td>0.843186</td>\n","      <td>0.8400</td>\n","      <td>23.00</td>\n","      <td>41.00</td>\n","      <td>0.779570</td>\n","      <td>0.863095</td>\n","      <td>00:05:40.513690</td>\n","      <td>191.0</td>\n","      <td>145.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 3</th>\n","      <td>0.768328</td>\n","      <td>0.799364</td>\n","      <td>0.8025</td>\n","      <td>37.00</td>\n","      <td>42.00</td>\n","      <td>0.757225</td>\n","      <td>0.779762</td>\n","      <td>00:05:43.482192</td>\n","      <td>190.0</td>\n","      <td>131.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 4</th>\n","      <td>0.754601</td>\n","      <td>0.790640</td>\n","      <td>0.8000</td>\n","      <td>45.00</td>\n","      <td>35.00</td>\n","      <td>0.778481</td>\n","      <td>0.732143</td>\n","      <td>00:05:42.643961</td>\n","      <td>197.0</td>\n","      <td>123.0</td>\n","    </tr>\n","    <tr>\n","      <th>CV Average</th>\n","      <td>0.777177</td>\n","      <td>0.807320</td>\n","      <td>0.8100</td>\n","      <td>35.25</td>\n","      <td>40.75</td>\n","      <td>0.765617</td>\n","      <td>0.790496</td>\n","      <td>00:05:39.535104</td>\n","      <td>191.0</td>\n","      <td>133.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            F1 Score       auc  ...  True Negatives  True Positives\n","Fold 1      0.766571  0.796089  ...  186.0           133.0         \n","Fold 2      0.819209  0.843186  ...  191.0           145.0         \n","Fold 3      0.768328  0.799364  ...  190.0           131.0         \n","Fold 4      0.754601  0.790640  ...  197.0           123.0         \n","CV Average  0.777177  0.807320  ...  191.0           133.0         \n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Fold 1:\tF-Score: 0.792022705078125\n","Training took time  0:05:41.983810\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 2:\tF-Score: 0.765714168548584\n","Training took time  0:05:44.589089\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 3:\tF-Score: 0.7272726893424988\n","Training took time  0:05:37.342505\n","---------------------------------------------------------------------------------------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yqaffnMfNQXe","colab_type":"text"},"source":["<b> Collecting and saving results </b>"]},{"cell_type":"code","metadata":{"id":"8Te2e53KNcrW","colab_type":"code","outputId":"9ec6aa6a-7a4b-4b7d-eada-18f4531a2b33","executionInfo":{"status":"ok","timestamp":1581698323695,"user_tz":0,"elapsed":493,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":217}},"source":["emoji = CV_Av\n","print(emoji)\n","emoji = pd.Series(emoji, name = 'Emoji Replacement')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["F1 Score           0.786213              \n","auc                0.814614              \n","Accuracy           0.81757               \n","False Negatives    171.6                 \n","False Positives    191.8                 \n","Precision          0.777174              \n","Recall             0.795718              \n","Training Time      0 days 00:08:17.360357\n","True Negatives     960.2                 \n","True Positives     668.4                 \n","Name: CV Average, dtype: object\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ksGfCY22fuIL","colab_type":"code","outputId":"70d7792a-8eaa-488f-d4e7-17c4340d538d","executionInfo":{"status":"ok","timestamp":1581698380927,"user_tz":0,"elapsed":547,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":416}},"source":["eval_df = pd.read_csv('gs://csc3002/hateval2019/preprocess_eval_df.csv', sep=',',  index_col = 0, encoding = 'utf-8')\n","eval_df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1 Score</th>\n","      <th>precision</th>\n","      <th>false_positives</th>\n","      <th>Training Time</th>\n","      <th>auc</th>\n","      <th>eval_accuracy</th>\n","      <th>false_negatives</th>\n","      <th>recall</th>\n","      <th>true_negatives</th>\n","      <th>true_positives</th>\n","      <th>Accuracy</th>\n","      <th>False Negatives</th>\n","      <th>False Positives</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>True Negatives</th>\n","      <th>True Positives</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Basic</th>\n","      <td>0.796373</td>\n","      <td>0.785083</td>\n","      <td>185.8</td>\n","      <td>0 days 00:09:16.609999</td>\n","      <td>0.822897</td>\n","      <td>0.825302</td>\n","      <td>160.8</td>\n","      <td>0.808148</td>\n","      <td>959.8</td>\n","      <td>677.6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Contractions</th>\n","      <td>0.796084</td>\n","      <td>0.788785</td>\n","      <td>180.4</td>\n","      <td>0 days 00:09:13.619132400</td>\n","      <td>0.823061</td>\n","      <td>0.826109</td>\n","      <td>164.6</td>\n","      <td>0.803677</td>\n","      <td>965.2</td>\n","      <td>673.8</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Hashtag Segmentation</th>\n","      <td>0.792463</td>\n","      <td>0.778531</td>\n","      <td>192.4</td>\n","      <td>0 days 00:09:21.422566</td>\n","      <td>0.819521</td>\n","      <td>0.821573</td>\n","      <td>161.6</td>\n","      <td>0.807077</td>\n","      <td>953.2</td>\n","      <td>676.8</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Emoji Replacement</th>\n","      <td>0.793692</td>\n","      <td>0.793950</td>\n","      <td>172.8</td>\n","      <td>0 days 00:09:15.707040</td>\n","      <td>0.821786</td>\n","      <td>0.826205</td>\n","      <td>173.4</td>\n","      <td>0.793561</td>\n","      <td>978.8</td>\n","      <td>667.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>All Preprocessing</th>\n","      <td>0.796205</td>\n","      <td>0.785145</td>\n","      <td>185.6</td>\n","      <td>0 days 00:09:23.425535</td>\n","      <td>0.823387</td>\n","      <td>0.825803</td>\n","      <td>161.4</td>\n","      <td>0.807766</td>\n","      <td>966.0</td>\n","      <td>679.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Removing Stopwords</th>\n","      <td>0.729251</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0 days 00:09:22.530510</td>\n","      <td>0.737039</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.757831</td>\n","      <td>317.6</td>\n","      <td>164.8</td>\n","      <td>0.608553</td>\n","      <td>0.617838</td>\n","      <td>987.2</td>\n","      <td>522.4</td>\n","    </tr>\n","    <tr>\n","      <th>Basic</th>\n","      <td>0.791682</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0 days 00:08:05.076683</td>\n","      <td>0.818791</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.820565</td>\n","      <td>161.4</td>\n","      <td>194.6</td>\n","      <td>0.776980</td>\n","      <td>0.807362</td>\n","      <td>951.6</td>\n","      <td>676.4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      F1 Score  precision  ...  True Negatives True Positives\n","Basic                 0.796373  0.785083   ... NaN             NaN           \n","Contractions          0.796084  0.788785   ... NaN             NaN           \n","Hashtag Segmentation  0.792463  0.778531   ... NaN             NaN           \n","Emoji Replacement     0.793692  0.793950   ... NaN             NaN           \n","All Preprocessing     0.796205  0.785145   ... NaN             NaN           \n","Removing Stopwords    0.729251 NaN         ...  987.2           522.4        \n","Basic                 0.791682 NaN         ...  951.6           676.4        \n","\n","[7 rows x 17 columns]"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"1T8q6AvG0H91","colab_type":"code","outputId":"52178894-6f7b-4533-a702-9047b77cafa9","executionInfo":{"status":"ok","timestamp":1581698383115,"user_tz":0,"elapsed":477,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":464}},"source":["#eval_df = pd.DataFrame(columns = ['F1 Score', 'auc', 'Accuracy'] ) # Instantise\n","eval_df = eval_df.append(emoji, ignore_index=False)\n","eval_df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1 Score</th>\n","      <th>precision</th>\n","      <th>false_positives</th>\n","      <th>Training Time</th>\n","      <th>auc</th>\n","      <th>eval_accuracy</th>\n","      <th>false_negatives</th>\n","      <th>recall</th>\n","      <th>true_negatives</th>\n","      <th>true_positives</th>\n","      <th>Accuracy</th>\n","      <th>False Negatives</th>\n","      <th>False Positives</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>True Negatives</th>\n","      <th>True Positives</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Basic</th>\n","      <td>0.796373</td>\n","      <td>0.785083</td>\n","      <td>185.8</td>\n","      <td>0 days 00:09:16.609999</td>\n","      <td>0.822897</td>\n","      <td>0.825302</td>\n","      <td>160.8</td>\n","      <td>0.808148</td>\n","      <td>959.8</td>\n","      <td>677.6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Contractions</th>\n","      <td>0.796084</td>\n","      <td>0.788785</td>\n","      <td>180.4</td>\n","      <td>0 days 00:09:13.619132400</td>\n","      <td>0.823061</td>\n","      <td>0.826109</td>\n","      <td>164.6</td>\n","      <td>0.803677</td>\n","      <td>965.2</td>\n","      <td>673.8</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Hashtag Segmentation</th>\n","      <td>0.792463</td>\n","      <td>0.778531</td>\n","      <td>192.4</td>\n","      <td>0 days 00:09:21.422566</td>\n","      <td>0.819521</td>\n","      <td>0.821573</td>\n","      <td>161.6</td>\n","      <td>0.807077</td>\n","      <td>953.2</td>\n","      <td>676.8</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Emoji Replacement</th>\n","      <td>0.793692</td>\n","      <td>0.793950</td>\n","      <td>172.8</td>\n","      <td>0 days 00:09:15.707040</td>\n","      <td>0.821786</td>\n","      <td>0.826205</td>\n","      <td>173.4</td>\n","      <td>0.793561</td>\n","      <td>978.8</td>\n","      <td>667.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>All Preprocessing</th>\n","      <td>0.796205</td>\n","      <td>0.785145</td>\n","      <td>185.6</td>\n","      <td>0 days 00:09:23.425535</td>\n","      <td>0.823387</td>\n","      <td>0.825803</td>\n","      <td>161.4</td>\n","      <td>0.807766</td>\n","      <td>966.0</td>\n","      <td>679.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Removing Stopwords</th>\n","      <td>0.729251</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0 days 00:09:22.530510</td>\n","      <td>0.737039</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.757831</td>\n","      <td>317.6</td>\n","      <td>164.8</td>\n","      <td>0.608553</td>\n","      <td>0.617838</td>\n","      <td>987.2</td>\n","      <td>522.4</td>\n","    </tr>\n","    <tr>\n","      <th>Basic</th>\n","      <td>0.791682</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0 days 00:08:05.076683</td>\n","      <td>0.818791</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.820565</td>\n","      <td>161.4</td>\n","      <td>194.6</td>\n","      <td>0.776980</td>\n","      <td>0.807362</td>\n","      <td>951.6</td>\n","      <td>676.4</td>\n","    </tr>\n","    <tr>\n","      <th>Emoji Replacement</th>\n","      <td>0.786213</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0 days 00:08:17.360357</td>\n","      <td>0.814614</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.817570</td>\n","      <td>171.6</td>\n","      <td>191.8</td>\n","      <td>0.777174</td>\n","      <td>0.795718</td>\n","      <td>960.2</td>\n","      <td>668.4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      F1 Score  precision  ...  True Negatives True Positives\n","Basic                 0.796373  0.785083   ... NaN             NaN           \n","Contractions          0.796084  0.788785   ... NaN             NaN           \n","Hashtag Segmentation  0.792463  0.778531   ... NaN             NaN           \n","Emoji Replacement     0.793692  0.793950   ... NaN             NaN           \n","All Preprocessing     0.796205  0.785145   ... NaN             NaN           \n","Removing Stopwords    0.729251 NaN         ...  987.2           522.4        \n","Basic                 0.791682 NaN         ...  951.6           676.4        \n","Emoji Replacement     0.786213 NaN         ...  960.2           668.4        \n","\n","[8 rows x 17 columns]"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"RQA09AG2foBH","colab_type":"code","colab":{}},"source":["eval_df.to_csv('gs://csc3002/hateval2019/preprocess_eval_df.csv', sep=',',  index = True, encoding = 'utf-8')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y4KnOByLTj9i","colab_type":"text"},"source":["## Adding in augmented back-translated hate speech tweets as extra data\n","\n","We have very few instances of hate speech labelled in this dataset. To remedy this I performed back_translation augmentation on this training set.\n","\n","Below I load in in the extra hate speech tweets I created via back-translation augmentation I performed in another colab notebook and I append it to the existing dataframe"]},{"cell_type":"code","metadata":{"id":"DHT16O8CTfpN","colab_type":"code","outputId":"8cd09f29-baf1-4fd0-fba9-e4ac17523842","executionInfo":{"status":"ok","timestamp":1580488889789,"user_tz":0,"elapsed":3685511,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"dat = '/content/drive/My Drive/hateval2019/backtranslated_hatEval.txt' \n","dat = pd.read_csv(dat, sep = '\\t', names = ['tweet'], header = None, encoding = 'utf-8')\n","pd.set_option('display.max_colwidth', -1)\n","dat = dat.astype(str)\n","dat.head(50)\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"dat = '/content/drive/My Drive/hateval2019/backtranslated_hatEval.txt' \\ndat = pd.read_csv(dat, sep = '\\t', names = ['tweet'], header = None, encoding = 'utf-8')\\npd.set_option('display.max_colwidth', -1)\\ndat = dat.astype(str)\\ndat.head(50)\""]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"X9jImOlRT2EO","colab_type":"text"},"source":["**See how the english is a little off?** \n","\n","That's because these are the hate speech tweets in the training set translated to french, then translated back again. This creates a whole new, yet similar set of hate speech tweets to train on. (Slightly augmented text)"]},{"cell_type":"code","metadata":{"id":"pe1Ndra6UAbb","colab_type":"code","outputId":"219efd82-0da1-46f3-fd7d-30f6077675a2","executionInfo":{"status":"ok","timestamp":1580488889789,"user_tz":0,"elapsed":3685501,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"print(\"There are\", len(dat.index), \"tweets\")\n","dat = dat[dat['tweet'].apply(lambda x: len(x) > 10)]\n","print(\"There are now\", len(dat.index), \"tweets\")\n","dat.head()\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'print(\"There are\", len(dat.index), \"tweets\")\\ndat = dat[dat[\\'tweet\\'].apply(lambda x: len(x) > 10)]\\nprint(\"There are now\", len(dat.index), \"tweets\")\\ndat.head()'"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"vNeswnQeUBY2","colab_type":"text"},"source":["<b>Rather than creating 3768 extra tweets, 19630 extra have been created. The tweets have been incorrectly parsed. Removing some tweets with a smaller length may mitigate this effect somewhat by removing tweets that were cut in half</b>\n","\n","Let's see if it helps by adding it to the original training set and testing it against our dev data"]},{"cell_type":"code","metadata":{"id":"apxp79BtUyf5","colab_type":"code","outputId":"cee28152-5561-4062-e3e2-12c5fb663e9f","executionInfo":{"status":"ok","timestamp":1580488889790,"user_tz":0,"elapsed":3685491,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"dat['label'] = 1\n","dat['id'] = 80000\n","frames = [dat,data]\n","data = pd.concat(frames)\n","print(data.info())\n","data.head()\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"dat['label'] = 1\\ndat['id'] = 80000\\nframes = [dat,data]\\ndata = pd.concat(frames)\\nprint(data.info())\\ndata.head()\""]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"lHT94i8VU4r7","colab_type":"text"},"source":["We'll shuffle the dataframe to make sure there's no funny business with the training of the model and we'll then reset the id field to make it unique and sequential for each row"]},{"cell_type":"code","metadata":{"id":"w8rPNri1VAiE","colab_type":"code","outputId":"1cbdbd13-3813-401a-d883-c5995c7c1158","executionInfo":{"status":"ok","timestamp":1580488889791,"user_tz":0,"elapsed":3685476,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"data = data.sample(frac=1, random_state = SEED)\n","data.reset_index(drop = True, inplace = True)\n","\n","data['id'] = data.reset_index().index + 1\n","print(data.label.value_counts(), \"\\n\")\n","print(data.info())\n","length = len(data.index)\n","print(\"\\nNow there are\", length , \"tweets total in this database\")\n","data.tail(10)\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'data = data.sample(frac=1)\\ndata.reset_index(drop = True, inplace = True)\\n\\ndata[\\'id\\'] = data.reset_index().index + 1\\nprint(data.label.value_counts(), \"\\n\")\\nprint(data.info())\\nlength = len(data.index)\\nprint(\"\\nNow there are\", length , \"tweets total in this database\")\\ndata.tail(10)'"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"SMgEcg_v4l7m","colab_type":"text"},"source":["# Training with both dev and training set. Then Testing with the holdout test set\n","<b>Loading in train and test data..."]},{"cell_type":"code","metadata":{"id":"c3KKifBM4kx5","colab_type":"code","outputId":"2666e6b3-f0a7-4e3b-e7b0-ae46dbe48b86","executionInfo":{"status":"ok","timestamp":1583766563003,"user_tz":0,"elapsed":39909,"user":{"displayName":"Fiona Fitwi","photoUrl":"","userId":"16113747830604572927"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["train = loadData(rawTrain, rawDev, params)\n","\n","\n","test = loadData(rawTest, params_dict = params)\n","test.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>catch us on and tonight at 5pm nervous recovery</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>fathers day dubai uae my dubai father</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>can't wait to see tonight</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>i am awesome. i am positive affirmation</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>care free. stress free. happy. {#carefree stress free bernese mountain dog berner</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                                                              tweet\n","0  0   catch us on and tonight at 5pm nervous recovery                                  \n","1  1   fathers day dubai uae my dubai father                                            \n","2  2   can't wait to see tonight                                                        \n","3  3   i am awesome. i am positive affirmation                                          \n","4  4   care free. stress free. happy. {#carefree stress free bernese mountain dog berner"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"ZcNSY0kA8N4N","colab_type":"text"},"source":["<b>Function to get predictions on test data </b>"]},{"cell_type":"code","metadata":{"id":"mvpPwfj08PmW","colab_type":"code","colab":{}},"source":["def getPrediction(in_sentences):\n","  #Makes output less verbose\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","  labels = [0, 1]\n","  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n","  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n","  predictions = list(estimator.predict(predict_input_fn))\n","\n","  #Initialise empty predicted labels array\n","  predicted_classes = [None] * len(predictions)\n","\n","  #Use a for loop to iterate through probabilities and for each prediction assign a label\n","  #corresponding to which label has the highest probability\n","  for i in range(0, len(predictions)):\n","    if predictions[i]['probabilities'][0] > predictions[i]['probabilities'][1]:\n","      predicted_classes[i] = 0\n","    else:\n","      predicted_classes[i] = 1\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO) # Reset tensorflow verboisty to normal\n","\n","  return predicted_classes"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rt4d5H3A6Iqu","colab_type":"text"},"source":["<b> Converting to features, setting run and model configs.\n","\n","Then training on train and dev set and predicting on unseen test set </b>"]},{"cell_type":"code","metadata":{"id":"Xf0Ofg3M6HBC","colab_type":"code","outputId":"8881b503-cfc5-46dd-aeec-21803414c90a","executionInfo":{"status":"ok","timestamp":1583775314695,"user_tz":0,"elapsed":1959944,"user":{"displayName":"Fiona Fitwi","photoUrl":"","userId":"16113747830604572927"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["LEARNING_RATE = 2e-5\n","num_train_steps = 6000 # Recommend 1000 for HatEval, 10000? for AnalyticsVidhya\n","train_batch_size = 32\n","if DATASET == \"HatEval\":\n","\n","  SAVE_CHECKPOINTS_STEPS = 1000\n","  run_config = tf.compat.v1.estimator.tpu.RunConfig(\n","      #I think the output file must be a sub-directory of the main BERT file\n","      model_dir=OUTPUT_DIR,\n","      tf_random_seed=SEED, \n","      cluster=cluster_resolver,\n","      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","      tpu_config=tf.contrib.tpu.TPUConfig(\n","          iterations_per_loop=200,\n","          num_shards=8,\n","          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","\n","  train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, \n","                                                                        text_a = x[DATA_COLUMN], \n","                                                                        text_b = None, \n","                                                                        label = x[LABEL_COLUMN]), axis = 1)\n","\n","  train_features =  run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","\n","\n","  #Delete prior model graph, checkpoints and eval files to make room for new model each loop\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","  # Doesn't matter if the directory didn't exist\n","    pass\n","  tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","  # Compute # warmup steps\n","  num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","  # Model configs\n","  model_fn = run_classifier.model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  use_one_hot_embeddings=True)\n","\n","  estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","    use_tpu=True,\n","    model_fn=model_fn,\n","    config=run_config,\n","    train_batch_size=train_batch_size,\n","    eval_batch_size=8,\n","    predict_batch_size=8)\n","\n","  # Create an input function for training. drop_remainder = True for using TPUs.\n","  train_input_fn = run_classifier.input_fn_builder(\n","      features=train_features,\n","      seq_length=MAX_SEQ_LENGTH,\n","      is_training=True,\n","      drop_remainder=True)\n","\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n","\n","  print(\"\\nThe model will stop training when it reaches\", num_train_steps, \"as a checkpoint\")\n","\n","  print(f'Beginning Training!')\n","  current_time = datetime.now()\n","  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","  train_time = datetime.now() - current_time\n","  print(\"Training took time \", train_time)\n","\n","  predictions = getPrediction(test.tweet)\n","  test['predictions'] = predictions\n","\n","  test.to_csv('gs://csc3002/hateval2019/predictions.csv', sep=',',  index = True, encoding = 'utf-8')\n","  print(\"\\n\\nF1 Score:\", metrics.f1_score(test.label, test.predictions))\n","  print(\"Accuracy\", metrics.accuracy_score(test.label, test.predictions))\n","\n","elif DATASET == \"AnalyticsVidhya\":\n","  \n","  SAVE_CHECKPOINTS_STEPS = 10000\n","  run_config = tf.compat.v1.estimator.tpu.RunConfig(\n","      #I think the output file must be a sub-directory of the main BERT file\n","      model_dir=OUTPUT_DIR,\n","      tf_random_seed=SEED, \n","      cluster=cluster_resolver,\n","      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","      tpu_config=tf.contrib.tpu.TPUConfig(\n","          iterations_per_loop=200,\n","          num_shards=8,\n","          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","\n","  train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, \n","                                                                        text_a = x[DATA_COLUMN], \n","                                                                        text_b = None, \n","                                                                        label = x[LABEL_COLUMN]), axis = 1)\n","\n","  train_features =  run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","\n","\n","  #Delete prior model graph, checkpoints and eval files to make room for new model each loop\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","  # Doesn't matter if the directory didn't exist\n","    pass\n","  tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","  # Compute # warmup steps\n","  num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","  # Model configs\n","  model_fn = run_classifier.model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  use_one_hot_embeddings=True)\n","\n","  estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","    use_tpu=True,\n","    model_fn=model_fn,\n","    config=run_config,\n","    train_batch_size=train_batch_size,\n","    eval_batch_size=8,\n","    predict_batch_size=8)\n","\n","  # Create an input function for training. drop_remainder = True for using TPUs.\n","  train_input_fn = run_classifier.input_fn_builder(\n","      features=train_features,\n","      seq_length=MAX_SEQ_LENGTH,\n","      is_training=True,\n","      drop_remainder=True)\n","\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n","\n","  print(\"\\nThe model will stop training when it reaches\", num_train_steps, \"as a checkpoint\")\n","\n","  print(f'Beginning Training!')\n","  current_time = datetime.now()\n","  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","  train_time = datetime.now() - current_time\n","  print(\"Training took time \", train_time)\n","\n","  predictions = getPrediction(test.tweet)\n","  test['label'] = predictions\n","  print(test.label.value_counts())\n","  print(predictions[0:20])\n","  test.to_csv('gs://csc3002/trial/submission.csv', sep=',', index = False)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 31814\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] new jersey beautiful sunny calm peaceful ##l flowers grass life in nyc [SEP]\n","INFO:tensorflow:input_ids: 101 2047 3933 3376 11559 5475 9379 2140 4870 5568 2166 1999 16392 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] en route to the film ##house to see 2001 ! don ##t get out much [SEP]\n","INFO:tensorflow:input_ids: 101 4372 2799 2000 1996 2143 4580 2000 2156 2541 999 2123 2102 2131 2041 2172 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] experimenting do you think you ' d enjoy coloring this ? calm nature intention healthy [SEP]\n","INFO:tensorflow:input_ids: 101 23781 2079 2017 2228 2017 1005 1040 5959 22276 2023 1029 5475 3267 6808 7965 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] happy hours and sing k ing with ma colleagues ! friday hours enjoy sing kara ##oke music [SEP]\n","INFO:tensorflow:input_ids: 101 3407 2847 1998 6170 1047 13749 2007 5003 8628 999 5958 2847 5959 6170 13173 11045 2189 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] morning ! t ##gi ##f good morning inspire [SEP]\n","INFO:tensorflow:input_ids: 101 2851 999 1056 5856 2546 2204 2851 18708 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:Writing example 10000 of 31814\n","INFO:tensorflow:Writing example 20000 of 31814\n","INFO:tensorflow:Writing example 30000 of 31814\n","WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f8a8233e1e0>) includes params argument, but params are not passed to Estimator.\n","INFO:tensorflow:Using config: {'_model_dir': 'gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output', '_tf_random_seed': 3060, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.103.143.2:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8a836a0c88>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.103.143.2:8470', '_evaluation_master': 'grpc://10.103.143.2:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=200, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f8a91f59278>}\n","INFO:tensorflow:_TPUContext: eval_on_tpu True\n","\n","The model will stop training when it reaches 6000 as a checkpoint\n","Beginning Training!\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.103.143.2:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6950939721587658405)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 7214882111008902385)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11723597538653886847)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10168099005517632449)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11977576496132178308)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 261289063090977959)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6461506172735637731)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 18118260143106526205)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 14835276859969354722)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 8603088287462879159)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10377542404254460937)\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output/model.ckpt.\n","INFO:tensorflow:Initialized dataset iterators in 3 seconds\n","INFO:tensorflow:Installing graceful shutdown hook.\n","INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 8 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:loss = 0.008544918, step = 200\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (1, 4)\n","INFO:tensorflow:loss = 0.0015602914, step = 400 (52.431 sec)\n","INFO:tensorflow:global_step/sec: 3.81458\n","INFO:tensorflow:examples/sec: 122.066\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (2, 91)\n","INFO:tensorflow:loss = 0.0008297247, step = 600 (43.334 sec)\n","INFO:tensorflow:global_step/sec: 4.61525\n","INFO:tensorflow:examples/sec: 147.688\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (3, 177)\n","INFO:tensorflow:loss = 0.00024089354, step = 800 (49.076 sec)\n","INFO:tensorflow:global_step/sec: 4.07527\n","INFO:tensorflow:examples/sec: 130.409\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 0.00016462688, step = 1000 (43.543 sec)\n","INFO:tensorflow:global_step/sec: 4.59317\n","INFO:tensorflow:examples/sec: 146.982\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (5, 9)\n","INFO:tensorflow:loss = 0.00047835277, step = 1200 (43.494 sec)\n","INFO:tensorflow:global_step/sec: 4.59838\n","INFO:tensorflow:examples/sec: 147.148\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (6, 94)\n","INFO:tensorflow:loss = 0.00016650985, step = 1400 (49.027 sec)\n","INFO:tensorflow:global_step/sec: 4.07935\n","INFO:tensorflow:examples/sec: 130.539\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (7, 151)\n","INFO:tensorflow:loss = 0.00022327402, step = 1600 (43.478 sec)\n","INFO:tensorflow:global_step/sec: 4.60012\n","INFO:tensorflow:examples/sec: 147.204\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 2.6315145e-05, step = 1800 (43.421 sec)\n","INFO:tensorflow:global_step/sec: 4.60601\n","INFO:tensorflow:examples/sec: 147.392\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (9, 13)\n","INFO:tensorflow:loss = 2.7237544e-05, step = 2000 (49.325 sec)\n","INFO:tensorflow:global_step/sec: 4.05476\n","INFO:tensorflow:examples/sec: 129.752\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (10, 68)\n","INFO:tensorflow:loss = 0.0006193553, step = 2200 (43.530 sec)\n","INFO:tensorflow:global_step/sec: 4.59456\n","INFO:tensorflow:examples/sec: 147.026\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (11, 154)\n","INFO:tensorflow:loss = 0.00010825169, step = 2400 (43.438 sec)\n","INFO:tensorflow:global_step/sec: 4.60426\n","INFO:tensorflow:examples/sec: 147.336\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 0.0008402325, step = 2600 (48.168 sec)\n","INFO:tensorflow:global_step/sec: 4.15208\n","INFO:tensorflow:examples/sec: 132.867\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (13, 0)\n","INFO:tensorflow:loss = 1.9909474e-05, step = 2800 (43.494 sec)\n","INFO:tensorflow:global_step/sec: 4.59833\n","INFO:tensorflow:examples/sec: 147.147\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (14, 86)\n","INFO:tensorflow:loss = 5.2454075e-06, step = 3000 (43.448 sec)\n","INFO:tensorflow:global_step/sec: 4.60325\n","INFO:tensorflow:examples/sec: 147.304\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (15, 171)\n","INFO:tensorflow:loss = 0.00017357004, step = 3200 (48.431 sec)\n","INFO:tensorflow:global_step/sec: 4.12962\n","INFO:tensorflow:examples/sec: 132.148\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 4.4405742e-06, step = 3400 (43.488 sec)\n","INFO:tensorflow:global_step/sec: 4.59894\n","INFO:tensorflow:examples/sec: 147.166\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (17, 7)\n","INFO:tensorflow:loss = 4.231884e-06, step = 3600 (43.417 sec)\n","INFO:tensorflow:global_step/sec: 4.6065\n","INFO:tensorflow:examples/sec: 147.408\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (18, 93)\n","INFO:tensorflow:loss = 4.4106687e-06, step = 3800 (63.520 sec)\n","INFO:tensorflow:global_step/sec: 3.14863\n","INFO:tensorflow:examples/sec: 100.756\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (19, 75)\n","INFO:tensorflow:loss = 5.7329777e-05, step = 4000 (43.604 sec)\n","INFO:tensorflow:global_step/sec: 4.58678\n","INFO:tensorflow:examples/sec: 146.777\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (20, 161)\n","INFO:tensorflow:loss = 1.01035685e-05, step = 4200 (43.390 sec)\n","INFO:tensorflow:global_step/sec: 4.60939\n","INFO:tensorflow:examples/sec: 147.5\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 5.6625067e-06, step = 4400 (48.122 sec)\n","INFO:tensorflow:global_step/sec: 4.1561\n","INFO:tensorflow:examples/sec: 132.995\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (22, 0)\n","INFO:tensorflow:loss = 5.0666476e-06, step = 4600 (43.502 sec)\n","INFO:tensorflow:global_step/sec: 4.59749\n","INFO:tensorflow:examples/sec: 147.12\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (23, 86)\n","INFO:tensorflow:loss = 9.2960676e-05, step = 4800 (43.284 sec)\n","INFO:tensorflow:global_step/sec: 4.62064\n","INFO:tensorflow:examples/sec: 147.86\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (24, 173)\n","INFO:tensorflow:loss = 1.9072967e-06, step = 5000 (47.879 sec)\n","INFO:tensorflow:global_step/sec: 4.1771\n","INFO:tensorflow:examples/sec: 133.667\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 1.162242e-06, step = 5200 (43.520 sec)\n","INFO:tensorflow:global_step/sec: 4.5956\n","INFO:tensorflow:examples/sec: 147.059\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (26, 12)\n","INFO:tensorflow:loss = 1.5498027e-06, step = 5400 (43.285 sec)\n","INFO:tensorflow:global_step/sec: 4.6206\n","INFO:tensorflow:examples/sec: 147.859\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (27, 98)\n","INFO:tensorflow:loss = 1.4006932e-06, step = 5600 (47.915 sec)\n","INFO:tensorflow:global_step/sec: 4.17409\n","INFO:tensorflow:examples/sec: 133.571\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (28, 161)\n","INFO:tensorflow:loss = 1.4901202e-06, step = 5800 (43.475 sec)\n","INFO:tensorflow:global_step/sec: 4.60035\n","INFO:tensorflow:examples/sec: 147.211\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 1.5794371e-06, step = 6000 (43.451 sec)\n","INFO:tensorflow:global_step/sec: 4.60291\n","INFO:tensorflow:examples/sec: 147.293\n","INFO:tensorflow:Saving checkpoints for 6000 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output/model.ckpt.\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Loss for final step: 1.5794371e-06.\n","INFO:tensorflow:training_loop marked as finished\n","Training took time  0:28:24.033739\n","0    15991\n","1    1130 \n","Name: label, dtype: int64\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_Do-iXtCOWBX","colab_type":"text"},"source":["## Error Analysis"]},{"cell_type":"code","metadata":{"id":"mEA145I2ZcTP","colab_type":"code","outputId":"2a27d669-47c0-4cdb-8208-c5543eeb9897","executionInfo":{"status":"ok","timestamp":1583777046285,"user_tz":0,"elapsed":1693,"user":{"displayName":"Fiona Fitwi","photoUrl":"","userId":"16113747830604572927"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["test.to_csv('gs://csc3002/trial/submission.csv', sep=',', index = False)\n","test.head(40)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>catch us on and tonight at 5pm nervous recovery</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>fathers day dubai uae my dubai father</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>can't wait to see tonight</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>i am awesome. i am positive affirmation</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>care free. stress free. happy. {#carefree stress free bernese mountain dog berner</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>live now girlfriend: webcam female webcam model babe pussy</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>live your dream of ice skating in nigeria visit the rink i promise you would be glad you came.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>ricky know what gets me so upset the president gives his condolence to victims that's not good enough all talk no action</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>nothing makes dad happier than a tasty meal! fathers day</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>12 hours to go. tonight is going to epic culture clash</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>my orchids early in the morning making me smile exciting orchids of insta gram</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>looking forward to having students working with us. world of work</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>which belgian festival(s) will you do with this year?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>nottingham on tuesday for my friends bihday. we are going to a cat cafe!!! kittys cute</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>if it walks like a duck, it quacks like, and it must be....and now the gop is concerned, and was quiet too late</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>sad that after a week long period of mourning in louisville for muhammad ali we have had a murder of a woman and now an lm</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>now in trend!!!registered</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>coolestlifehack: this is what landscape arch, devil's garden, utah looks like! motivation</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>i'm guessing that they're insecure and it makes them feel better to hate on successful people from our community.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>could this be our year? england euro2016</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>you are a child of the universe; no less than the trees and the stars buddah</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>90 minutes until vacation beings vacation all i ever wanted vacation had2 getaway</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>babies ergobaby original infant inse bouncing baby</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>first day at the beach weddings nervous new job</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>withasumi yesterday shot disney disneyland enjoy</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>i just realized that being a laker fan makes u hate every western conference team. just cant root 4 the dubs im petty lakers 2017 kobe</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>wishing all a and blessed sunday tnt bird pic via g.nawrang i appreciate all the follows, retweets,</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>geoffrey s royal orchid wishes you father's day</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>can't wait to eat theeese</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>penultimate week of placement stas tomorrow. can't believe how fast this has all gone and that in 2 weeks i will be leaving</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>30</td>\n","      <td>its official family guy has been confirmed for season 15 on fox adult comedy show animation show counting down cant wait</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>31</td>\n","      <td>knittin agin...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>32</td>\n","      <td>the bottom one is everything that's wrong with football, slowly turning me against the spo that was my first true love</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>33</td>\n","      <td>you bitches don't ever have y'all children. just let any and everybody watch them. trif bum</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>34</td>\n","      <td>brisk_and_vagabond-innocent___funky_joint-(ng072)-web-2007-ukhx_int . next generation web hardcore 1gabb</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>35</td>\n","      <td>i believe r willingness 2 try 2 understand 2 encourage and 2 inspire others 2 b and grow is a greater spiritual purpose grok</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>36</td>\n","      <td>tbt 1 :)! i am so 2 announce the land an twins will b involved in this years volleywood beach...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>37</td>\n","      <td>seems like \"#canadianvalues\" are ignoring reality, dividing and weakening, hate and . science cdn poli</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>38</td>\n","      <td>first for with our new customer pane rs drupa2016 hall 10 stand c13/3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>39</td>\n","      <td>why u change ur api policy</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  ... label\n","0   0   ...  0   \n","1   1   ...  0   \n","2   2   ...  0   \n","3   3   ...  0   \n","4   4   ...  0   \n","5   5   ...  0   \n","6   6   ...  0   \n","7   7   ...  0   \n","8   8   ...  0   \n","9   9   ...  0   \n","10  10  ...  0   \n","11  11  ...  0   \n","12  12  ...  0   \n","13  13  ...  0   \n","14  14  ...  0   \n","15  15  ...  0   \n","16  16  ...  0   \n","17  17  ...  0   \n","18  18  ...  0   \n","19  19  ...  0   \n","20  20  ...  0   \n","21  21  ...  0   \n","22  22  ...  0   \n","23  23  ...  0   \n","24  24  ...  0   \n","25  25  ...  0   \n","26  26  ...  0   \n","27  27  ...  0   \n","28  28  ...  0   \n","29  29  ...  0   \n","30  30  ...  0   \n","31  31  ...  0   \n","32  32  ...  0   \n","33  33  ...  0   \n","34  34  ...  0   \n","35  35  ...  0   \n","36  36  ...  0   \n","37  37  ...  1   \n","38  38  ...  0   \n","39  39  ...  0   \n","\n","[40 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"B9lyIifupp6U","colab_type":"text"},"source":["<b>Our test dataset is supervised for HatEval so perhaps we can do some analysis here by peeking at where we went wrong</b>"]},{"cell_type":"code","metadata":{"id":"w_ek6QNkOXqe","colab_type":"code","colab":{}},"source":["if DATASET == \"HatEval\":\n","  def get_missed_HS(df):\n","    df = df.loc[(df[\"label\"] == 1) & (df[\"predictions\"] == 0)]\n","    return df\n","\n","  def get_misclassified_HS(df):\n","    df = df.loc[(df[\"label\"] == 0) & (df[\"predictions\"] == 1)]\n","    return df\n","  #Perhaps having a separate function for both is a good idea\n","\n","  df = get_missed_HS(test)\n","  df.to_csv('gs://csc3002/hateval2019/falsenegatives.csv', sep=',',  index = True, encoding = 'utf-8')\n","  df.head(50)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-kH3pri2hFlU","colab_type":"code","outputId":"215a4615-40db-42d3-9af1-e655247ba942","executionInfo":{"status":"ok","timestamp":1583415953336,"user_tz":0,"elapsed":1101,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["if DATASET == \"HatEval\":\n","  df = get_misclassified_HS(test)\n","  df.to_csv('gs://csc3002/hateval2019/falsepositives.csv', sep=',',  index = True, encoding = 'utf-8')\n","  df.head(50)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","      <th>predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>y'all: \"you a hoe\" hoe me: \"that's hoe culture to you bitch\"</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>the usa has a major debt problem and thus cannot afford big tax cuts for the wealthy, big globalist corporations, rinos, &amp; wall street. usa americans america pj net end daca no amnesty congress</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>theres nothing more i hate than joffery. cunt lookin ass bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>ben, you are shit on my shoe still bitch.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>thank you president trump. keeping his promise to farmers. confirm judge kavanaugh keep maga build that wall thanks to all that support trump and his supporters.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>pocohantas speaks with forked tongue. liberalism is a mental disorder maga america first term limits drain the swamp build the wall send them back</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>you got me with the bitch ass hoe comment!! face with tears of joy face with tears of joy loudly crying face skull</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>you can rent a hoe but you cant buy a bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>build that wall ted cruz has a plan on how to help republicans win big in november!</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>this is way scary, someone stop this evil person build the wall end daca boycott nfl boycott nike lock them all up</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>whoever called me unknown and said the shit they said...you're a pussy ass bitch.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>yep, five lib tarde d countries that opened borders for rapefugee savages and are blaming their own women and girls for being raped, nice try, america first walk away maga2018 liberalism is a mental disease</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>bitch i'm kuntry i do what i want</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>32</td>\n","      <td>stop the invasion use our defense budget now 2 defend america! don't wait on mitch &amp; paul! friday fee i ing</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>35</td>\n","      <td>preach! nails it (may contain strong language) rt if uagree the walk away woke movement will bring the democrat party down! americans first build that wall thursday thoughts flashback friday saturday morning sunday fun day mag a party</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>36</td>\n","      <td>\"time to wake up!\"\"i walked away from the democrats when they said illegal immigration &amp; open borers mattered more than me &amp; my family,\"\"you &amp; your family deserve to be safe.president trump's highest duty is to keep america safe.#noamnesty no daca e verify build the wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>42</td>\n","      <td>calling bs. as a femboy or a tomboy this bitch would have better \"rapey\" radar.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>44</td>\n","      <td>they can scrim whoever they fucking want this isn't a fucking chall you dumb bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>46</td>\n","      <td>for every social security number that was sold there is an american victim. build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>47</td>\n","      <td>it's dispicable that he is mayor, and really goes to show what the demographics of london are. stop immigration</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>48</td>\n","      <td>amen one nation under god to godbetheglorywwg1wga blue lives matter walk away patriots united america first build that wall best president ever save the children choose life</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>49</td>\n","      <td>illegal aliens or as the irs refers to them undocumented aliens let's not let the left brain wash us. and it is the legal term.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>50</td>\n","      <td>, mr negativity is there any good in your world? not a very uplifting speech. how sad for you. me? i thrilled with the state of my nation. things are wonderful and we are winning god bless america maga build that wall lock her up</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>51</td>\n","      <td>the first person (trump) to pack a stadium &amp; not get paid for it. priceless. o'puppet on the other hand threatened to cancel a speech if the fee $$ amt was disclosed to the public. sad. maga kag drain the deep state obama speech build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>52</td>\n","      <td>wheres the justice? oh right. shes an american. build that wall maga2kag mag a veteran</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>53</td>\n","      <td>like he ever kept out any threats. he's lying as usual. build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>56</td>\n","      <td>i want them and to fight you bc a bitch can't even buy those shoes in america</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>60</td>\n","      <td>this is what corrupt democrat leadership gets you.q.e.d.#dealwithit wwg1wga q a non q red wave walk away america first fisa gate lock her up lock them all up build the wall deport them all</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>61</th>\n","      <td>61</td>\n","      <td>i agree with you and look forward to you replacing the swamp rat we need congress members willing to work with . he has done a great job in just 18 months cant wait for the next 6 &amp; 1/2 years. build that wall maga</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>62</th>\n","      <td>62</td>\n","      <td>no never, they are some sick sick people, attacking children as they do build the wall end daca boycott nfl boycott nike lock them all up</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>64</td>\n","      <td>daca is an executive order written by barack hussein obama. trump has every right to undo this travesty. decision must be fast tracked to the supreme court asap! end daca white house ordered to restart daca</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>65</td>\n","      <td>drop that ugly soundcloud bum ass nigga already let his hoes lurk bitch they aint got shit on you</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>66</td>\n","      <td>im a texan but im rooting for to win. saw you this morning and was very impressed with your credentials and your service to our great country! wish i could cast a vote for you. best of luck! maga kaga build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>67</th>\n","      <td>67</td>\n","      <td>westminster terror attack suspect named as 'sudanese rapefugee who drove around london looking for targets' before driving car into cyclists</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>68</td>\n","      <td>of course life is a bitch.. if it was a slut, it would be easy.. mayward mahalkitakasibiga10na</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>69</td>\n","      <td>block u bot. democrats hate america walk away from democrats democrats are racists watch death of a nation trump forever god bless potus trump god bless f lotus melania god bless america keep america great trump2020 winning build that wall votered2018 usa usa usa usa usa usa usa usa</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>70</th>\n","      <td>70</td>\n","      <td>pussy ass bitch cunt fuck i hate you.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>72</th>\n","      <td>72</td>\n","      <td>confirm kavanaugh democrats will thrash and scream no matter what does. rinos stop obstructing let's fulfill maga agenda!#buildthatwall#nosanctuary#draintheswamp#americafirst#confirmjudgekavanaugh#democratshateamerica#walkaway#voteredtosaveamerica</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>74</th>\n","      <td>74</td>\n","      <td>the reason for the additional beds is based on the number of (illegal alien) minors, who crossed the border alone without their parent or legal guardian.#keepfamiliestogether deport illegal aliens</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>76</th>\n","      <td>76</td>\n","      <td>cant people have new friends?! why do you have to keep thoes people all to yourself?! tf bitch!!</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>78</th>\n","      <td>78</td>\n","      <td>bitch, as if there's any greater joy in life than negativity and discourse.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>80</th>\n","      <td>80</td>\n","      <td>take your shoes off when you enter my home. bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>82</th>\n","      <td>82</td>\n","      <td>lol had to share! i am spartacus liberalism is a mental disorder redwaverising2018 build that wall kag2018 kag2020 kavanaugh hearings</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>86</th>\n","      <td>86</td>\n","      <td>sorry for all the cunts in this thread. little boys wouldn't know how to handle a punk bitch anyways clapping hands clapping hands</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>88</th>\n","      <td>88</td>\n","      <td>we are thrilled he is our president and cant wait for him to build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>89</th>\n","      <td>89</td>\n","      <td>how about bishops who know something is wrong should do something?#buyfrominandout internet bill of rights dark left walk away reinstate robyn gritz jimjordan4 speaker 2a de fund pp chain deportation build that wall national reciprocity 1a free julian boycott nfl</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>90</th>\n","      <td>90</td>\n","      <td>rape the rapist fuck the rapist rape the rapist and im doing it with h8te bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>91</th>\n","      <td>91</td>\n","      <td>coulter: liberals never sleep (and neither does jeff sessions) | breitbart via is the only person in dc serious about immigration reform. build the wall now we want our wall and end daca</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>93</th>\n","      <td>93</td>\n","      <td>do you mean illegal alien or illegal aliens or the legal definition of illegal alien as defined by the federal government.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>95</td>\n","      <td>bitch and you froze mine! now how am i gonna get new shoes before i leave? expressionless face</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  ... predictions\n","2   2   ...  1         \n","4   4   ...  1         \n","6   6   ...  1         \n","7   7   ...  1         \n","11  11  ...  1         \n","12  12  ...  1         \n","17  17  ...  1         \n","18  18  ...  1         \n","21  21  ...  1         \n","24  24  ...  1         \n","25  25  ...  1         \n","26  26  ...  1         \n","27  27  ...  1         \n","32  32  ...  1         \n","35  35  ...  1         \n","36  36  ...  1         \n","42  42  ...  1         \n","44  44  ...  1         \n","46  46  ...  1         \n","47  47  ...  1         \n","48  48  ...  1         \n","49  49  ...  1         \n","50  50  ...  1         \n","51  51  ...  1         \n","52  52  ...  1         \n","53  53  ...  1         \n","56  56  ...  1         \n","60  60  ...  1         \n","61  61  ...  1         \n","62  62  ...  1         \n","64  64  ...  1         \n","65  65  ...  1         \n","66  66  ...  1         \n","67  67  ...  1         \n","68  68  ...  1         \n","69  69  ...  1         \n","70  70  ...  1         \n","72  72  ...  1         \n","74  74  ...  1         \n","76  76  ...  1         \n","78  78  ...  1         \n","80  80  ...  1         \n","82  82  ...  1         \n","86  86  ...  1         \n","88  88  ...  1         \n","89  89  ...  1         \n","90  90  ...  1         \n","91  91  ...  1         \n","93  93  ...  1         \n","95  95  ...  1         \n","\n","[50 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"fpDsa8ejf14-","colab_type":"text"},"source":["# Using Tensorboard to get deeper insight"]},{"cell_type":"code","metadata":{"id":"x3yg60YrO-Ub","colab_type":"code","outputId":"9b5458df-1e8f-4ee7-fc5f-33451376938b","executionInfo":{"status":"ok","timestamp":1581533997663,"user_tz":0,"elapsed":16002,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip   #Downloads file to google drive"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-02-12 18:59:42--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 34.233.35.85, 3.229.196.117, 34.193.139.214, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|34.233.35.85|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13773305 (13M) [application/octet-stream]\n","Saving to: ‚Äòngrok-stable-linux-amd64.zip.1‚Äô\n","\n","ngrok-stable-linux- 100%[===================>]  13.13M  34.5MB/s    in 0.4s    \n","\n","2020-02-12 18:59:43 (34.5 MB/s) - ‚Äòngrok-stable-linux-amd64.zip.1‚Äô saved [13773305/13773305]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y97-_YvVXeIy","colab_type":"code","outputId":"ffe1589a-0c5e-41b9-91a9-37673560dd33","executionInfo":{"status":"ok","timestamp":1581701877698,"user_tz":0,"elapsed":2183,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":217}},"source":["\n","def get_tensorboard(path_to_event_file = OUTPUT_DIR):\n","  get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 --reload_multifile=true &'\n",".format(path_to_event_file))\n","  \n","  get_ipython().system_raw('./ngrok http 6006 &')\n","\n","  !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","      \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n","\n","get_tensorboard(OUTPUT_DIR)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"<string>\", line 1, in <module>\n","  File \"/usr/lib/python3.6/json/__init__.py\", line 299, in load\n","    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n","  File \"/usr/lib/python3.6/json/__init__.py\", line 354, in loads\n","    return _default_decoder.decode(s)\n","  File \"/usr/lib/python3.6/json/decoder.py\", line 339, in decode\n","    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n","  File \"/usr/lib/python3.6/json/decoder.py\", line 357, in raw_decode\n","    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n","json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"],"name":"stdout"}]}]}