{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Running_BERT_Tensorflow_TPU.ipynb","provenance":[{"file_id":"1EBH_dCvSIfTM-VlKeBfV2Em3exIJzkih","timestamp":1576806467871},{"file_id":"1JuDopUfWUBPOSoIOBuqs7S2tH2FJHokl","timestamp":1575402454368},{"file_id":"1VNukx0WgDZ6kdgs4gvDj6zZ5czaNCAO6","timestamp":1575299325999},{"file_id":"https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb","timestamp":1574879594425}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"j0a4mTk9o1Qg","colab_type":"code","colab":{}},"source":["# Copyright 2019 Google Inc.\n","\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Q4eUHbzUO-c","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/kpe/bert-for-tf2/blob/master/examples/tpu_movie_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"dCpvgG0vwXAZ","colab_type":"text"},"source":["# Detecting Hate Speech Tweets With BERT"]},{"cell_type":"markdown","metadata":{"id":"xiYrZKaHwV81","colab_type":"text"},"source":["We are using bert-tensorflow for this classification task. At the moment I'm making sure it's tensorflow version 1.x because tensorflow version 2 gives issues with Bert at the moment. I believe Tensorflow hopes to have this issue resolved in tensorflow v 2.1\n","\n","We are using a TPU as a GPU does not have the required memory for Large BERT models- it can only cope with the base model. We'll see if there a TPU detected and we'll set it to a global environment variable so it can be accessed by our BERT functions later."]},{"cell_type":"code","metadata":{"id":"mYEcG2vlPumC","colab_type":"code","outputId":"70dc9996-17b2-4c3b-f5f1-c7c6324aef6a","executionInfo":{"status":"ok","timestamp":1585598960141,"user_tz":-60,"elapsed":86016,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":884}},"source":["!pip install gcsfs \n","import pandas as pd\n","import numpy as np\n","\n","#Make sure to use tensorflow version 1.x, version 2 doesn't work with bert\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","#!pip install gast==0.2.2\n","import os\n","\n","#For cross-validation and grid search\n","from itertools import product\n","from tensorflow.python.summary.summary_iterator import summary_iterator\n","from google.cloud import storage\n","import ipywidgets as widgets\n","from IPython.display import display\n","\n","import sklearn\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from sklearn import metrics\n","\n","\n","import html\n","import re\n","import json\n","import pprint\n","import random\n","import string\n","import nltk\n","from datetime import datetime\n","import time\n","\n","\n","assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n","TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","print('TPU address is', TPU_ADDRESS)\n","\n","#Below we give ourselves as well as the TPU access to our private GCS bucket\n","from google.colab import auth\n","auth.authenticate_user()\n","tf.reset_default_graph()  \n","with tf.Session(TPU_ADDRESS) as session:\n","  # Upload credentials to TPU.\n","  with open('/content/adc.json', 'r') as f:\n","    auth_info = json.load(f)\n","  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n","\n","USE_TPU=True\n","try:\n","  #tf.config.experimental_connect_to_host(TPU_ADDRESS)\n","  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n","  tf.config.experimental_connect_to_cluster(cluster_resolver)\n","  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n","  tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n","except Exception as ex:\n","  print(ex)\n","  USE_TPU=False\n","\n","print(\"        USE_TPU:\", USE_TPU)\n","print(\"Eager Execution:\", tf.executing_eagerly())\n","\n","assert not tf.executing_eagerly(), \"Eager execution on TPUs have issues currently\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting gcsfs\n","  Downloading https://files.pythonhosted.org/packages/18/3b/454be7c97d05e15eb20a0099f425f0ed6b7552e352c77adb923c3872ba14/gcsfs-0.6.1-py2.py3-none-any.whl\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.6.3)\n","Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.2)\n","Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.7.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.21.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (46.0.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (3.1.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.11.28)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n","Installing collected packages: gcsfs\n","Successfully installed gcsfs-0.6.1\n","TensorFlow 1.x selected.\n","TPU address is grpc://10.38.114.34:8470\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","INFO:tensorflow:Initializing the TPU system: 10.38.114.34:8470\n","INFO:tensorflow:Finished initializing TPU system.\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.38.114.34:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 7486113471355553058)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 10881844383774876286)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 14885532453347823988)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5416682320606148372)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9224989939203307028)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 948681528051587946)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 12113359471842665753)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15005023230875124826)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 13454155276213341957)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 5916834379988377915)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 1708833923954035705)\n","        USE_TPU: True\n","Eager Execution: False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sRh9YYYfgUTE","colab_type":"text"},"source":["Setting a random seed for reproducability of results and checking version of tensorflow"]},{"cell_type":"code","metadata":{"id":"dgVteKeTgXrg","colab_type":"code","outputId":"4fc8d6b6-829f-4d7e-b711-415452083c46","executionInfo":{"status":"ok","timestamp":1585598960142,"user_tz":-60,"elapsed":85994,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Setting the graph-level random seed for the default graph. Different than operation level seed\n","SEED = 3060\n","tf.reset_default_graph()\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","tf.set_random_seed(SEED) \n","random.seed(SEED)\n","np.random.seed(SEED)\n","print(\"Tensorflow Version:\", tf.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tensorflow Version: 1.15.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KVB3eOcjxxm1","colab_type":"text"},"source":["Below we will set the directory where we will store our output model. To ensure the right variables are loaded in our run config function later, our output directory must be in the same directory as our pre-trained bert model directory.\n","\n","Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."]},{"cell_type":"code","metadata":{"id":"US_EAnICvP7f","colab_type":"code","cellView":"both","outputId":"29e84428-8285-4a61-dab8-cd258a24bbe7","executionInfo":{"status":"ok","timestamp":1585598965509,"user_tz":-60,"elapsed":91343,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#Large whole word masking BERT pre-trained weights\n","bert_model_name = 'wwm_uncased_L-24_H-1024_A-16' \n","\n","#Where we output the fine tuned model\n","output_dir = os.path.join(bert_model_name, 'output1')\n","\n","DATASET = \"HatEval\" #@param [\"HatEval\", \"AnalyticsVidhya\"]\n","\n","#@markdown Whether or not to use the further pretrained model\n","FURTHER_PRETRAINED = True #@param {type:\"boolean\"}\n","if FURTHER_PRETRAINED == True:\n","\n","  further_pretrained_model = os.path.join(bert_model_name, 'further_pretrained_model')\n","  output_dir = os.path.join(further_pretrained_model, 'output1')\n","\n","#@markdown Whether or not to clear/delete the directory and create a new one\n","DO_DELETE = True #@param {type:\"boolean\"}\n","#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","USE_BUCKET = True #@param {type:\"boolean\"}\n","BUCKET = 'csc3002' #@param {type:\"string\"}\n","os.environ[\"GCLOUD_PROJECT\"] = \"csc3002\"\n","\n","if USE_BUCKET:\n","  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, output_dir)\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","if DO_DELETE:\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","    # Doesn't matter if the directory didn't exist\n","    pass\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["***** Model output directory: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output1 *****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yDFCBYMB-m3v","colab_type":"text"},"source":["<b> If you're not connected to a TPU environment but still want to access GCS bucket - run below: </b>"]},{"cell_type":"code","metadata":{"id":"65aCrle2Bmmi","colab_type":"code","outputId":"e8c77507-5d25-4fd9-cfc8-6228a4ff3935","executionInfo":{"status":"ok","timestamp":1585598965510,"user_tz":-60,"elapsed":91327,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\"\"\"from google.colab import drive\n","drive.mount('/content/drive')\n","!gcloud auth activate-service-account --key-file '/content/drive/My Drive/storageCreds.json'\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"from google.colab import drive\\ndrive.mount('/content/drive')\\n!gcloud auth activate-service-account --key-file '/content/drive/My Drive/storageCreds.json'\\n\""]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"UmeFZJIvTy33","colab_type":"text"},"source":["<b>Setting up Data Based Upon Choice of DATASET</b>\n"]},{"cell_type":"code","metadata":{"id":"jhIIdbOETxC4","colab_type":"code","colab":{}},"source":["if DATASET == 'HatEval':\n","  dirc = 'gs://csc3002/hateval2019'\n","\n","  rawTrain = pd.read_csv(os.path.join(dirc, 'hateval2019_en_train.csv'), sep=',',  index_col = False, encoding = 'utf-8')\n","  rawTrain.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","  rawTrain.drop(['TR', 'AG'], inplace = True, axis = 1)\n","\n","  rawDev = pd.read_csv(os.path.join(dirc, 'hateval2019_en_dev.csv'), sep=',',  index_col = False, encoding = 'utf-8')\n","  rawDev.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","  rawDev.drop(['TR', 'AG'], inplace = True, axis = 1)\n","\n","  rawTest = pd.read_csv(os.path.join(dirc, 'hateval2019_en_test.csv'), sep=',',  index_col = False, encoding = 'utf-8')\n","  rawTest.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","  rawTest.drop(['TR', 'AG'], inplace = True, axis = 1)\n","\n","  imbalanced = False\n","\n","elif DATASET == \"AnalyticsVidhya\":\n","  dirc = 'gs://csc3002/trial'\n","\n","  rawTrain= pd.read_csv(os.path.join(dirc, 'train_E6oV3lV.csv'),  sep=',',  index_col = False, encoding = 'utf-8')\n","  \n","  rawTrain, rawDev = train_test_split(rawTrain, test_size=0.20, random_state=SEED)\n","  \n","  rawTest = pd.read_csv(os.path.join(dirc, 'test_tweets_anuFYb8.csv'), sep=',',  index_col = False, encoding = 'utf-8')\n","  \n","  imbalanced = True\n","\n","else:\n","  raise ValueError('No Valid DATASET chosen')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pmFYvkylMwXn","colab_type":"text"},"source":["# Training Data\n","I've stored all of the data, (train, dev and test),  in my google bucket for ease of access, authentication will have to be provided"]},{"cell_type":"code","metadata":{"id":"UvknR984WeDW","colab_type":"code","outputId":"2c3c97ef-3a6c-4325-b611-0bd78e02fda1","executionInfo":{"status":"ok","timestamp":1585598973572,"user_tz":-60,"elapsed":99359,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!gcloud config set project 'my-project-csc3002'\n","\n","train = rawTrain.sample(frac=1, random_state = SEED) #Shuffling really helps model performance\n","train.reset_index(drop = True, inplace = True)\n","train.id = train.index\n","pd.set_option('display.max_colwidth', -1)\n","print(\"Out of {} tweets in this database, {} are not hate, {} are hate\".format(len(train.index), \n","                                                      len(train[train['label']==0]),\n","                                                      len(train[train['label']==1])))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Updated property [core/project].\n","Out of 9000 tweets in this database, 5217 are not hate, 3783 are hate\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"57uQmKX8yQGq","colab_type":"text"},"source":["<b>Original Dataset </b>"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"8434e635-c130-4505-c55c-f4cba3e8aedb","executionInfo":{"status":"ok","timestamp":1585598973573,"user_tz":-60,"elapsed":99340,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"id":"TblezF_V2DrA","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train.head(30)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>How come Allah is not helping you it is up to Christian countries to  protect you feed you ,The countries hit by violence from islam take refugees in feed them etcPlease no more explaining about your hard times we are doing our best for uYes there is good and bad every where</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>With todays #JalalabadAttack &amp;amp; other vicious attacks claimed by ISIS, I smell a spillover of refugees in Pak again. This time we should not open borders for them. We cant afford terrorists taking undue advantage. Let Americans deal with it.#Afghanistan #Jalalabad</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>https://t.co/i9LJDjtGz7Migration greatest threat’ to Austrian security, says top military figure.EU and Europe bitterly dividedðŸ‘‰major confrontations between the two.Nothing more counterproductive than “centers” on European territory or euro bribes for migrants.#Visegrad #V4 https://t.co/VnPCTe7opC</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>When all your friends are out hoe'in and you're stuck at home in a shitty relationship https://t.co/X9oz1Tx7TC</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>I wonder if rick will make another deal with those crazy ass women 🤔 and if that crazy ass nigga will actually hoe Daryl again 😐</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>Worker Charged With Sexually Molesting Eight Children at Immigrant Shelter https://t.co/D6HcH03nGL via @CitizenTruth_ #realDonaldTrump do something about this disgrace and stop separating children from their parents.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>UN seeks new funding pledges for Palestinian refugees... https://t.co/SNJhD1PWxT https://t.co/DlHQ8fc5N6</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>If you really wanna know what someone you're fucking thinks about you, make them show you how you're stored in their phone...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>Going to make Du'a at the shrine of Imam Reza(AS) for the refugees in Athens.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>Poor kid. Someone wise must have told him, \"When the world gives you lemons, make lemonade.\" He listened. His lemonade should now be offered with ICE in abundance. #BuildTheWall #SendThemBack https://t.co/8AM7fgo9ph</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>@TVRav You got reply in before mine, but what I'm pointing out is women work their lives around being safe from *some men*, not all men</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>Her- remember when you said you loved me? Me- https://t.co/hv9WiaqXTu</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>I stand with the victims of sexual harassment or rape. No matter if they're a woman or a man. A rape is a rape what… https://t.co/PpiL6mBtqV</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>Canada's @JustinTrudeau appointed to his cabinet immigrants from such cultures. One is in charge of immigration &amp;amp; refugee affairs - license to flood Canada with own kind. The other, Education Minister, tried to sneak into new Education code female genital mutilation as a right!</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>@graysonslays marcos you skank hannah montana is my thinf</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>@_miidnightr Wanna speak down to my girl bitch then step up big nose ass bitch. Saw that shit off.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>Philippines - Christians are facing starvation as they continue to hide from jihadists.  An estimate 1500 remain... https://t.co/P04dSj0nbs</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>Thank you so much to @WValderrama &amp;amp; @BoomEventsLA for hosting such an amazing event this past weekend! No child should face immigration court w/o an attorney by their side &amp;amp; this event helped raise critical awareness for these immigrant &amp;amp; refugee children #KINDLA #GivingKINDness https://t.co/v3bxbNMsYx</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>Could you open up your home to refugees in need? https://t.co/0F2rzmYCSw</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>It's unfortunate that such racist tactics are used against migrant communities. Remember those who are spreading fear are those who cut the funding of vital services for migrants such as healthcare and education!  https://t.co/82hkXRW6L1 via @theage</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>bitch what the fuck do you mean you cant find the fucking page cunt i need to vote for my boys dont you understand https://t.co/xbdlkyXah7</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>World Refugee Day was this past Wednesday, but there is still time to make a difference for the 20,000 refugees who will arrive in the U.S. this year. Rally the support of your friends and family by becoming a fundraiser for refugees. Get started here âž_x009d_ https://t.co/QzFjSqWwJ8 https://t.co/vS9TqvpVm6</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>US Senate Calls On Julian Assange To Testify https://t.co/Qqy6GO6jm7 https://t.co/arrxfZVTdb</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>When you find a bag of drugs on the ground https://t.co/j8JcFgLCHP</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>When you're flexin on these bitches https://t.co/7fR6wozeU4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>@SenKamalaHarris So now the word hysterical is a completely sexist term? So let's get this straight nobody can say the word monkey anymore when they are describing anyting and the word hysterical can no longer be used to describe a hysterical woman?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>VIDEO: Immigrant activist who climbed Statue of Liberty has a new song: \"America, you motherfuckers,... https://t.co/YSjmys3MPK</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>@Scouse_ma hi 😘</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>Picture: the couple Engelbert and Spera, both Jews. Engelbert is now elected in the Austrian parliament for the Ã–VP. He says in the newspaper HaÃ retz the danger for Jews doesnt come from FPÃ–Nazis, more from Islamic refugees, which import antisemitism to Austria. https://t.co/PhsxnDXWxP</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>* Croatia ðŸ‡­ðŸ‡·:  The Croatian authorities, surprisingly, reject the EU’s bid of 6,000 euros per migrant.  ðŸ‘_x008f_ https://t.co/hccQmsQ9Vl #v4 #visegrad https://t.co/1x1MXOQm2t</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  ... label\n","0   0   ...  0   \n","1   1   ...  1   \n","2   2   ...  0   \n","3   3   ...  0   \n","4   4   ...  1   \n","5   5   ...  0   \n","6   6   ...  0   \n","7   7   ...  0   \n","8   8   ...  0   \n","9   9   ...  0   \n","10  10  ...  0   \n","11  11  ...  0   \n","12  12  ...  0   \n","13  13  ...  1   \n","14  14  ...  0   \n","15  15  ...  1   \n","16  16  ...  0   \n","17  17  ...  0   \n","18  18  ...  0   \n","19  19  ...  0   \n","20  20  ...  0   \n","21  21  ...  0   \n","22  22  ...  0   \n","23  23  ...  0   \n","24  24  ...  0   \n","25  25  ...  0   \n","26  26  ...  0   \n","27  27  ...  0   \n","28  28  ...  0   \n","29  29  ...  1   \n","\n","[30 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"aRLD57l1z631","colab_type":"text"},"source":["### Text Preprocessing\n","\n","The text pre-processing for this project is detailed in the notebook `Text_Preprocessing.ipynb` in the github repo. Below is an import of the repo into the google colab workspace so I can retrieve and use these functions at convenience\n","\n","Also below is a function which loads whichever dataset I choose to load from my GCS bucket or local system. This will be useful later when I want to quickly load in data without the messy, long-winded code to go along with it."]},{"cell_type":"code","metadata":{"id":"QR1Ee-91AMgd","colab_type":"code","cellView":"both","colab":{}},"source":["#@title Text Pre-Processing Options\n","HASHTAG_SEGMENTATION = True #@param {type:\"boolean\"}\n","EMOJI_REPLACEMENT = \"Replace_Emoji_v1\" #@param [\"None\", \"Replace_Emoji_v1\", \"Replace_Emoji_v2\"]\n","LEMMATIZE = False #@param {type:\"boolean\"}\n","REMOVE_STOPWORDS = False #@param {type:\"boolean\"}\n","REMOVE_PUNCTUATION = False #@param {type:\"boolean\"}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sxrBp7dg0bp-","colab_type":"code","outputId":"c908172b-3419-466c-da06-ae2da79df65c","executionInfo":{"status":"ok","timestamp":1585599001643,"user_tz":-60,"elapsed":127375,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["!git clone https://github.com/fionnmcconville/Automatic-Detection-of-Hate-Speech-Online-Using-BERT.git\n","%cd Automatic-Detection-of-Hate-Speech-Online-Using-BERT \n","#!ls\n","import preprocessing as pre\n","#Return to original workspace\n","%cd ..\n","\n","#Function caller can optionally load two dataframes and combine them\n","def loadData(data1, data2 = None):\n","\n","  if data2 is not None:\n","    frames = [data1,data2]\n","    data = pd.concat(frames)\n","  else:\n","    data = data1\n","  \n","  #Replace emoji must be done before basic preprocess otherwise unicode will be wiped out\n","  #And this function will be ineffective\n","  if EMOJI_REPLACEMENT == 'Replace_Emoji_v1':\n","    data['tweet'] = data['tweet'].apply(pre.emojiReplace)\n","\n","  if EMOJI_REPLACEMENT == 'Replace_Emoji_v2':\n","    data['tweet'] = data['tweet'].apply(pre.emojiReplace_v2)\n","\n","  #Must be performed after emoji translation\n","  data['tweet'] = data['tweet'].apply(pre.preprocess)\n","\n","  if HASHTAG_SEGMENTATION == True:\n","    data['tweet'] = data['tweet'].apply(pre.hashtagSegment)\n","\n","  if REMOVE_PUNCTUATION == True:\n","    data['tweet'] = data['tweet'].apply(lambda x: pre.remove_punct(x))\n","\n","  if REMOVE_STOPWORDS == True:\n","    data['tweet'] = data['tweet'].apply(lambda x: pre.remove_stopwords(x))\n","\n","  if LEMMATIZE == True:\n","    data['tweet'] = data['tweet'].apply(lambda x: pre.lemmatizing(x))\n","\n","  #Remove small sequences that could skew model\n","  #data = data[data['tweet'].apply(lambda x: len(x) > 10)]\n","  data.dropna(inplace = True)\n","  data.reset_index(drop = True, inplace = True) \n","  if DATASET == \"AnalyticsVidhya\"  and len(data.index) < 20000:\n","    return data\n","  else:\n","    #We don't shuffle data when it is the analytics vidhya test set\n","    data = data.sample(frac = 1, random_state=SEED) # Shuffle data \n","  return data\n","\n","#Testing function\n","train = loadData(rawTrain)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'Automatic-Detection-of-Hate-Speech-Online-Using-BERT'...\n","remote: Enumerating objects: 75, done.\u001b[K\n","remote: Counting objects: 100% (75/75), done.\u001b[K\n","remote: Compressing objects: 100% (55/55), done.\u001b[K\n","remote: Total 115 (delta 36), reused 53 (delta 19), pack-reused 40\u001b[K\n","Receiving objects: 100% (115/115), 87.26 MiB | 35.78 MiB/s, done.\n","Resolving deltas: 100% (40/40), done.\n","/content/Automatic-Detection-of-Hate-Speech-Online-Using-BERT\n","\u001b[33mDownloading emoji data ...\u001b[0m\n","\u001b[92m... OK\u001b[0m (Got response in 0.13 seconds)\n","\u001b[33mWriting emoji data to /root/.demoji/codes.json ...\u001b[0m\n","\u001b[92m... OK\u001b[0m\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g020ACV22uFG","colab_type":"text"},"source":["**Cleaned tweet text dataset**"]},{"cell_type":"code","metadata":{"id":"lBP7J70v2ueo","colab_type":"code","outputId":"df74cc25-df65-4ea5-a788-f8faf7c6215e","executionInfo":{"status":"ok","timestamp":1585599001644,"user_tz":-60,"elapsed":127353,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train[:30]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>479</th>\n","      <td>680</td>\n","      <td>how come allah is not helping you it is up to christian countries to protect you feed you ,the countries hit by violence from islam take refugees in feed them etcplease no more explaining about your hard times we are doing our best for uyes there is good and bad every where</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3794</th>\n","      <td>3995</td>\n","      <td>with todays jalalabad attack and other vicious attacks claimed by isis, i smell a spillover of refugees in pak again. this time we should not open borders for them. we cant afford terrorists taking undue advantage. let americans deal with it.#afghanistan jalalabad</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2512</th>\n","      <td>2713</td>\n","      <td>greatest threat to austrian security, says top military figure.eu and europe bitterly dividedmajor confrontations between the two.nothing more counterproductive than centers on european territory or euro bribes for migrants.#visegrad v4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7308</th>\n","      <td>7509</td>\n","      <td>when all your friends are out hoe'in and you're stuck at home in a shitty relationship</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6183</th>\n","      <td>6384</td>\n","      <td>i wonder if rick will make another deal with those crazy ass women thinking face and if that crazy ass nigga will actually hoe daryl again neutral face</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>427</th>\n","      <td>628</td>\n","      <td>worker charged with sexually molesting eight children at immigrant shelter via real donald trump do something about this disgrace and stop separating children from their parents.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3040</th>\n","      <td>3241</td>\n","      <td>un seeks new funding pledges for palestinian refugees...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7419</th>\n","      <td>7620</td>\n","      <td>if you really wanna know what someone you're fucking thinks about you, make them show you how you're stored in their phone...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4332</th>\n","      <td>4533</td>\n","      <td>going to make du'a at the shrine of imam reza(as) for the refugees in athens.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3554</th>\n","      <td>3755</td>\n","      <td>poor kid. someone wise must have told him, \"when the world gives you lemons, make lemonade.\" he listened. his lemonade should now be offered with ice in abundance. build the wall send them back</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8128</th>\n","      <td>8329</td>\n","      <td>you got reply in before mine, but what i'm pointing out is women work their lives around being safe from *some men*, not all men</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6922</th>\n","      <td>7123</td>\n","      <td>her- remember when you said you loved me? me-</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8554</th>\n","      <td>8755</td>\n","      <td>i stand with the victims of sexual harassment or rape. no matter if they're a woman or a man. a rape is a rape what</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1237</th>\n","      <td>1438</td>\n","      <td>canada's appointed to his cabinet immigrants from such cultures. one is in charge of immigration and refugee affairs - license to flood canada with own kind. the other, education minister, tried to sneak into new education code female genital mutilation as a right!</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7818</th>\n","      <td>8019</td>\n","      <td>marcos you skank hannah montana is my thinf</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5119</th>\n","      <td>5320</td>\n","      <td>wanna speak down to my girl bitch then step up big nose ass bitch. saw that shit off.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2983</th>\n","      <td>3184</td>\n","      <td>philippines - christians are facing starvation as they continue to hide from jihadists. an estimate 1500 remain...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1258</th>\n","      <td>1459</td>\n","      <td>thank you so much to and for hosting such an amazing event this past weekend! no child should face immigration court w/o an attorney by their side and this event helped raise critical awareness for these immigrant and refugee children kind la giving kindness</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>229</td>\n","      <td>could you open up your home to refugees in need?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>301</th>\n","      <td>502</td>\n","      <td>it's unfortunate that such racist tactics are used against migrant communities. remember those who are spreading fear are those who cut the funding of vital services for migrants such as healthcare and education! via</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8290</th>\n","      <td>8491</td>\n","      <td>bitch what the fuck do you mean you cant find the fucking page cunt i need to vote for my boys dont you understand</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>901</th>\n","      <td>1102</td>\n","      <td>world refugee day was this past wednesday, but there is still time to make a difference for the 20,000 refugees who will arrive in the u.s. this year. rally the support of your friends and family by becoming a fundraiser for refugees. get started here _x009d_</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2594</th>\n","      <td>2795</td>\n","      <td>us senate calls on julian assange to testify</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7336</th>\n","      <td>7537</td>\n","      <td>when you find a bag of drugs on the ground</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4656</th>\n","      <td>4857</td>\n","      <td>when you're flexin on these bitches</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8770</th>\n","      <td>8971</td>\n","      <td>so now the word hysterical is a completely sexist term? so let's get this straight nobody can say the word monkey anymore when they are describing anyting and the word hysterical can no longer be used to describe a hysterical woman?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3546</th>\n","      <td>3747</td>\n","      <td>video: immigrant activist who climbed statue of liberty has a new song: \"america, you motherfuckers,...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7206</th>\n","      <td>7407</td>\n","      <td>hi face blowing a kiss</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3087</th>\n","      <td>3288</td>\n","      <td>picture: the couple engelbert and spera, both jews. engelbert is now elected in the austrian parliament for the vp. he says in the newspaper haretz the danger for jews doesnt come from fpnazis, more from islamic refugees, which import antisemitism to austria.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1474</th>\n","      <td>1675</td>\n","      <td>* croatia : the croatian authorities, surprisingly, reject the eus bid of 6,000 euros per migrant. _x008f_ v4 visegrad</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        id  ... label\n","479   680   ...  0   \n","3794  3995  ...  1   \n","2512  2713  ...  0   \n","7308  7509  ...  0   \n","6183  6384  ...  1   \n","427   628   ...  0   \n","3040  3241  ...  0   \n","7419  7620  ...  0   \n","4332  4533  ...  0   \n","3554  3755  ...  0   \n","8128  8329  ...  0   \n","6922  7123  ...  0   \n","8554  8755  ...  0   \n","1237  1438  ...  1   \n","7818  8019  ...  0   \n","5119  5320  ...  1   \n","2983  3184  ...  0   \n","1258  1459  ...  0   \n","28    229   ...  0   \n","301   502   ...  0   \n","8290  8491  ...  0   \n","901   1102  ...  0   \n","2594  2795  ...  0   \n","7336  7537  ...  0   \n","4656  4857  ...  0   \n","8770  8971  ...  0   \n","3546  3747  ...  0   \n","7206  7407  ...  0   \n","3087  3288  ...  0   \n","1474  1675  ...  1   \n","\n","[30 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"6uMuDR0pqChY","colab_type":"text"},"source":["Loading in dev data and specifying global variables"]},{"cell_type":"code","metadata":{"id":"IuMOGwFui4it","colab_type":"code","outputId":"3b48f9b8-fdb1-4e07-bee2-8f89d67269a3","executionInfo":{"status":"ok","timestamp":1585599002892,"user_tz":-60,"elapsed":128573,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["dev = loadData(rawDev)\n","\n","DATA_COLUMN = 'tweet'\n","LABEL_COLUMN = 'label'\n","# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = [0, 1]\n","\n","print(\"Size of training data\", len(train.index))\n","print(\"Size of development data\", len(dev.index), '\\n')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Size of training data 9000\n","Size of development data 1000 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DO0dWxHYi_UA","colab_type":"text"},"source":["# Setting Up BERT Training "]},{"cell_type":"markdown","metadata":{"id":"HhIvI0kDXoUq","colab_type":"text"},"source":["##Custom BERT Repositry For Custom Functionality\n","\n","Rather than using the official BERT setup, I have instead forked the BERT rpoe and customised it to allow for a more tailor-made approach when evaluating and fine-tuning the model\n","\n","The`create_model` function in my BERT repo has been edited to allow for multiple <b>Fine-Tuning</b> strategies. Normally, the default function from BERT simply fine-tunes a single layer that will be trained on top of BERT to adapt it to our classification problem. This strategy of using a pre-trained model, then fine-tuning it is called <b>Transfer Learning</b>.\n","\n","Also the `model_fn` method in my BERT repo provides far more detailed metrics than just accuracy and loss - which is all the default repo provides. It has metrics such as F-Score, AUC, precision and recall; so I can better analyse the performance of different models"]},{"cell_type":"code","metadata":{"id":"ACDvt5FYXvvm","colab_type":"code","outputId":"a5e34765-da6c-4dea-e0a0-f0e3548c5900","executionInfo":{"status":"ok","timestamp":1585599010878,"user_tz":-60,"elapsed":136536,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["!git clone https://github.com/fionnmcconville/bert.git\n","%cd bert\n","import run_classifier\n","import optimization\n","import tokenization\n","import modeling\n","#Return to original workspace\n","%cd .."],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'bert'...\n","remote: Enumerating objects: 360, done.\u001b[K\n","Receiving objects:   0% (1/360)   \rReceiving objects:   1% (4/360)   \rReceiving objects:   2% (8/360)   \rReceiving objects:   3% (11/360)   \rReceiving objects:   4% (15/360)   \rReceiving objects:   5% (18/360)   \rReceiving objects:   6% (22/360)   \rReceiving objects:   7% (26/360)   \rReceiving objects:   8% (29/360)   \rReceiving objects:   9% (33/360)   \rReceiving objects:  10% (36/360)   \rReceiving objects:  11% (40/360)   \rReceiving objects:  12% (44/360)   \rReceiving objects:  13% (47/360)   \rReceiving objects:  14% (51/360)   \rReceiving objects:  15% (54/360)   \rReceiving objects:  16% (58/360)   \rReceiving objects:  17% (62/360)   \rReceiving objects:  18% (65/360)   \rReceiving objects:  19% (69/360)   \rReceiving objects:  20% (72/360)   \rReceiving objects:  21% (76/360)   \rReceiving objects:  22% (80/360)   \rReceiving objects:  23% (83/360)   \rReceiving objects:  24% (87/360)   \rReceiving objects:  25% (90/360)   \rReceiving objects:  26% (94/360)   \rReceiving objects:  27% (98/360)   \rReceiving objects:  28% (101/360)   \rReceiving objects:  29% (105/360)   \rReceiving objects:  30% (108/360)   \rReceiving objects:  31% (112/360)   \rReceiving objects:  32% (116/360)   \rReceiving objects:  33% (119/360)   \rReceiving objects:  34% (123/360)   \rReceiving objects:  35% (126/360)   \rReceiving objects:  36% (130/360)   \rReceiving objects:  37% (134/360)   \rReceiving objects:  38% (137/360)   \rReceiving objects:  39% (141/360)   \rReceiving objects:  40% (144/360)   \rReceiving objects:  41% (148/360)   \rReceiving objects:  42% (152/360)   \rReceiving objects:  43% (155/360)   \rReceiving objects:  44% (159/360)   \rremote: Total 360 (delta 0), reused 0 (delta 0), pack-reused 360\u001b[K\n","Receiving objects:  45% (162/360)   \rReceiving objects:  46% (166/360)   \rReceiving objects:  47% (170/360)   \rReceiving objects:  48% (173/360)   \rReceiving objects:  49% (177/360)   \rReceiving objects:  50% (180/360)   \rReceiving objects:  51% (184/360)   \rReceiving objects:  52% (188/360)   \rReceiving objects:  53% (191/360)   \rReceiving objects:  54% (195/360)   \rReceiving objects:  55% (198/360)   \rReceiving objects:  56% (202/360)   \rReceiving objects:  57% (206/360)   \rReceiving objects:  58% (209/360)   \rReceiving objects:  59% (213/360)   \rReceiving objects:  60% (216/360)   \rReceiving objects:  61% (220/360)   \rReceiving objects:  62% (224/360)   \rReceiving objects:  63% (227/360)   \rReceiving objects:  64% (231/360)   \rReceiving objects:  65% (234/360)   \rReceiving objects:  66% (238/360)   \rReceiving objects:  67% (242/360)   \rReceiving objects:  68% (245/360)   \rReceiving objects:  69% (249/360)   \rReceiving objects:  70% (252/360)   \rReceiving objects:  71% (256/360)   \rReceiving objects:  72% (260/360)   \rReceiving objects:  73% (263/360)   \rReceiving objects:  74% (267/360)   \rReceiving objects:  75% (270/360)   \rReceiving objects:  76% (274/360)   \rReceiving objects:  77% (278/360)   \rReceiving objects:  78% (281/360)   \rReceiving objects:  79% (285/360)   \rReceiving objects:  80% (288/360)   \rReceiving objects:  81% (292/360)   \rReceiving objects:  82% (296/360)   \rReceiving objects:  83% (299/360)   \rReceiving objects:  84% (303/360)   \rReceiving objects:  85% (306/360)   \rReceiving objects:  86% (310/360)   \rReceiving objects:  87% (314/360)   \rReceiving objects:  88% (317/360)   \rReceiving objects:  89% (321/360)   \rReceiving objects:  90% (324/360)   \rReceiving objects:  91% (328/360)   \rReceiving objects:  92% (332/360)   \rReceiving objects:  93% (335/360)   \rReceiving objects:  94% (339/360)   \rReceiving objects:  95% (342/360)   \rReceiving objects:  96% (346/360)   \rReceiving objects:  97% (350/360)   \rReceiving objects:  98% (353/360)   \rReceiving objects:  99% (357/360)   \rReceiving objects: 100% (360/360)   \rReceiving objects: 100% (360/360), 304.34 KiB | 3.38 MiB/s, done.\n","Resolving deltas:   0% (0/199)   \rResolving deltas:   3% (7/199)   \rResolving deltas:   4% (8/199)   \rResolving deltas:   8% (16/199)   \rResolving deltas:  17% (35/199)   \rResolving deltas:  18% (37/199)   \rResolving deltas:  19% (38/199)   \rResolving deltas:  21% (42/199)   \rResolving deltas:  22% (44/199)   \rResolving deltas:  26% (52/199)   \rResolving deltas:  27% (54/199)   \rResolving deltas:  31% (62/199)   \rResolving deltas:  32% (64/199)   \rResolving deltas:  37% (74/199)   \rResolving deltas:  42% (84/199)   \rResolving deltas:  43% (86/199)   \rResolving deltas:  45% (91/199)   \rResolving deltas:  50% (101/199)   \rResolving deltas:  81% (163/199)   \rResolving deltas:  92% (185/199)   \rResolving deltas: 100% (199/199)   \rResolving deltas: 100% (199/199), done.\n","/content/bert\n","WARNING:tensorflow:From /content/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V399W0rqNJ-Z","colab_type":"text"},"source":["## BERT Preprocessing and Setup\n","We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n","\n","- `text_a` is the text we want to classify, which in this case, is the `tweet` field in our Dataframe. \n","- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n","- `label` is the label for our example, i.e. HS, Not HS"]},{"cell_type":"code","metadata":{"id":"p9gEt5SmM6i6","colab_type":"code","colab":{}},"source":["# Use the InputExample class from BERT's run_classifier code to create examples from the data\n","train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for book-keeping, unused in this example\n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)\n","\n","dev_InputExamples = dev.apply(lambda x: run_classifier.InputExample(guid=None, \n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCZWZtKxObjh","colab_type":"text"},"source":["Next, we need to preprocess our data so that it matches the data BERT was trained on.\n","\n","\n","1. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n","2. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n","3. Map our words to indexes using a vocab file that BERT provides\n","4. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n","5. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n","\n","Happily, we don't have to worry about most of these details. It's automated with the below inbuilt functions\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Aws4Q_SXZENG","colab_type":"text"},"source":["Below is a way to retrieve desired BERT parameters, such as it's pre-trained checkpoints and it's vocab file, from my google storage bucket where I've downloaded the uncased LARGE version of bert."]},{"cell_type":"code","metadata":{"id":"UtZavIhEaWF5","colab_type":"code","outputId":"e0dd9962-0b4b-4354-d3ed-61540f104cf7","executionInfo":{"status":"ok","timestamp":1585599013907,"user_tz":-60,"elapsed":139528,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":425}},"source":["bucket_dir = 'gs://csc3002'\n","bert_ckpt_dir = os.path.join(bucket_dir, bert_model_name)\n","\n","#For further pretrained model\n","if FURTHER_PRETRAINED:\n","  further_pretrained_model = os.path.join(bert_model_name, 'further_pretrained_model')\n","  further_pretrained_model = os.path.join(bucket_dir, further_pretrained_model)\n","  bert_ckpt_file = tf.train.latest_checkpoint(further_pretrained_model)\n","  print(\"\\nUsing BERT checkpoint from directory:\", os.path.join(further_pretrained_model))\n","\n","else:\n","  bert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n","  print(\"\\nUsing BERT checkpoint from directory:\", bert_ckpt_dir)\n","\n","print(\"\\nBERT checkpoint file is:\", bert_ckpt_file)\n","\n","#Setting up BERT config, vocab file and tokenizer - all default from the BERT repo\n","bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")\n","vocab_file = os.path.join(bert_ckpt_dir, \"vocab1.txt\")\n","  \n","tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file)\n","\n","\n","print(\"\\nMake sure that the function loads a checkpoint, if it doesn't an error will be thrown here\")\n","assert bert_ckpt_file is not None, \"No BERT checkpoint file loaded\"\n","\n","print(\"\\nUsing vocab file:\", vocab_file)\n","print(\"\\nBelow is an example of the BERT tokenizer in action:\")\n","tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Using BERT checkpoint from directory: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model\n","\n","BERT checkpoint file is: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/model.ckpt-80000\n","WARNING:tensorflow:From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","\n","Make sure that the function loads a checkpoint, if it doesn't an error will be thrown here\n","\n","Using vocab file: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/vocab1.txt\n","\n","Below is an example of the BERT tokenizer in action:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['this',\n"," 'here',\n"," \"'\",\n"," 's',\n"," 'an',\n"," 'example',\n"," 'of',\n"," 'using',\n"," 'the',\n"," 'bert',\n"," 'token',\n"," '##izer']"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"0OEzfFIt6GIc","colab_type":"text"},"source":["Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."]},{"cell_type":"code","metadata":{"id":"I_xOUO7T_k50","colab_type":"code","cellView":"form","colab":{}},"source":["# BERT is limited to 512 tokens in length\n","MAX_SEQ_LENGTH = 256 #@param {type:\"slider\", min:128, max:512, step:32}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cm8RLoJ31WLa","outputId":"5ad736f8-0048-4790-c625-2b149ff02405","executionInfo":{"status":"ok","timestamp":1585599016786,"user_tz":-60,"elapsed":142372,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Convert our train and dev features to InputFeatures that BERT understands.\n","train_features =  run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","dev_features = run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /content/bert/run_classifier.py:786: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n","INFO:tensorflow:Writing example 0 of 9000\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] how come allah is not helping you it is up to christian countries to protect you feed you , the countries hit by violence from islam take refugees in feed them etc ##ple ##ase no more explaining about your hard times we are doing our best for u ##yes there is good and bad every where [SEP]\n","INFO:tensorflow:input_ids: 101 2129 2272 16455 2003 2025 5094 2017 2009 2003 2039 2000 3017 3032 2000 4047 2017 5438 2017 1010 1996 3032 2718 2011 4808 2013 7025 2202 8711 1999 5438 2068 4385 10814 11022 2053 2062 9990 2055 2115 2524 2335 2057 2024 2725 2256 2190 2005 1057 23147 2045 2003 2204 1998 2919 2296 2073 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] with today ##s ja ##lal ##abad attack and other vicious attacks claimed by isis , i smell a spill ##over of refugees in pak again . this time we should not open borders for them . we can ##t afford terrorists taking und ##ue advantage . let americans deal with it . # afghanistan ja ##lal ##abad [SEP]\n","INFO:tensorflow:input_ids: 101 2007 2651 2015 14855 13837 10542 2886 1998 2060 13925 4491 3555 2011 18301 1010 1045 5437 1037 14437 7840 1997 8711 1999 22190 2153 1012 2023 2051 2057 2323 2025 2330 6645 2005 2068 1012 2057 2064 2102 8984 15554 2635 6151 5657 5056 1012 2292 4841 3066 2007 2009 1012 1001 7041 14855 13837 10542 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] greatest threat to austrian security , says top military figure . eu and europe bitterly divided ##ma ##jo ##r confrontation ##s between the two . nothing more counter ##pro ##ductive than centers on european territory or euro bribes for migrants . # vis ##eg ##rad v ##4 [SEP]\n","INFO:tensorflow:input_ids: 101 4602 5081 2000 6161 3036 1010 2758 2327 2510 3275 1012 7327 1998 2885 19248 4055 2863 5558 2099 13111 2015 2090 1996 2048 1012 2498 2062 4675 21572 26638 2084 6401 2006 2647 3700 2030 9944 29117 2005 16836 1012 1001 25292 13910 12173 1058 2549 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] when all your friends are out ho ##e ' in and you ' re stuck at home in a shitty relationship [SEP]\n","INFO:tensorflow:input_ids: 101 2043 2035 2115 2814 2024 2041 7570 2063 1005 1999 1998 2017 1005 2128 5881 2012 2188 1999 1037 28543 3276 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] i wonder if rick will make another deal with those crazy ass women thinking face and if that crazy ass ni ##gga will actually ho ##e daryl again neutral face [SEP]\n","INFO:tensorflow:input_ids: 101 1045 4687 2065 6174 2097 2191 2178 3066 2007 2216 4689 4632 2308 3241 2227 1998 2065 2008 4689 4632 9152 23033 2097 2941 7570 2063 22514 2153 8699 2227 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:Writing example 0 of 1000\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] fight russia now you pussy pot ##us . [SEP]\n","INFO:tensorflow:input_ids: 101 2954 3607 2085 2017 22418 8962 2271 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] hai ##nes city , fl ##a wil ##iba ##ld ##o garcia an illegal alien from mexico charged with sexual battery on an 8 y ##r old girl . garcia has an alias lorenzo sip ##rian ##o with prior criminal history . # ms ##ms ##ile ##nce # build ##tha ##t ##wall [SEP]\n","INFO:tensorflow:input_ids: 101 15030 5267 2103 1010 13109 2050 19863 18410 6392 2080 7439 2019 6206 7344 2013 3290 5338 2007 4424 6046 2006 2019 1022 1061 2099 2214 2611 1012 7439 2038 2019 14593 12484 10668 6862 2080 2007 3188 4735 2381 1012 1001 5796 5244 9463 5897 1001 3857 8322 2102 9628 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] study con ##fl ##ants illegal immigrants with legal immigrants to get a low crime number ##r trump mag ##a red nation rising red hen [SEP]\n","INFO:tensorflow:input_ids: 101 2817 9530 10258 11390 6206 7489 2007 3423 7489 2000 2131 1037 2659 4126 2193 2099 8398 23848 2050 2417 3842 4803 2417 21863 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] already won most hysterical woman with her 2 day twitter freak ##out over leg room on a plane . [SEP]\n","INFO:tensorflow:input_ids: 101 2525 2180 2087 25614 2450 2007 2014 1016 2154 10474 11576 5833 2058 4190 2282 2006 1037 4946 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] you ' re just a ska ##nk and everyone knows it otherwise you wouldn ' t be making excuses for not settling down whilst hanging out with multiple different guys . [SEP]\n","INFO:tensorflow:input_ids: 101 2017 1005 2128 2074 1037 24053 8950 1998 3071 4282 2009 4728 2017 2876 1005 1056 2022 2437 21917 2005 2025 9853 2091 5819 5689 2041 2007 3674 2367 4364 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5gdp568cyB3M","colab_type":"text"},"source":["The Run config will be the same across all evaluation options below for running BERT. In it we define the amount of summary steps, as well as how often we should checkpoint the model"]},{"cell_type":"code","metadata":{"id":"1J5F87pQKD3I","colab_type":"code","cellView":"form","colab":{}},"source":["SAVE_CHECKPOINTS_STEPS = 10000 #@param {type:\"number\"}\n","SUMMARY_STEPS = 200 #@param {type:\"number\"}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QPYzNbvcx94h","colab_type":"code","outputId":"5de91f25-5009-4e71-9dba-0fbd910607f9","executionInfo":{"status":"ok","timestamp":1585599016789,"user_tz":-60,"elapsed":142339,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["run_config = tf.compat.v1.estimator.tpu.RunConfig(  \n","    #I think the output file must be a sub-directory of the main BERT file\n","    model_dir=OUTPUT_DIR, \n","    tf_random_seed=SEED,\n","    cluster=cluster_resolver,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=SUMMARY_STEPS,    #Shows us summary metrics every 100 steps\n","        num_shards=8,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","print(run_config.session_config)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.38.114.34:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZY_71Lwmxwm1","colab_type":"text"},"source":["## Fine-Tuning Model"]},{"cell_type":"code","metadata":{"id":"663eGl9xko7b","colab_type":"code","cellView":"both","colab":{}},"source":["#@title Fine-Tuning Options\n","FT_MODEL = \"BiLTSM\" #@param ['Default', 'Multi-Layer Perceptron', 'BiLTSM']\n","LOSS_FN = \"Default\" #@param  ['focal_loss','binary_cross_entropy','kld','squared_hinge','hinge', 'Default']\n","TRAIN_BATCH_SIZE = 32 #@param {type:\"slider\", min:16, max:32, step:16}\n","\n","#Must be set to 8 because on a TPU, model will truncate last few entries if they don't fit in the specified batch size\n","EVAL_BATCH_SIZE = 8 \n","PREDICT_BATCH_SIZE = 8 \n","\n","LEARNING_RATE = 0.00002  #@param {type:\"slider\", min:1e-5, max:5e-5, step:1e-6}\n","NUM_TRAIN_STEPS = 800 #@param {type:\"slider\", min:0, max:10000, step:50}\n","#@markdown The parameters below are not relevant if the FT_MODEL is set to 'Default\n","FT_LAYERS = 2 #@param {type:\"slider\", min:2, max:4, step:1}\n","HIDDEN_SIZE = 256 #@param {type:\"slider\", min:32, max:382, step:4}\n","FT_PARAMS = [FT_MODEL, LOSS_FN, FT_LAYERS, HIDDEN_SIZE]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbTfvqgJPw4n","colab_type":"code","colab":{}},"source":["# Maybe have a params dict with model params\n","def create_model(bert_config, is_training, input_ids, input_mask, segment_ids, \n","                 labels, num_labels, FT_PARAMS):\n","  \n","  \"\"\" Create a classification model based on BERT \"\"\"\n","  model = modeling.BertModel(\n","      config=bert_config,\n","      is_training=is_training,\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      token_type_ids=segment_ids,\n","      use_one_hot_embeddings=True,  # True if use TPU\n","  )\n","\n","  model_type = FT_PARAMS[0]\n","  loss_type = FT_PARAMS[1]\n","  num_extra_layers = FT_PARAMS[2]\n","  h_size = FT_PARAMS[3]\n","\n","\n","  if model_type == \"BiLTSM\":\n","    tf.logging.info(\"Using Bi-Directional LTSM for Fine-Tuning. %d extra layer\" % (num_extra_layers)) \n","    #output layer must be rank 3 for biltsm\n","    output_layer = model.get_sequence_output()\n","    #Output shape of 4, 246, 1024 which is [batch_size, seq_len, input_size]\n","    #seq len parameter corresponds to max_time param in bi_dynamic_rnn function\n","\n","    for layer in range(num_extra_layers):\n","\n","      #Using different variable scopes, if you want though you can just make the name\n","      #\"hidden\" each time and the weights will be shared across layers\n","      with tf.variable_scope('hidden_{}'.format(layer),reuse=tf.AUTO_REUSE):\n","        \n","        #num units in LTSMCell for fw and bw must match\n","        cell_fw = tf.nn.rnn_cell.LSTMCell(num_units=h_size)\n","        cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = 0.9)\n","\n","        cell_bw = tf.nn.rnn_cell.LSTMCell(num_units=h_size)\n","        cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = 0.9)\n","        outputs, states = tf.nn.bidirectional_dynamic_rnn(\n","            cell_fw=cell_fw, cell_bw=cell_bw, inputs=output_layer, dtype=tf.float32)\n","\n","        output_layer = tf.concat(outputs,2) #Could be this too\n","\n","    output_layer = output_layer[:,-1,:] #Flatten output logits to [batch_size, hidden_size]\n","\n","  elif model_type == \"MLP\":\n","    tf.logging.info(\"Using Multi-Layer Perceptron for Fine-Tuning. %d extra layer\" % (num_extra_layers))  \n","    final_hidden = model.get_pooled_output()\n","    final_hidden_shape = modeling.get_shape_list(final_hidden, expected_rank=2)\n","    batch_size = final_hidden_shape[0]\n","    hidden_size = final_hidden_shape[1]\n","    \n","    for layer in range(num_extra_layers):\n","      with tf.variable_scope('hidden_{}'.format(layer),reuse=tf.AUTO_REUSE):\n","        h_weights = tf.get_variable(\"h{}_weights\".format(layer), [h_size, hidden_size],\n","                                      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","        h_bias = tf.get_variable(\"h{}_bias\".format(layer), [h_size], initializer=tf.zeros_initializer())\n","\n","        #final_hidden_matrix = tf.reshape(final_hidden, [batch_size * seq_length, hidden_size])\n","        h_logits = tf.nn.bias_add(tf.matmul(final_hidden, h_weights, transpose_b=True), h_bias)\n","        h_logits = tf.nn.relu(h_logits)\n","        #Dropout after activation\n","        if is_training:       \n","          h_logits = tf.nn.dropout(h_logits, rate=0.1) #not sure if this is needed\n","        \n","        #Reset values to reflect current last layer\n","        final_hidden = h_logits\n","        hidden_size = h_logits.shape[-1].value\n","    \n","    output_layer = final_hidden # Output layer for MLP\n","\n","  # Revert to default BERT Fine-Tuning\n","  else:\n","    tf.logging.info(\"\\nUsing original BERT Model for Fine-Tuning\\n\") \n","    output_layer = model.get_pooled_output() #Output layer for default BERT\n","\n","  hidden_size = output_layer.shape[-1].value\n","\n","  output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02)) \n","\n","  output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","  with tf.variable_scope(\"output\"):\n","    if is_training and model_type != \"BiLTSM\": #We already do dropout in BiLTSM loop\n","      # I.e., 0.1 dropout\n","      output_layer = tf.nn.dropout(output_layer, rate=0.1) \n","\n","    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, output_bias)\n","    probabilities = tf.nn.softmax(logits, axis=-1 )\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32 )\n","\n","    if loss_type == 'focal_loss':\n","      # Focal loss (Set default focal loss gamma to 2)\n","      per_example_loss = -one_hot_labels * ((1 - probabilities) ** 2) * log_probs\n","      per_example_loss = tf.reduce_sum(per_example_loss, axis=1)\n","\n","    elif loss_type == 'binary_cross_entropy':\n","        per_example_loss = tf.keras.losses.binary_crossentropy(y_true=one_hot_labels,\n","                                                                          y_pred=probabilities)\n","    elif loss_type == 'kld':\n","        per_example_loss = tf.keras.metrics.KLDivergence(y_true=one_hot_labels,\n","                                                    y_pred=probabilities)\n","    elif loss_type == 'squared_hinge':\n","        per_example_loss = tf.keras.losses.squared_hinge(y_true=one_hot_labels,\n","                                                              y_pred=probabilities)\n","    elif loss_type == 'hinge':\n","        per_example_loss = tf.keras.metrics.hinge(y_true=one_hot_labels,\n","                                                      y_pred=probabilities)\n","    else:   # Fallback to cross-entropy\n","        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","\n","    loss = tf.reduce_mean(per_example_loss)\n","  return loss, per_example_loss, log_probs, probabilities"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oj5kzTOxLJhb","colab_type":"code","colab":{}},"source":["def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n","                     num_train_steps, num_warmup_steps, use_tpu, ft_params):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","\n","  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    tf.logging.info(\"*** Features ***\")\n","    for name in sorted(features.keys()):\n","      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n","\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","    is_real_example = None\n","    if \"is_real_example\" in features:\n","      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n","    else:\n","      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n","\n","    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n","\n","    loss_types = ['focal_loss','binary_cross_entropy','kld','squared_hinge','hinge', 'cross-entropy']\n","    model_types = ['Default', 'Multi-Layer Perceptron', 'BiLTSM']\n","    loss_type = ft_params[1]\n","    tf.logging.info(\"\\nUsing loss type:%s\" % (loss_type)) \n","\n","    (total_loss, per_example_loss, log_probs, probabilities) = create_model(\n","        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids, num_labels,\n","        FT_PARAMS)\n","    \n","    tvars = tf.trainable_variables()\n","    initialized_variable_names = {}\n","    scaffold_fn = None\n","    if init_checkpoint:\n","      (assignment_map, initialized_variable_names\n","      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n","      if use_tpu:\n","\n","        def tpu_scaffold():\n","          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n","          return tf.train.Scaffold()\n","\n","        scaffold_fn = tpu_scaffold\n","      else:\n","        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n","\n","    output_spec = None\n","    if mode == tf.estimator.ModeKeys.TRAIN:\n","\n","      train_op = optimization.create_optimizer(\n","          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n","\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          loss=total_loss,\n","          train_op=train_op,\n","          scaffold_fn=scaffold_fn)\n","    elif mode == tf.estimator.ModeKeys.EVAL:\n","\n","      def metric_fn(per_example_loss, label_ids, log_probs, is_real_example):\n","        predictions = tf.argmax(log_probs, axis=-1, output_type=tf.int32)\n","\n","        accuracy = tf.compat.v1.metrics.accuracy(labels=label_ids, predictions=predictions, weights=is_real_example)\n","        loss = tf.compat.v1.metrics.mean(values=per_example_loss, weights=is_real_example)\n","        f1_score = tf.contrib.metrics.f1_score(label_ids, predictions)\n","        auc = tf.compat.v1.metrics.auc(label_ids, predictions)\n","        recall = tf.compat.v1.metrics.recall(label_ids, predictions)\n","        precision = tf.compat.v1.metrics.precision(label_ids, predictions)\n","        true_pos = tf.compat.v1.metrics.true_positives(label_ids, predictions)\n","        true_neg = tf.compat.v1.metrics.true_negatives(label_ids, predictions)\n","        false_pos = tf.compat.v1.metrics.false_positives(label_ids, predictions)  \n","        false_neg = tf.compat.v1.metrics.false_negatives(label_ids, predictions)\n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"eval_loss\": loss,\n","            \"F1_Score\": f1_score,\n","            \"auc\": auc,\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"true_positives\": true_pos,\n","            \"true_negatives\": true_neg,\n","            \"false_positives\": false_pos,\n","            \"false_negatives\": false_neg\n","        }\n","\n","      eval_metrics = (metric_fn, [per_example_loss, label_ids, log_probs, is_real_example])\n","\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          loss=total_loss,\n","          eval_metrics=eval_metrics,\n","          scaffold_fn=scaffold_fn)\n","    else:\n","      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n","          mode=mode,\n","          predictions={\"probabilities\": probabilities},\n","          scaffold_fn=scaffold_fn)\n","    return output_spec\n","\n","  return model_fn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjwJ4bTeWXD8","colab_type":"code","outputId":"1c5acec8-2b1e-4b12-f9a4-a0d61abd0f44","executionInfo":{"status":"ok","timestamp":1585599018420,"user_tz":-60,"elapsed":143924,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["# Warmup is a period of time where the learning rate \n","#is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","num_warmup_steps = int(NUM_TRAIN_STEPS * WARMUP_PROPORTION)\n","\n","print(\"The model will stop training when it reaches\", NUM_TRAIN_STEPS, \"as a checkpoint\")\n","print(\"\\nThe bert checkpoint directory is\", bert_ckpt_dir)\n","print(\"\\nThe output directory is\", OUTPUT_DIR, '\\n')\n","\n","#This is the model function, which feeds in the bert configurations, the pretrained model itself and the parameters for the fine tuning of the model\n","\"\"\"model_fn = run_classifier.model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=NUM_TRAIN_STEPS,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  )\"\"\"\n","\n","model_fn = model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=NUM_TRAIN_STEPS,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  ft_params = FT_PARAMS)\n","\n","\n","#We use Tensorflow estimators to train, evaluate and test our model\n","estimator = tf.contrib.tpu.TPUEstimator(\n","    use_tpu=True,\n","    model_fn=model_fn,\n","    config=run_config,\n","    train_batch_size=TRAIN_BATCH_SIZE,\n","    eval_batch_size=EVAL_BATCH_SIZE,\n","    predict_batch_size=PREDICT_BATCH_SIZE)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The model will stop training when it reaches 800 as a checkpoint\n","\n","The bert checkpoint directory is gs://csc3002/wwm_uncased_L-24_H-1024_A-16\n","\n","The output directory is gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output1 \n","\n","WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f5288fb3f28>) includes params argument, but params are not passed to Estimator.\n","INFO:tensorflow:Using config: {'_model_dir': 'gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output1', '_tf_random_seed': 3060, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.38.114.34:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f52836eb6a0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.38.114.34:8470', '_evaluation_master': 'grpc://10.38.114.34:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=200, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f5295165ac8>}\n","INFO:tensorflow:_TPUContext: eval_on_tpu True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7siI74tc27eN","colab_type":"text"},"source":["Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator.\n","\n","This is a pretty standard design pattern for working with Tensorflow Estimators"]},{"cell_type":"code","metadata":{"id":"JUu_zpYV25z5","colab_type":"code","colab":{}},"source":["# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=True,)\n","\n","# Input function for dev data, we feed in our previously created dev_features for this\n","test_input_fn = run_classifier.input_fn_builder(\n","    features=dev_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qp8UD9h13SvX","colab_type":"text"},"source":["# Train Model\n","<b>Now we train our BERT fine-tuned model"]},{"cell_type":"code","metadata":{"id":"vkNOJkVJ3SUh","colab_type":"code","outputId":"68fc0c48-4f6c-49c6-a15a-86bdba337a9a","executionInfo":{"status":"error","timestamp":1585316973597,"user_tz":0,"elapsed":179410,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["print(\"\\nThe model will stop training when it reaches\", num_train_steps, \"as a checkpoint\")\n","print(f'Beginning Training!')\n","current_time = datetime.now()\n","estimator.train(input_fn=train_input_fn, max_steps=NUM_TRAIN_STEPS)\n","train_time = datetime.now() - current_time\n","print(\"Training took time \", train_time)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","The model will stop training when it reaches 843 as a checkpoint\n","Beginning Training!\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.46.146.242:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14212402838772463947)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 2645688729414921122)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4430974932734288524)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 1719957632343019041)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 4078087536351862910)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 10893043772854973746)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5317000536736360056)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 401364738684342591)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6514690444497763496)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 12329345700720922195)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7792667053053043885)\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","\n","Using loss type: binary_cross_entropy\n","WARNING:tensorflow:From /content/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","WARNING:tensorflow:From /content/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n","\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt.\n","INFO:tensorflow:training_loop marked as finished\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-9c276deea000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Beginning Training!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcurrent_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training took time \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m   3028\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m           \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m           saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m   3031\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_loop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1159\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1193\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1194\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_creation_timeout_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m         log_step_count_steps=log_step_count_steps) as mon_sess:\n\u001b[0m\u001b[1;32m   1491\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mMonitoredTrainingSession\u001b[0;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\u001b[0m\n\u001b[1;32m    582\u001b[0m       \u001b[0msession_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m       stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0mshould_recover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    723\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess_creator)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \"\"\"\n\u001b[1;32m   1206\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m     \u001b[0m_WrappedSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36m_create_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         logging.info(\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m       \u001b[0;31m# Inform the hooks that a new session has been created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m       return _CoordinatedSession(\n\u001b[1;32m    887\u001b[0m           \u001b[0m_HookedSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36mafter_create_session\u001b[0;34m(self, session, coord)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;31m# The checkpoint saved here is the state at step \"global_step\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_last_triggered_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/basic_session_run_hooks.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self, session, step)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_saver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     self._summary_writer.add_session_log(\n\u001b[1;32m    613\u001b[0m         SessionLog(\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1174\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1175\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"T_MPSFxy3gNG","colab_type":"text"},"source":["# Evaluate Model\n","\n","We now evaluate the performance of our model on the development data"]},{"cell_type":"code","metadata":{"id":"uPLbCsbK3zHW","colab_type":"code","outputId":"62b64d96-0e4b-4fac-a63b-51cc71a92f23","executionInfo":{"status":"ok","timestamp":1585151407395,"user_tz":0,"elapsed":119291,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#You need to provide number of steps for a TPU\n","eval_steps = int(len(dev_features) / EVAL_BATCH_SIZE)\n","\n","#Eval will be slightly WRONG on the TPU because it will drop the last batch (drop_remainder = True).\n","estimator.evaluate(input_fn=test_input_fn, steps=eval_steps)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Querying Tensorflow master (grpc://10.47.247.18:8470) for TPU system metadata.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Querying Tensorflow master (grpc://10.47.247.18:8470) for TPU system metadata.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 16348832557802236257)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 16348832557802236257)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9442498558503587591)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9442498558503587591)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1819677390197866551)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1819677390197866551)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9397544896479164317)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9397544896479164317)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 2302517781714340699)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 2302517781714340699)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 11410651471969234772)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 11410651471969234772)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15075635063155519615)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15075635063155519615)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 14212200204431715904)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 14212200204431715904)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5287456729480160148)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5287456729480160148)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 3443115717158911620)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 3443115717158911620)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 13313383819007549821)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 13313383819007549821)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Features ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Features ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:  name = input_ids, shape = (1, 256)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:  name = input_ids, shape = (1, 256)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:  name = input_mask, shape = (1, 256)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:  name = input_mask, shape = (1, 256)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:  name = label_ids, shape = (1,)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:  name = label_ids, shape = (1,)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:  name = segment_ids, shape = (1, 256)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:  name = segment_ids, shape = (1, 256)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2020-03-25T15:48:19Z\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2020-03-25T15:48:19Z\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:TPU job name worker\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:TPU job name worker\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt-843\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt-843\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Init TPU system\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Init TPU system\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Initialized TPU in 9 seconds\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Initialized TPU in 9 seconds\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Starting infeed thread controller.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Starting infeed thread controller.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Starting outfeed thread controller.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Starting outfeed thread controller.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Initialized dataset iterators in 0 seconds\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Initialized dataset iterators in 0 seconds\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Enqueue next (125) batch(es) of data to infeed.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Enqueue next (125) batch(es) of data to infeed.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Dequeue next (125) batch(es) of data from outfeed.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Dequeue next (125) batch(es) of data from outfeed.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Outfeed finished for iteration (0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Outfeed finished for iteration (0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [125/125]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [125/125]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Stop infeed thread controller\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Stop infeed thread controller\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Shutting down InfeedController thread.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Shutting down InfeedController thread.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:InfeedController received shutdown signal, stopping.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:InfeedController received shutdown signal, stopping.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Infeed thread finished, shutting down.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Infeed thread finished, shutting down.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:infeed marked as finished\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:infeed marked as finished\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Stop output thread controller\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Stop output thread controller\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Shutting down OutfeedController thread.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Shutting down OutfeedController thread.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Outfeed thread finished, shutting down.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Outfeed thread finished, shutting down.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:outfeed marked as finished\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:outfeed marked as finished\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Shutdown TPU system.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Shutdown TPU system.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2020-03-25-15:49:59\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2020-03-25-15:49:59\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 843: F1_Score = 0.7973856, auc = 0.8194964, eval_accuracy = 0.814, eval_loss = 0.86818445, false_negatives = 61.0, false_positives = 125.0, global_step = 843, loss = 0.5969863, precision = 0.74541754, recall = 0.85714287, true_negatives = 448.0, true_positives = 366.0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 843: F1_Score = 0.7973856, auc = 0.8194964, eval_accuracy = 0.814, eval_loss = 0.86818445, false_negatives = 61.0, false_positives = 125.0, global_step = 843, loss = 0.5969863, precision = 0.74541754, recall = 0.85714287, true_negatives = 448.0, true_positives = 366.0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 843: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt-843\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 843: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt-843\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:evaluation_loop marked as finished\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:evaluation_loop marked as finished\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'F1_Score': 0.7973856,\n"," 'auc': 0.8194964,\n"," 'eval_accuracy': 0.814,\n"," 'eval_loss': 0.86818445,\n"," 'false_negatives': 61.0,\n"," 'false_positives': 125.0,\n"," 'global_step': 843,\n"," 'loss': 0.5969863,\n"," 'precision': 0.74541754,\n"," 'recall': 0.85714287,\n"," 'true_negatives': 448.0,\n"," 'true_positives': 366.0}"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"markdown","metadata":{"id":"SHqEs8G6C8X7","colab_type":"text"},"source":["### Running Evaluation Whilst Training\n","\n","Below is a custom function to run evaluation on the fine-tuned BERT model whilst training. The `tf.train_and_evaluate` inbuilt function for tensorflow doesn't work as it hasn't been made compatible with a distributed TPU strategy. \n","\n","Thus, instead of being able to evaluate the model in memory, we must save the model graph and metadata to a checkpoint and reload it every n steps we want to run an evaluation.\n","\n","The function finds the optimum number of steps the fine-tuning should run for based upon F1 Score by testing the trained model to that point against the dev set\n"]},{"cell_type":"code","metadata":{"id":"fQsD0qy6DE5U","colab_type":"code","colab":{}},"source":["#We'll set a large value for train steps because we want to make this model run\n","#for as long as possible before it finds the optimimum model\n","hparams = {'train_steps': 3000, \n","            'train_batch_size': 32,\n","            'eval_batch_size': 8,\n","            'use_tpu': True,\n","            'num_train_features': len(train_features),\n","            'num_eval_features': len(dev_features),\n","           'learning_rate': 2e-5 \n","            }\n","if DATASET == 'AnalyticsVidhya':\n","  hparams['train_steps'] = 12000\n","          \n","def load_global_step_from_checkpoint_dir(checkpoint_dir):\n","  try:\n","    checkpoint_reader = tf.train.NewCheckpointReader(\n","        tf.train.latest_checkpoint(checkpoint_dir))\n","    return checkpoint_reader.get_tensor(tf.GraphKeys.GLOBAL_STEP)\n","  except:  \n","    return 0\n","\n","def train_and_evaluate(out_dir, hparams, steps_per_eval):\n","\n","#Delete prior model graph, checkpoints and eval files to enable consecutive runs, rather than resetting runtime\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","  # Doesn't matter if the directory didn't exist\n","    pass\n","  tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","  max_steps = hparams['train_steps']\n","  train_batch_size = hparams['train_batch_size']\n","  eval_batch_size = hparams['eval_batch_size']\n","  print('\\ntrain_batch_size={:d}  eval_batch_size={:d}  max_steps={:d}'.format(\n","                  train_batch_size,\n","                  eval_batch_size,\n","                  max_steps))\n","\n","  config = tf.contrib.tpu.RunConfig(\n","    cluster=cluster_resolver,\n","    model_dir=out_dir,\n","    save_checkpoints_steps=steps_per_eval,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","      iterations_per_loop=steps_per_eval,\n","      per_host_input_for_training=True))\n","\n","  model_fn = run_classifier.model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=hparams['learning_rate'],\n","  num_train_steps=NUM_TRAIN_STEPS,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  use_one_hot_embeddings=True)\n","\n","  estimator = tf.contrib.tpu.TPUEstimator(  # TPU change 4\n","    model_fn=model_fn,\n","    config=config,\n","    params=hparams,\n","    model_dir=out_dir,\n","    train_batch_size=train_batch_size,\n","    eval_batch_size=eval_batch_size,\n","    use_tpu=True\n","  )\n"," # load last checkpoint and start from there\n","  current_step = load_global_step_from_checkpoint_dir(out_dir)\n","  steps_per_epoch = hparams['num_train_features'] // train_batch_size\n","  print('\\nTraining for {:d} steps ({:2f} epochs in total). Current'\n","                  ' step {:d}.'.format(\n","                  max_steps,\n","                  max_steps / steps_per_epoch,\n","                  current_step))\n","\n","  start_timestamp = time.time()  # This time will include compilation time\n","  best_score = 0\n","  best_model = 0\n","  while current_step < max_steps:\n","    # Train for up to steps_per_eval number of steps.\n","    # At the end of training, a checkpoint will be written to --model_dir.\n","    next_checkpoint = min(current_step + steps_per_eval, max_steps)\n","    estimator.train(input_fn=train_input_fn, max_steps=next_checkpoint)\n","    current_step = next_checkpoint\n","    print('\\nFinished training up to step {:d}. Elapsed seconds {:d}.\\n'.format(\n","                    next_checkpoint, int(time.time() - start_timestamp)))\n","\n","    print('\\nStarting to evaluate at step {:d} \\n'.format(next_checkpoint))\n","    eval_results = estimator.evaluate(\n","      input_fn=test_input_fn,\n","      steps=hparams['num_eval_features'] // eval_batch_size)\n","    print('\\nEval results at step {:d}: \\n'.format(next_checkpoint), eval_results)\n","    \n","    current_score = eval_results['F1_Score']\n","    if current_score > best_score:\n","      best_score = current_score \n","      best_model = current_step\n","      score_buffer = [] #Reset buffer\n","    else:\n","      score_buffer.append(current_score)\n","    #If 3 times in a row evaluation results haven't improved; we stop training\n","    if len(score_buffer) == 3:\n","      elapsed_time = int(time.time() - start_timestamp)\n","      \n","      print('\\nFinished training at step {:d} as there has been no improvement on the previous 3 iterations'.format(current_step),\n","      '\\nElapsed seconds {:d}. \\n'.format(elapsed_time), \n","      \"\\nBest model is at step {:d} with the best F-score {:d}\".format(best_model, best_score),\n","      \"\\nNow edit the protocol buffer file and set the most recent step to\", best_model,\n","            \"so this model checkpoint can be loaded using the tf.train.latest_checkpoint function\")\n","      \n","      return best_model\n","    \n","\n","  elapsed_time = int(time.time() - start_timestamp)\n","  print('\\nFinished training up to step {:d}. Elapsed seconds {:d}. \\n'.format(max_steps, elapsed_time))\n","  return best_model\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"keoF6rLIDqzl","colab_type":"text"},"source":["Now run the train_and_evaluate function. We can toggle the steps_pereval in the params to control how often we checkpoint and evaluate"]},{"cell_type":"code","metadata":{"id":"8YHGavBJDqhY","colab_type":"code","outputId":"a28f7513-7b48-4fe1-ca21-3ad1ce14d529","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["best_step = train_and_evaluate(OUTPUT_DIR, hparams, steps_per_eval=1000) # Will return the optimum step for the BERT model\n","print(\"\\nBest step for model is at\", best_step)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","train_batch_size=32  eval_batch_size=8  max_steps=12000\n","INFO:tensorflow:Using config: {'_model_dir': 'gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.103.143.2:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8a7f9e16a0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.103.143.2:8470', '_evaluation_master': 'grpc://10.103.143.2:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f8a91f59278>}\n","INFO:tensorflow:_TPUContext: eval_on_tpu True\n","\n","Training for 12000 steps (12.072435 epochs in total). Current step 0.\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.103.143.2:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6950939721587658405)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 7214882111008902385)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11723597538653886847)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10168099005517632449)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11977576496132178308)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 261289063090977959)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6461506172735637731)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 18118260143106526205)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 14835276859969354722)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 8603088287462879159)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 10377542404254460937)\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output/model.ckpt.\n","INFO:tensorflow:Initialized dataset iterators in 3 seconds\n","INFO:tensorflow:Installing graceful shutdown hook.\n","INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 8 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VaGhttd75vOv","colab_type":"text"},"source":["### Cross Validation Evaluation\n","\n","Does not provide in depth tensorflow logging but it does provide evaluation at the end. As mentioned above, we combine the provided training and dev files\n","\n"]},{"cell_type":"code","metadata":{"id":"Hvxh57Zh5zto","colab_type":"code","colab":{}},"source":["def bertCV(data, train_batch_size = 32, learn_rate = 2e-5,\\\n","           num_train_steps = 850, folds = 5):\n","\n","  #Filter out all log messages so console isn't consumed with memory\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","  #Dataframe where grid search results will be stored. Empty to begin with\n","  eval_df = pd.DataFrame(columns = ['F1 Score', 'auc', 'Accuracy'] )\n","  \n","  k = 1 # Fold counter\n","\n","  #Stratified K fold ensures the folds are made by preserving the percentage of samples for each class.\n","  cv = StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n","\n","  # Sticking within the training dataset for evaluation. Data is the combination of the provided train and dev sets\n","  for train_index, dev_index in cv.split(data.tweet, data.label): \n","    \n","    #Shuffling again because otherwise the StratifiedKFold function groups a lot of 0's at the start\n","    training  = data.iloc[train_index]\n","    training = training.sample(frac = 1, random_state=SEED)\n","    develop = data.iloc[dev_index]\n","    develop = develop.sample(frac = 1, random_state=SEED)\n","    \n","    \"\"\"Unlike before where I only one test set and one training set, this time I have K different sets of training and testing.\n","    Therefore, in each fold I need to get a new set of data and convert it to features each time.\"\"\"\n","    \n","    # Use the InputExample class from BERT's run_classifier code to create examples from the data\n","\n","    train_InputExamples = training.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                      text_a = x[DATA_COLUMN], \n","                                                                      text_b = None, \n","                                                                      label = x[LABEL_COLUMN]), axis = 1)\n","\n","    dev_InputExamples = develop.apply(lambda x: run_classifier.InputExample(guid=None, \n","                                                                      text_a = x[DATA_COLUMN], \n","                                                                      text_b = None, \n","                                                                      label = x[LABEL_COLUMN]), axis = 1)\n","    \n","    #Convert these examples to features that BERT can interpret\n","    train_features =  run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","    dev_features = run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","\n","    #Delete prior model graph, checkpoints and eval files to make room for new model each loop\n","    try:\n","      tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","    except:\n","    # Doesn't matter if the directory didn't exist\n","      pass\n","    tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","    num_warmup_steps = int(NUM_TRAIN_STEPS * WARMUP_PROPORTION)\n","\n","    # Model configs\n","    \"\"\"\n","    model_fn = run_classifier.model_fn_builder(\n","    bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","    num_labels=len(label_list),\n","    init_checkpoint=bert_ckpt_file,\n","    learning_rate=LEARNING_RATE,\n","    num_train_steps=num_train_steps,\n","    num_warmup_steps=num_warmup_steps,\n","    use_tpu=True,\n","    use_one_hot_embeddings=True)\n","    \"\"\"\n","\n","    model_fn = model_fn_builder(\n","    bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","    num_labels=len(label_list),\n","    init_checkpoint=bert_ckpt_file,\n","    learning_rate=LEARNING_RATE,\n","    num_train_steps=NUM_TRAIN_STEPS,\n","    num_warmup_steps=num_warmup_steps,\n","    use_tpu=True,\n","    ft_params = FT_PARAMS)\n","    \n","    estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","      use_tpu=True,\n","      model_fn=model_fn,\n","      config=run_config,\n","      train_batch_size=TRAIN_BATCH_SIZE,\n","      eval_batch_size=EVAL_BATCH_SIZE,\n","      predict_batch_size=PREDICT_BATCH_SIZE)\n","    \n","    # Create an input function for training. drop_remainder = True for using TPUs.\n","    train_input_fn = run_classifier.input_fn_builder(\n","        features=train_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=True,\n","        drop_remainder=True)\n","\n","    #input function for dev data, we feed in our previously created dev_features for this\n","    dev_input_fn = run_classifier.input_fn_builder(\n","        features=dev_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=False,\n","        drop_remainder=True)\n","   \n","    \n","    current_time = datetime.now()\n","    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps )\n","    train_time = datetime.now() - current_time\n","    \n","\n","    #You need to provide number of steps for a TPU\n","    eval_steps = int(len(dev_InputExamples) / EVAL_BATCH_SIZE)\n","\n","    #Eval may be slightly WRONG on the TPU because it will truncate the last batch.\n","    eval_results = estimator.evaluate(input_fn=dev_input_fn, steps=eval_steps)\n","\n","    row = pd.Series({'F1 Score': eval_results['F1_Score'], 'auc': eval_results['auc'], 'Accuracy': eval_results['eval_accuracy'],'Precision': eval_results['precision'],'Recall': eval_results['recall'],\\\n","                                    'False Negatives': eval_results['false_negatives'],'False Positives': eval_results['false_positives'],\\\n","                    'True Negatives':eval_results['true_negatives'] ,'True Positives': eval_results['true_positives'], 'Training Time': train_time })\n","    #row = get_metrics(OUTPUT_DIR, train_time, k)\n","    row = pd.Series(row, name = 'Fold ' + str(k))\n","\n","    \"\"\"Below statement controls for whenever we get a bad fold which results in a model predicting only one class.\n","    This isn't truly representative of normal performance and can bring down CV score, so we omit model evaluation\n","    if the below statement is true\"\"\"\n","    if eval_results['false_negatives'] < 1 or eval_results['false_positives'] < 1: \n","      print(\"Classifier predicts one class. Thus not recording this metric as it will skew CV\\n\")\n","      #k = k + 1\n","      continue\n","\n","    eval_df = eval_df.append(row)\n","    print(\"Fold \" + str(k) + \":\\tF-Score:\", eval_df[\"F1 Score\"][k-1])\n","    print(\"Training took time \", train_time)\n","    print('---------------------------------------------------------------------------------------------------------\\n')\n","    k = k + 1 #Increment on fold counter\n","\n","  row = eval_df.mean(axis = 0)\n","  row = pd.Series(row, name = 'CV Average')\n","  eval_df = eval_df.append(row)\n","  print(\"\\nTraining Batch Size: \", train_batch_size, \"\\tLearn Rate: \", learn_rate, \"\\tNumSteps: \", NUM_TRAIN_STEPS)\n","  display(eval_df)\n","\n","  return row # Also return row of CV-Average"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l9Y4Ql38-YQe","colab_type":"text"},"source":["\n","Basic cross-validation can be performed here"]},{"cell_type":"code","metadata":{"id":"h1ctK3c4LXVf","colab_type":"code","outputId":"bdfe0f4c-9f17-414f-89df-7ba94d0988d7","executionInfo":{"status":"error","timestamp":1583231842508,"user_tz":0,"elapsed":1136,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":231}},"source":["data = loadData(rawTrain, rawDev, params)\n","\n","CV_Av = bertCV(data, learn_rate = 2e-5, num_train_steps = 850)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-738bb652a453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m data = loadData(train, dev, replaceEmoji = False, segmentHashtag = True,\n\u001b[0;32m----> 5\u001b[0;31m                 remove_stop = False, lemmatize= False, remove_punctuation = False)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mCV_Av\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbertCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: loadData() got an unexpected keyword argument 'replaceEmoji'"]}]},{"cell_type":"markdown","metadata":{"id":"TFVD9MdCM5nH","colab_type":"text"},"source":["#### Cross-Validation of Cross-Validation\n","\n","Tensorflow 1.x is non-deterministic, which has resulted in the variability between each run to be greater than the difference in performance gained between introductions of different configurations and parameters. This makes it difficult to determine what is the best pre-training, text preprocessing and fine-tuning pipeline to undertake.\n","\n","To better ensure the reliability of experiments my solution is to have a 5 fold cross-validation of a cross-validated sample of my data which should reduce the variance run to run significantly."]},{"cell_type":"code","metadata":{"id":"V_CA6NTiM4wQ","colab_type":"code","outputId":"114df8ba-cf09-41da-f04f-8da613a7dcb3","executionInfo":{"status":"ok","timestamp":1584922836570,"user_tz":0,"elapsed":12166721,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["data = loadData(rawTrain, rawDev)\n","\n","#Stratified K fold ensures the folds are made by preserving the percentage of samples for each class.\n","folds = 5\n","cv = StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n","eval_df = pd.DataFrame(columns = ['F1 Score', 'auc', 'Accuracy'])\n","\n","#This will be a 5-fold CV so the sample each time will be a fifth of the data\n","i = 1\n","for __, data_index in cv.split(data.tweet, data.label):\n","  dat = data.iloc[data_index]\n","  CV_Av = bertCV(dat, learn_rate = 2e-5, num_train_steps=900)\n","  CV_Av = pd.Series(CV_Av, name = 'CV Average' + str(i))\n","  eval_df = eval_df.append(CV_Av)\n","\n","row = eval_df.mean(axis = 0)\n","row = pd.Series(row, name = 'BiLTSM')\n","eval_df1 = pd.read_csv('gs://csc3002/hateval2019/models_eval_df.csv', sep=',',  index_col = 0, encoding = 'utf-8')\n","\n","eval_df1 = eval_df1.append(row)\n","eval_df1.to_csv('gs://csc3002/hateval2019/models_eval_df.csv', sep=',',  index = True, encoding = 'utf-8')\n","eval_df1\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Fold 1:\tF-Score: 0.7241378426551819\n","Training took time  0:12:03.859953\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 2:\tF-Score: 0.7256637215614319\n","Training took time  0:09:59.304479\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 3:\tF-Score: 0.7317072153091431\n","Training took time  0:10:04.926114\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 4:\tF-Score: 0.7586206197738647\n","Training took time  0:10:01.333893\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 5:\tF-Score: 0.8011527061462402\n","Training took time  0:10:15.034749\n","---------------------------------------------------------------------------------------------------------\n","\n","\n","Training Batch Size:  32 \tLearn Rate:  2e-05 \tNumSteps:  800\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1 Score</th>\n","      <th>auc</th>\n","      <th>Accuracy</th>\n","      <th>False Negatives</th>\n","      <th>False Positives</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Training Time</th>\n","      <th>True Negatives</th>\n","      <th>True Positives</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Fold 1</th>\n","      <td>0.724138</td>\n","      <td>0.758621</td>\n","      <td>0.7600</td>\n","      <td>42.0</td>\n","      <td>54.0</td>\n","      <td>0.700000</td>\n","      <td>0.750000</td>\n","      <td>00:12:03.859953</td>\n","      <td>178.0</td>\n","      <td>126.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 2</th>\n","      <td>0.725664</td>\n","      <td>0.762623</td>\n","      <td>0.7675</td>\n","      <td>45.0</td>\n","      <td>48.0</td>\n","      <td>0.719298</td>\n","      <td>0.732143</td>\n","      <td>00:09:59.304479</td>\n","      <td>184.0</td>\n","      <td>123.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 3</th>\n","      <td>0.731707</td>\n","      <td>0.770936</td>\n","      <td>0.7800</td>\n","      <td>48.0</td>\n","      <td>40.0</td>\n","      <td>0.750000</td>\n","      <td>0.714286</td>\n","      <td>00:10:04.926114</td>\n","      <td>192.0</td>\n","      <td>120.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 4</th>\n","      <td>0.758621</td>\n","      <td>0.788801</td>\n","      <td>0.7900</td>\n","      <td>37.0</td>\n","      <td>47.0</td>\n","      <td>0.737430</td>\n","      <td>0.781065</td>\n","      <td>00:10:01.333893</td>\n","      <td>184.0</td>\n","      <td>132.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 5</th>\n","      <td>0.801153</td>\n","      <td>0.826827</td>\n","      <td>0.8275</td>\n","      <td>30.0</td>\n","      <td>39.0</td>\n","      <td>0.780899</td>\n","      <td>0.822485</td>\n","      <td>00:10:15.034749</td>\n","      <td>192.0</td>\n","      <td>139.0</td>\n","    </tr>\n","    <tr>\n","      <th>CV Average</th>\n","      <td>0.748256</td>\n","      <td>0.781562</td>\n","      <td>0.7850</td>\n","      <td>40.4</td>\n","      <td>45.6</td>\n","      <td>0.737525</td>\n","      <td>0.759996</td>\n","      <td>00:10:28.891837</td>\n","      <td>186.0</td>\n","      <td>128.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            F1 Score       auc  ...  True Negatives  True Positives\n","Fold 1      0.724138  0.758621  ...  178.0           126.0         \n","Fold 2      0.725664  0.762623  ...  184.0           123.0         \n","Fold 3      0.731707  0.770936  ...  192.0           120.0         \n","Fold 4      0.758621  0.788801  ...  184.0           132.0         \n","Fold 5      0.801153  0.826827  ...  192.0           139.0         \n","CV Average  0.748256  0.781562  ...  186.0           128.0         \n","\n","[6 rows x 10 columns]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Fold 1:\tF-Score: 0.7398843765258789\n","Training took time  0:10:57.391297\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 2:\tF-Score: 0.7547169327735901\n","Training took time  0:10:17.240653\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 3:\tF-Score: 0.7155963182449341\n","Training took time  0:10:15.866988\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 4:\tF-Score: 0.7514792084693909\n","Training took time  0:10:17.786046\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 5:\tF-Score: 0.7283581495285034\n","Training took time  0:10:22.062096\n","---------------------------------------------------------------------------------------------------------\n","\n","\n","Training Batch Size:  32 \tLearn Rate:  2e-05 \tNumSteps:  800\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1 Score</th>\n","      <th>auc</th>\n","      <th>Accuracy</th>\n","      <th>False Negatives</th>\n","      <th>False Positives</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Training Time</th>\n","      <th>True Negatives</th>\n","      <th>True Positives</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Fold 1</th>\n","      <td>0.739884</td>\n","      <td>0.773194</td>\n","      <td>0.7750</td>\n","      <td>40.0</td>\n","      <td>50.0</td>\n","      <td>0.719101</td>\n","      <td>0.761905</td>\n","      <td>00:10:57.391297</td>\n","      <td>182.0</td>\n","      <td>128.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 2</th>\n","      <td>0.754717</td>\n","      <td>0.792488</td>\n","      <td>0.8050</td>\n","      <td>48.0</td>\n","      <td>30.0</td>\n","      <td>0.800000</td>\n","      <td>0.714286</td>\n","      <td>00:10:17.240653</td>\n","      <td>202.0</td>\n","      <td>120.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 3</th>\n","      <td>0.715596</td>\n","      <td>0.757697</td>\n","      <td>0.7675</td>\n","      <td>51.0</td>\n","      <td>42.0</td>\n","      <td>0.735849</td>\n","      <td>0.696429</td>\n","      <td>00:10:15.866988</td>\n","      <td>190.0</td>\n","      <td>117.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 4</th>\n","      <td>0.751479</td>\n","      <td>0.784831</td>\n","      <td>0.7900</td>\n","      <td>42.0</td>\n","      <td>42.0</td>\n","      <td>0.751479</td>\n","      <td>0.751479</td>\n","      <td>00:10:17.786046</td>\n","      <td>189.0</td>\n","      <td>127.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 5</th>\n","      <td>0.728358</td>\n","      <td>0.765709</td>\n","      <td>0.7725</td>\n","      <td>47.0</td>\n","      <td>44.0</td>\n","      <td>0.734940</td>\n","      <td>0.721893</td>\n","      <td>00:10:22.062096</td>\n","      <td>187.0</td>\n","      <td>122.0</td>\n","    </tr>\n","    <tr>\n","      <th>CV Average</th>\n","      <td>0.738007</td>\n","      <td>0.774784</td>\n","      <td>0.7820</td>\n","      <td>45.6</td>\n","      <td>41.6</td>\n","      <td>0.748274</td>\n","      <td>0.729198</td>\n","      <td>00:10:26.069416</td>\n","      <td>190.0</td>\n","      <td>122.8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            F1 Score       auc  ...  True Negatives  True Positives\n","Fold 1      0.739884  0.773194  ...  182.0           128.0         \n","Fold 2      0.754717  0.792488  ...  202.0           120.0         \n","Fold 3      0.715596  0.757697  ...  190.0           117.0         \n","Fold 4      0.751479  0.784831  ...  189.0           127.0         \n","Fold 5      0.728358  0.765709  ...  187.0           122.0         \n","CV Average  0.738007  0.774784  ...  190.0           122.8         \n","\n","[6 rows x 10 columns]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Fold 1:\tF-Score: 0.7621950507164001\n","Training took time  0:10:16.814623\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 2:\tF-Score: 0.777108371257782\n","Training took time  0:10:21.201833\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 3:\tF-Score: 0.782608687877655\n","Training took time  0:10:21.547542\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 4:\tF-Score: 0.7816091179847717\n","Training took time  0:10:21.263959\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 5:\tF-Score: 0.7090908288955688\n","Training took time  0:10:35.990588\n","---------------------------------------------------------------------------------------------------------\n","\n","\n","Training Batch Size:  32 \tLearn Rate:  2e-05 \tNumSteps:  800\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1 Score</th>\n","      <th>auc</th>\n","      <th>Accuracy</th>\n","      <th>False Negatives</th>\n","      <th>False Positives</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Training Time</th>\n","      <th>True Negatives</th>\n","      <th>True Positives</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Fold 1</th>\n","      <td>0.762195</td>\n","      <td>0.796593</td>\n","      <td>0.8050</td>\n","      <td>43.0</td>\n","      <td>35.0</td>\n","      <td>0.781250</td>\n","      <td>0.744048</td>\n","      <td>00:10:16.814623</td>\n","      <td>197.0</td>\n","      <td>125.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 2</th>\n","      <td>0.777108</td>\n","      <td>0.808497</td>\n","      <td>0.8150</td>\n","      <td>39.0</td>\n","      <td>35.0</td>\n","      <td>0.786585</td>\n","      <td>0.767857</td>\n","      <td>00:10:21.201833</td>\n","      <td>197.0</td>\n","      <td>129.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 3</th>\n","      <td>0.782609</td>\n","      <td>0.811269</td>\n","      <td>0.8125</td>\n","      <td>33.0</td>\n","      <td>42.0</td>\n","      <td>0.762712</td>\n","      <td>0.803571</td>\n","      <td>00:10:21.547542</td>\n","      <td>190.0</td>\n","      <td>135.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 4</th>\n","      <td>0.781609</td>\n","      <td>0.809293</td>\n","      <td>0.8100</td>\n","      <td>33.0</td>\n","      <td>43.0</td>\n","      <td>0.759777</td>\n","      <td>0.804734</td>\n","      <td>00:10:21.263959</td>\n","      <td>188.0</td>\n","      <td>136.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 5</th>\n","      <td>0.709091</td>\n","      <td>0.750916</td>\n","      <td>0.7600</td>\n","      <td>52.0</td>\n","      <td>44.0</td>\n","      <td>0.726708</td>\n","      <td>0.692308</td>\n","      <td>00:10:35.990588</td>\n","      <td>187.0</td>\n","      <td>117.0</td>\n","    </tr>\n","    <tr>\n","      <th>CV Average</th>\n","      <td>0.762522</td>\n","      <td>0.795314</td>\n","      <td>0.8005</td>\n","      <td>40.0</td>\n","      <td>39.8</td>\n","      <td>0.763406</td>\n","      <td>0.762504</td>\n","      <td>00:10:23.363709</td>\n","      <td>191.8</td>\n","      <td>128.4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            F1 Score       auc  ...  True Negatives  True Positives\n","Fold 1      0.762195  0.796593  ...  197.0           125.0         \n","Fold 2      0.777108  0.808497  ...  197.0           129.0         \n","Fold 3      0.782609  0.811269  ...  190.0           135.0         \n","Fold 4      0.781609  0.809293  ...  188.0           136.0         \n","Fold 5      0.709091  0.750916  ...  187.0           117.0         \n","CV Average  0.762522  0.795314  ...  191.8           128.4         \n","\n","[6 rows x 10 columns]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Fold 1:\tF-Score: 0.7869821190834045\n","Training took time  0:10:23.567323\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 2:\tF-Score: 0.7803467512130737\n","Training took time  0:10:42.339975\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 3:\tF-Score: 0.7818180918693542\n","Training took time  0:10:29.001128\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 4:\tF-Score: 0.7714285254478455\n","Training took time  0:10:29.696910\n","---------------------------------------------------------------------------------------------------------\n","\n","Fold 5:\tF-Score: 0.7861270904541016\n","Training took time  0:10:29.761172\n","---------------------------------------------------------------------------------------------------------\n","\n","\n","Training Batch Size:  32 \tLearn Rate:  2e-05 \tNumSteps:  800\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>F1 Score</th>\n","      <th>auc</th>\n","      <th>Accuracy</th>\n","      <th>False Negatives</th>\n","      <th>False Positives</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Training Time</th>\n","      <th>True Negatives</th>\n","      <th>True Positives</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Fold 1</th>\n","      <td>0.786982</td>\n","      <td>0.815569</td>\n","      <td>0.820</td>\n","      <td>36.0</td>\n","      <td>36.0</td>\n","      <td>0.786982</td>\n","      <td>0.786982</td>\n","      <td>00:10:23.567323</td>\n","      <td>195.0</td>\n","      <td>133.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 2</th>\n","      <td>0.780347</td>\n","      <td>0.808499</td>\n","      <td>0.810</td>\n","      <td>34.0</td>\n","      <td>42.0</td>\n","      <td>0.762712</td>\n","      <td>0.798817</td>\n","      <td>00:10:42.339975</td>\n","      <td>189.0</td>\n","      <td>135.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 3</th>\n","      <td>0.781818</td>\n","      <td>0.812808</td>\n","      <td>0.820</td>\n","      <td>39.0</td>\n","      <td>33.0</td>\n","      <td>0.796296</td>\n","      <td>0.767857</td>\n","      <td>00:10:29.001128</td>\n","      <td>199.0</td>\n","      <td>129.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 4</th>\n","      <td>0.771429</td>\n","      <td>0.800493</td>\n","      <td>0.800</td>\n","      <td>33.0</td>\n","      <td>47.0</td>\n","      <td>0.741758</td>\n","      <td>0.803571</td>\n","      <td>00:10:29.696910</td>\n","      <td>185.0</td>\n","      <td>135.0</td>\n","    </tr>\n","    <tr>\n","      <th>Fold 5</th>\n","      <td>0.786127</td>\n","      <td>0.814245</td>\n","      <td>0.815</td>\n","      <td>32.0</td>\n","      <td>42.0</td>\n","      <td>0.764045</td>\n","      <td>0.809524</td>\n","      <td>00:10:29.761172</td>\n","      <td>190.0</td>\n","      <td>136.0</td>\n","    </tr>\n","    <tr>\n","      <th>CV Average</th>\n","      <td>0.781341</td>\n","      <td>0.810323</td>\n","      <td>0.813</td>\n","      <td>34.8</td>\n","      <td>40.0</td>\n","      <td>0.770359</td>\n","      <td>0.793350</td>\n","      <td>00:10:30.873301</td>\n","      <td>191.6</td>\n","      <td>133.6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            F1 Score       auc  ...  True Negatives  True Positives\n","Fold 1      0.786982  0.815569  ...  195.0           133.0         \n","Fold 2      0.780347  0.808499  ...  189.0           135.0         \n","Fold 3      0.781818  0.812808  ...  199.0           129.0         \n","Fold 4      0.771429  0.800493  ...  185.0           135.0         \n","Fold 5      0.786127  0.814245  ...  190.0           136.0         \n","CV Average  0.781341  0.810323  ...  191.6           133.6         \n","\n","[6 rows x 10 columns]"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Fold 1:\tF-Score: 0.7820895314216614\n","Training took time  0:10:37.546774\n","---------------------------------------------------------------------------------------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I7Qwmr3GP1-6","colab_type":"text"},"source":["# Testing Model"]},{"cell_type":"markdown","metadata":{"id":"y4KnOByLTj9i","colab_type":"text"},"source":["## Adding in Back-Translated Hate Speech Tweets as Extra Data\n","\n","We have very few instances of hate speech labelled in this dataset. To remedy this I performed back_translation augmentation on this training set.\n","\n","Below I load in in the extra hate speech tweets I created via back-translation augmentation I performed in another colab notebook and I append it to the existing dataframe"]},{"cell_type":"code","metadata":{"id":"DHT16O8CTfpN","colab_type":"code","outputId":"8cd09f29-baf1-4fd0-fba9-e4ac17523842","executionInfo":{"status":"ok","timestamp":1580488889789,"user_tz":0,"elapsed":3685511,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"dat = '/content/drive/My Drive/hateval2019/backtranslated_hatEval.txt' \n","dat = pd.read_csv(dat, sep = '\\t', names = ['tweet'], header = None, encoding = 'utf-8')\n","pd.set_option('display.max_colwidth', -1)\n","dat = dat.astype(str)\n","dat.head(50)\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"dat = '/content/drive/My Drive/hateval2019/backtranslated_hatEval.txt' \\ndat = pd.read_csv(dat, sep = '\\t', names = ['tweet'], header = None, encoding = 'utf-8')\\npd.set_option('display.max_colwidth', -1)\\ndat = dat.astype(str)\\ndat.head(50)\""]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"X9jImOlRT2EO","colab_type":"text"},"source":["**See how the english is a little off?** \n","\n","That's because these are the hate speech tweets in the training set translated to french, then translated back again. This creates a whole new, yet similar set of hate speech tweets to train on. (Slightly augmented text)"]},{"cell_type":"code","metadata":{"id":"pe1Ndra6UAbb","colab_type":"code","outputId":"219efd82-0da1-46f3-fd7d-30f6077675a2","executionInfo":{"status":"ok","timestamp":1580488889789,"user_tz":0,"elapsed":3685501,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"print(\"There are\", len(dat.index), \"tweets\")\n","dat = dat[dat['tweet'].apply(lambda x: len(x) > 10)]\n","print(\"There are now\", len(dat.index), \"tweets\")\n","dat.head()\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'print(\"There are\", len(dat.index), \"tweets\")\\ndat = dat[dat[\\'tweet\\'].apply(lambda x: len(x) > 10)]\\nprint(\"There are now\", len(dat.index), \"tweets\")\\ndat.head()'"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"vNeswnQeUBY2","colab_type":"text"},"source":["<b>Rather than creating 3768 extra tweets, 19630 extra have been created. The tweets have been incorrectly parsed. Removing some tweets with a smaller length may mitigate this effect somewhat by removing tweets that were cut in half</b>\n","\n","Let's see if it helps by adding it to the original training set and testing it against our dev data"]},{"cell_type":"code","metadata":{"id":"apxp79BtUyf5","colab_type":"code","outputId":"cee28152-5561-4062-e3e2-12c5fb663e9f","executionInfo":{"status":"ok","timestamp":1580488889790,"user_tz":0,"elapsed":3685491,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"dat['label'] = 1\n","dat['id'] = 80000\n","frames = [dat,data]\n","data = pd.concat(frames)\n","print(data.info())\n","data.head()\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"dat['label'] = 1\\ndat['id'] = 80000\\nframes = [dat,data]\\ndata = pd.concat(frames)\\nprint(data.info())\\ndata.head()\""]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"lHT94i8VU4r7","colab_type":"text"},"source":["We'll shuffle the dataframe to make sure there's no funny business with the training of the model and we'll then reset the id field to make it unique and sequential for each row"]},{"cell_type":"code","metadata":{"id":"w8rPNri1VAiE","colab_type":"code","outputId":"1cbdbd13-3813-401a-d883-c5995c7c1158","executionInfo":{"status":"ok","timestamp":1580488889791,"user_tz":0,"elapsed":3685476,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"data = data.sample(frac=1, random_state = SEED)\n","data.reset_index(drop = True, inplace = True)\n","\n","data['id'] = data.reset_index().index + 1\n","print(data.label.value_counts(), \"\\n\")\n","print(data.info())\n","length = len(data.index)\n","print(\"\\nNow there are\", length , \"tweets total in this database\")\n","data.tail(10)\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'data = data.sample(frac=1)\\ndata.reset_index(drop = True, inplace = True)\\n\\ndata[\\'id\\'] = data.reset_index().index + 1\\nprint(data.label.value_counts(), \"\\n\")\\nprint(data.info())\\nlength = len(data.index)\\nprint(\"\\nNow there are\", length , \"tweets total in this database\")\\ndata.tail(10)'"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"SMgEcg_v4l7m","colab_type":"text"},"source":["<b>Training with both dev and training set. Then Testing with the holdout test set</b>\n","\n","Loading in train and test data..."]},{"cell_type":"code","metadata":{"id":"c3KKifBM4kx5","colab_type":"code","outputId":"ec306351-5744-42b3-c22d-7b6b6b297143","executionInfo":{"status":"ok","timestamp":1585591971952,"user_tz":-60,"elapsed":16267,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["train = loadData(rawTrain, rawDev)\n","\n","test = loadData(rawTest)\n","test.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2317</th>\n","      <td>31035</td>\n","      <td>anyone whoever doubted louis &amp; said he couldnt sing go listen to back to you &amp; if u dont change ur mind ur just a bitter bitch</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2867</th>\n","      <td>34114</td>\n","      <td>bitch i was fuckn up till 4:30 but your hoe ass didn't text me back. pouting face</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1378</th>\n","      <td>34255</td>\n","      <td>this is nancynancy called my pay raise crumbs nancy doesn't want to fund the military nancy puts illegal aliens rights before citizen rights.nancy wants to house the illegals before our homeless veterans don't be a nancy. trump wednesday wisdom maga</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2281</th>\n","      <td>34280</td>\n","      <td>well bitch tell me how you download viss</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1481</th>\n","      <td>33582</td>\n","      <td>tx: man arrested trying to get into house with knife - heriberto coronado, 28, is alleged to have held a knife to a female victim's throat at one point as well. \"he was also named in a detainer on an immigration charge.\" deport them all</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id  ... label\n","2317  31035  ...  0   \n","2867  34114  ...  1   \n","1378  34255  ...  0   \n","2281  34280  ...  0   \n","1481  33582  ...  0   \n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"ZcNSY0kA8N4N","colab_type":"text"},"source":["Function to get predictions on test data"]},{"cell_type":"code","metadata":{"id":"mvpPwfj08PmW","colab_type":"code","colab":{}},"source":["def getPrediction(in_sentences):\n","  #Makes output less verbose\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","  labels = [0, 1]\n","  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n","  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n","  predictions = list(estimator.predict(predict_input_fn))\n","\n","  #Initialise empty predicted labels array\n","  predicted_classes = [None] * len(predictions)\n","\n","  #Use a for loop to iterate through probabilities and for each prediction assign a label\n","  #corresponding to which label has the highest probability\n","  for i in range(0, len(predictions)):\n","    if predictions[i]['probabilities'][0] > predictions[i]['probabilities'][1]:\n","      predicted_classes[i] = 0\n","    else:\n","      predicted_classes[i] = 1\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO) # Reset tensorflow verboisty to normal\n","\n","  return predicted_classes"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rt4d5H3A6Iqu","colab_type":"text"},"source":["<b> Converting to features, setting run and model configs.\n","\n","Then training on train and dev set and predicting on unseen test set </b>"]},{"cell_type":"code","metadata":{"id":"Xf0Ofg3M6HBC","colab_type":"code","outputId":"ccd23782-571f-4566-dad7-1882e006db44","executionInfo":{"status":"error","timestamp":1585525022648,"user_tz":-60,"elapsed":3356905,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["if DATASET == \"HatEval\":\n","\n","  SAVE_CHECKPOINTS_STEPS = 1000\n","  run_config = tf.compat.v1.estimator.tpu.RunConfig(\n","      #I think the output file must be a sub-directory of the main BERT file\n","      model_dir=OUTPUT_DIR,\n","      tf_random_seed=SEED, \n","      cluster=cluster_resolver,\n","      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","      tpu_config=tf.contrib.tpu.TPUConfig(\n","          iterations_per_loop=200,\n","          num_shards=8,\n","          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","\n","  train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, \n","                                                                        text_a = x[DATA_COLUMN], \n","                                                                        text_b = None, \n","                                                                        label = x[LABEL_COLUMN]), axis = 1)\n","\n","  train_features =  run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","\n","\n","  #Delete prior model graph, checkpoints and eval files to make room for new model each loop\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","  # Doesn't matter if the directory didn't exist\n","    pass\n","  tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","  # Compute # warmup steps\n","  num_warmup_steps = int(NUM_TRAIN_STEPS * WARMUP_PROPORTION)\n","\n","  # Model configs\n","  \n","  \"\"\"\n","  model_fn = run_classifier.model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  use_one_hot_embeddings=True)\n","  \"\"\"\n","  model_fn = model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=NUM_TRAIN_STEPS,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  ft_params = FT_PARAMS)\n","    \n","  estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","    use_tpu=True,\n","    model_fn=model_fn,\n","    config=run_config,\n","    train_batch_size=TRAIN_BATCH_SIZE,\n","    eval_batch_size=8,\n","    predict_batch_size=8)\n","\n","  # Create an input function for training. drop_remainder = True for using TPUs.\n","  train_input_fn = run_classifier.input_fn_builder(\n","      features=train_features,\n","      seq_length=MAX_SEQ_LENGTH,\n","      is_training=True,\n","      drop_remainder=True)\n","\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n","\n","  print(\"\\nThe model will stop training when it reaches\", num_train_steps, \"as a checkpoint\")\n","\n","  print(f'Beginning Training!')\n","  current_time = datetime.now()\n","  estimator.train(input_fn=train_input_fn, max_steps=NUM_TRAIN_STEPS)\n","  train_time = datetime.now() - current_time\n","  print(\"Training took time \", train_time)\n","\n","  predictions = getPrediction(test.tweet)\n","  test['predictions'] = predictions\n","\n","  test.to_csv('gs://csc3002/hateval2019/predictions.csv', sep=',',  index = True, encoding = 'utf-8')\n","  print(\"\\n\\nF1 Score:\", metrics.f1_score(test.label, test.predictions))\n","  print(\"Accuracy\", metrics.accuracy_score(test.label, test.predictions))\n","\n","elif DATASET == \"AnalyticsVidhya\":\n","  \n","  SAVE_CHECKPOINTS_STEPS = 10000\n","  run_config = tf.compat.v1.estimator.tpu.RunConfig(\n","      #I think the output file must be a sub-directory of the main BERT file\n","      model_dir=OUTPUT_DIR,\n","      tf_random_seed=SEED, \n","      cluster=cluster_resolver,\n","      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","      tpu_config=tf.contrib.tpu.TPUConfig(\n","          iterations_per_loop=200,\n","          num_shards=8,\n","          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","\n","  train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, \n","                                                                        text_a = x[DATA_COLUMN], \n","                                                                        text_b = None, \n","                                                                        label = x[LABEL_COLUMN]), axis = 1)\n","\n","  train_features =  run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","\n","\n","  #Delete prior model graph, checkpoints and eval files to make room for new model each loop\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","  # Doesn't matter if the directory didn't exist\n","    pass\n","  tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","  # Compute # warmup steps\n","  num_warmup_steps = int(NUM_TRAIN_STEPS * WARMUP_PROPORTION)\n","\n","  \"\"\" Model Configs\"\"\"\n","\n","  model_fn = model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=NUM_TRAIN_STEPS,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  ft_params = FT_PARAMS)\n","\n","  estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","    use_tpu=True,\n","    model_fn=model_fn,\n","    config=run_config,\n","    train_batch_size=TRAIN_BATCH_SIZE,\n","    eval_batch_size=8,\n","    predict_batch_size=8)\n","\n","  # Create an input function for training. drop_remainder = True for using TPUs.\n","  train_input_fn = run_classifier.input_fn_builder(\n","      features=train_features,\n","      seq_length=MAX_SEQ_LENGTH,\n","      is_training=True,\n","      drop_remainder=True)\n","\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n","\n","  print(\"\\nThe model will stop training when it reaches\", NUM_TRAIN_STEPS, \"as a checkpoint\")\n","\n","  print(f'Beginning Training!')\n","  current_time = datetime.now()\n","  estimator.train(input_fn=train_input_fn, max_steps=NUM_TRAIN_STEPS)\n","  train_time = datetime.now() - current_time\n","  print(\"Training took time \", train_time)\n","\n","  predictions = getPrediction(test.tweet)\n","  test['label'] = predictions\n","  print(test.label.value_counts())\n","  print(predictions[0:20])\n","  test.drop(columns = ['tweet'], axis = 1,inplace = True)\n","  test.to_csv('gs://csc3002/trial/submission.csv', sep=',', index = False)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 31962\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] bi ##h day fl ##or . . . the best master mixer in the world . . . . . . x ##ox ##o fe ##li ##z cum ##ple ao ##s fl ##or . . . . el . . . [SEP]\n","INFO:tensorflow:input_ids: 101 12170 2232 2154 13109 2953 1012 1012 1012 1996 2190 3040 23228 1999 1996 2088 1012 1012 1012 1012 1012 1012 1060 11636 2080 10768 3669 2480 13988 10814 20118 2015 13109 2953 1012 1012 1012 1012 3449 1012 1012 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] it ' s almost volleyball season ! official things [SEP]\n","INFO:tensorflow:input_ids: 101 2009 1005 1055 2471 7454 2161 999 2880 2477 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] out ##cast . . i failed but raise again . cover singing singer song disney audio [SEP]\n","INFO:tensorflow:input_ids: 101 2041 10526 1012 1012 1045 3478 2021 5333 2153 1012 3104 4823 3220 2299 6373 5746 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] you have the power to heal yourself / / louise hay healthy [SEP]\n","INFO:tensorflow:input_ids: 101 2017 2031 1996 2373 2000 11005 4426 1013 1013 8227 10974 7965 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] our first game is almost ready for purchase ! tablet ##op [SEP]\n","INFO:tensorflow:input_ids: 101 2256 2034 2208 2003 2471 3201 2005 5309 999 13855 7361 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:Writing example 10000 of 31962\n","INFO:tensorflow:Writing example 20000 of 31962\n","INFO:tensorflow:Writing example 30000 of 31962\n","WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7ff70d7b1158>) includes params argument, but params are not passed to Estimator.\n","INFO:tensorflow:Using config: {'_model_dir': 'gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output', '_tf_random_seed': 3060, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.84.176.178:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff707b962b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.84.176.178:8470', '_evaluation_master': 'grpc://10.84.176.178:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=200, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7ff722742eb8>}\n","INFO:tensorflow:_TPUContext: eval_on_tpu True\n","\n","The model will stop training when it reaches 8000 as a checkpoint\n","Beginning Training!\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.84.176.178:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4749744381645999822)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 6236472980143800598)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 15384134400022073624)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 8703291896865245298)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 15444855322906250596)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3951367853733287682)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5249770085924543224)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5980241302106014006)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 14292293327675859813)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 8751357451425363312)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9395462038306012250)\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","INFO:tensorflow:\n","Using loss type:binary_cross_entropy\n","INFO:tensorflow:Using Bi-Directional LTSM for Fine-Tuning. 2 extra layer\n","WARNING:tensorflow:From <ipython-input-64-7d96dd214f8f>:34: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-64-7d96dd214f8f>:40: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt.\n","INFO:tensorflow:Initialized dataset iterators in 3 seconds\n","INFO:tensorflow:Installing graceful shutdown hook.\n","INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 8 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:Outfeed finished for iteration (0, 179)\n","INFO:tensorflow:loss = 0.035189122, step = 200\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (1, 97)\n","INFO:tensorflow:loss = 0.0072796107, step = 400 (82.262 sec)\n","INFO:tensorflow:global_step/sec: 2.43125\n","INFO:tensorflow:examples/sec: 77.7999\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (2, 61)\n","INFO:tensorflow:loss = 0.003925852, step = 600 (72.083 sec)\n","INFO:tensorflow:global_step/sec: 2.77455\n","INFO:tensorflow:examples/sec: 88.7855\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (3, 8)\n","INFO:tensorflow:Outfeed finished for iteration (3, 187)\n","INFO:tensorflow:loss = 0.021184273, step = 800 (78.230 sec)\n","INFO:tensorflow:global_step/sec: 2.5566\n","INFO:tensorflow:examples/sec: 81.8113\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (4, 152)\n","INFO:tensorflow:loss = 0.017594118, step = 1000 (72.091 sec)\n","INFO:tensorflow:global_step/sec: 2.77423\n","INFO:tensorflow:examples/sec: 88.7753\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (5, 99)\n","INFO:tensorflow:loss = 0.0016815059, step = 1200 (77.973 sec)\n","INFO:tensorflow:global_step/sec: 2.565\n","INFO:tensorflow:examples/sec: 82.08\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (6, 64)\n","INFO:tensorflow:loss = 0.0024504531, step = 1400 (76.876 sec)\n","INFO:tensorflow:global_step/sec: 2.6016\n","INFO:tensorflow:examples/sec: 83.2512\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (7, 14)\n","INFO:tensorflow:Outfeed finished for iteration (7, 193)\n","INFO:tensorflow:loss = 0.00075672375, step = 1600 (72.209 sec)\n","INFO:tensorflow:global_step/sec: 2.76975\n","INFO:tensorflow:examples/sec: 88.6319\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (8, 157)\n","INFO:tensorflow:loss = 0.0006425771, step = 1800 (72.156 sec)\n","INFO:tensorflow:global_step/sec: 2.77175\n","INFO:tensorflow:examples/sec: 88.6961\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (9, 105)\n","INFO:tensorflow:loss = 0.0011651311, step = 2000 (77.654 sec)\n","INFO:tensorflow:global_step/sec: 2.57555\n","INFO:tensorflow:examples/sec: 82.4175\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (10, 70)\n","INFO:tensorflow:loss = 0.00049957517, step = 2200 (72.242 sec)\n","INFO:tensorflow:global_step/sec: 2.76849\n","INFO:tensorflow:examples/sec: 88.5918\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (11, 17)\n","INFO:tensorflow:Outfeed finished for iteration (11, 196)\n","INFO:tensorflow:loss = 0.00036058624, step = 2400 (78.016 sec)\n","INFO:tensorflow:global_step/sec: 2.56357\n","INFO:tensorflow:examples/sec: 82.0342\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (12, 161)\n","INFO:tensorflow:loss = 0.0015221995, step = 2600 (77.859 sec)\n","INFO:tensorflow:global_step/sec: 2.56875\n","INFO:tensorflow:examples/sec: 82.2001\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (13, 108)\n","INFO:tensorflow:loss = 0.0002953164, step = 2800 (72.206 sec)\n","INFO:tensorflow:global_step/sec: 2.76984\n","INFO:tensorflow:examples/sec: 88.6348\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (14, 72)\n","INFO:tensorflow:loss = 0.00028238836, step = 3000 (72.298 sec)\n","INFO:tensorflow:global_step/sec: 2.76633\n","INFO:tensorflow:examples/sec: 88.5225\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (15, 18)\n","INFO:tensorflow:Outfeed finished for iteration (15, 197)\n","INFO:tensorflow:loss = 0.00023859728, step = 3200 (78.168 sec)\n","INFO:tensorflow:global_step/sec: 2.55859\n","INFO:tensorflow:examples/sec: 81.8749\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (16, 161)\n","INFO:tensorflow:loss = 0.0006586466, step = 3400 (72.159 sec)\n","INFO:tensorflow:global_step/sec: 2.77166\n","INFO:tensorflow:examples/sec: 88.6932\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (17, 107)\n","INFO:tensorflow:loss = 0.00013900858, step = 3600 (78.318 sec)\n","INFO:tensorflow:global_step/sec: 2.55369\n","INFO:tensorflow:examples/sec: 81.718\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (18, 72)\n","INFO:tensorflow:loss = 0.00014944894, step = 3800 (77.442 sec)\n","INFO:tensorflow:global_step/sec: 2.5826\n","INFO:tensorflow:examples/sec: 82.6431\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (19, 20)\n","INFO:tensorflow:Outfeed finished for iteration (19, 199)\n","INFO:tensorflow:loss = 0.00026089078, step = 4000 (72.268 sec)\n","INFO:tensorflow:global_step/sec: 2.76747\n","INFO:tensorflow:examples/sec: 88.5591\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (20, 163)\n","INFO:tensorflow:loss = 0.00012189885, step = 4200 (72.247 sec)\n","INFO:tensorflow:global_step/sec: 2.76829\n","INFO:tensorflow:examples/sec: 88.5853\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (21, 108)\n","INFO:tensorflow:loss = 9.163533e-05, step = 4400 (78.635 sec)\n","INFO:tensorflow:global_step/sec: 2.5434\n","INFO:tensorflow:examples/sec: 81.3887\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (22, 72)\n","INFO:tensorflow:loss = 8.7246524e-05, step = 4600 (72.256 sec)\n","INFO:tensorflow:global_step/sec: 2.76791\n","INFO:tensorflow:examples/sec: 88.5732\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (23, 20)\n","INFO:tensorflow:Outfeed finished for iteration (23, 199)\n","INFO:tensorflow:loss = 0.00028883523, step = 4800 (77.746 sec)\n","INFO:tensorflow:global_step/sec: 2.57249\n","INFO:tensorflow:examples/sec: 82.3198\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (24, 163)\n","INFO:tensorflow:loss = 0.0003584839, step = 5000 (77.944 sec)\n","INFO:tensorflow:global_step/sec: 2.56593\n","INFO:tensorflow:examples/sec: 82.1096\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (25, 110)\n","INFO:tensorflow:loss = 6.7997964e-05, step = 5200 (72.376 sec)\n","INFO:tensorflow:global_step/sec: 2.76337\n","INFO:tensorflow:examples/sec: 88.4278\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (26, 74)\n","INFO:tensorflow:loss = 5.8113947e-05, step = 5400 (72.201 sec)\n","INFO:tensorflow:global_step/sec: 2.77003\n","INFO:tensorflow:examples/sec: 88.6408\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (27, 21)\n","INFO:tensorflow:loss = 9.2224174e-05, step = 5600 (77.952 sec)\n","INFO:tensorflow:global_step/sec: 2.56567\n","INFO:tensorflow:examples/sec: 82.1015\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (28, 0)\n","INFO:tensorflow:Outfeed finished for iteration (28, 179)\n","INFO:tensorflow:loss = 4.7657457e-05, step = 5800 (72.205 sec)\n","INFO:tensorflow:global_step/sec: 2.76989\n","INFO:tensorflow:examples/sec: 88.6364\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (29, 127)\n","INFO:tensorflow:loss = 0.00018533906, step = 6000 (77.594 sec)\n","INFO:tensorflow:global_step/sec: 2.5775\n","INFO:tensorflow:examples/sec: 82.4801\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (30, 92)\n","INFO:tensorflow:loss = 4.6733363e-05, step = 6200 (78.626 sec)\n","INFO:tensorflow:global_step/sec: 2.54367\n","INFO:tensorflow:examples/sec: 81.3974\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (31, 37)\n","INFO:tensorflow:loss = 4.3201522e-05, step = 6400 (72.342 sec)\n","INFO:tensorflow:global_step/sec: 2.76467\n","INFO:tensorflow:examples/sec: 88.4693\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (32, 1)\n","INFO:tensorflow:Outfeed finished for iteration (32, 180)\n","INFO:tensorflow:loss = 4.2933534e-05, step = 6600 (72.253 sec)\n","INFO:tensorflow:global_step/sec: 2.76805\n","INFO:tensorflow:examples/sec: 88.5774\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (33, 126)\n","INFO:tensorflow:loss = 0.0001424483, step = 6800 (78.308 sec)\n","INFO:tensorflow:global_step/sec: 2.554\n","INFO:tensorflow:examples/sec: 81.728\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (34, 90)\n","INFO:tensorflow:loss = 3.7388138e-05, step = 7000 (72.237 sec)\n","INFO:tensorflow:global_step/sec: 2.7687\n","INFO:tensorflow:examples/sec: 88.5984\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (35, 31)\n","INFO:tensorflow:loss = 3.777576e-05, step = 7200 (80.117 sec)\n","INFO:tensorflow:global_step/sec: 2.49632\n","INFO:tensorflow:examples/sec: 79.8824\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (36, 0)\n","INFO:tensorflow:Outfeed finished for iteration (36, 179)\n","INFO:tensorflow:loss = 3.6598358e-05, step = 7400 (77.299 sec)\n","INFO:tensorflow:global_step/sec: 2.58737\n","INFO:tensorflow:examples/sec: 82.7959\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (37, 127)\n","INFO:tensorflow:loss = 0.00015052361, step = 7600 (72.284 sec)\n","INFO:tensorflow:global_step/sec: 2.76687\n","INFO:tensorflow:examples/sec: 88.5399\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (38, 91)\n","INFO:tensorflow:loss = 0.00013578072, step = 7800 (72.289 sec)\n","INFO:tensorflow:global_step/sec: 2.76665\n","INFO:tensorflow:examples/sec: 88.5329\n","INFO:tensorflow:Enqueue next (200) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (200) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (39, 35)\n","INFO:tensorflow:loss = 3.5748024e-05, step = 8000 (79.010 sec)\n","INFO:tensorflow:global_step/sec: 2.53134\n","INFO:tensorflow:examples/sec: 81.0028\n","INFO:tensorflow:Saving checkpoints for 8000 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model/output/model.ckpt.\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Loss for final step: 3.5748024e-05.\n","INFO:tensorflow:training_loop marked as finished\n","Training took time  0:55:39.989655\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-68-b9cc206008cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training took time \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tweet'"]}]},{"cell_type":"markdown","metadata":{"id":"_Do-iXtCOWBX","colab_type":"text"},"source":["## Error Analysis"]},{"cell_type":"code","metadata":{"id":"mEA145I2ZcTP","colab_type":"code","outputId":"889473f5-c419-4352-db88-f6f1b4f98172","executionInfo":{"status":"ok","timestamp":1585325144818,"user_tz":0,"elapsed":951,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["test.head(40)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","      <th>predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>anyone whoever doubted louis &amp; said he couldnt sing go listen to back to you &amp; if u dont change ur mind ur just a bitter bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>bitch i was fuckn up till 4:30 but your hoe ass didn't text me back. pouting face</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>this is nancynancy called my pay raise crumbs nancy doesn't want to fund the military nancy puts illegal aliens rights before citizen rights.nancy wants to house the illegals before our homeless veterans don't be a nancy. trump wednesday wisdom maga</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>well bitch tell me how you download viss</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>tx: man arrested trying to get into house with knife - heriberto coronado, 28, is alleged to have held a knife to a female victim's throat at one point as well. \"he was also named in a detainer on an immigration charge.\" deport them all</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>whatever it takes. take the money out of welfare payments to the lazy and use that. take it from aid to mexico and south and central america. just build that wall .</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>this bitch got one time to fuck wit me or my dad n ima let her whore ass know</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>ban rape fugees ban islam in the us ten-year-old german boy raped by migrant classmates on school trip via</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>when hoe's talk shit about me i started to get mad like hell bitch i can make your life like a living hell don't play with me!!</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>and anyone who says otherwise is either a grade-a asshole or has no idea what they are talking about. being depressed is a bitch, but...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>your shoes are ugly and anarcho primitivism sucks, bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>this is a pretty sight build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>**illegal criminal alien raped young girl in alabama.**elias pablo, who is a resident of guatemala and an illegal alien to the united states, has been charged with having sex with a child under the age of 12.#deportthemall#buildthedamnwall</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>im tryna go ghost n come back a whole new bitch like bam</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>i'm actually in the same position but this bitch wanna act like a cunt towards me so guess what u ain't knowing shit</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>kyungsoo and fucking baekhyun in todays performance of the eve fucking told me im a dirty crack whore bitchhhhh</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>you can continue to sit in the punk bitch section with all the other blocked hoes</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>if its gets you upset - hes the best man for the job.#gohome strong borders send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>bitch take \"kardash\" out yo name. you ain't worthy hoe</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>hate seeing articles about \"what men should or should not wear/do...\" to whoever comes up with this sh!t: bitch, mind your own business...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>i want them all deported !! i want my vote to count again !!! deport them all no daca no amnesty build the damn wall use the army core of engineers</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>what is the thinking behind veterans against trump ?#potus has reformed &amp; improved the va, while hundreds died during obama reign waiting for treatment.they can think what they want, but let's be rational.#maga kag potus trump news vote red no daca walk away red wave</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>y'all: \"you a hoe\" hoe me: \"that's hoe culture to you bitch\"</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>as usual i scroll a misogynists timeline &amp; it's sports, slut shaming wmn, calling hoes ugly, w/ sad \"why don't women like me?!\" rts mixed in</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>hey bitch stop quoting jimmy tatro you fake ass bitch hoe</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>i have an idea. build that wall detention centers would be reduced dramatically if not eliminated. problem solved.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>eyebrows waxed...feel like a whole new bitch upside-down face</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>got my young bitch kickin flavor she gone flex on you hoes</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>bitch you the one that said you wanted to play</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>and bitch why is you still watching my page hoe you been blocked face with tears of joy df</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>30</td>\n","      <td>my nail girl was hyping me up the whole time \"i love your hair\" \"i love your eyelashes\" \"i love your outfit\" like yass bitch, here's ya tip</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>31</td>\n","      <td>let this hoe touch this kid i swear to fucking god i will murder you bitch, facts</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>32</td>\n","      <td>you hoes can't rap stop it bitch raised hand</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>33</td>\n","      <td>a nigga that beat thou women is a bitch hundred points</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>34</td>\n","      <td>i'm not out here entertaining dudes bout shit. 1. i'm pregnant with a whole ass baby. 2. what kinda dirty trick ass bitch u take me for?</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>35</td>\n","      <td>my mom called me a shoe whore and it's true &lt;/3 lol</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>36</td>\n","      <td>illegal immigrants don't be europe</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>37</td>\n","      <td>too many immigrants unlicensed to kill illegal aliens illegal immigration transportation build that wall</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>38</td>\n","      <td>u a bitch</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>39</td>\n","      <td>mann fuck a bitch , fuck feelings middle finger .yall hoes sneaky litter in bin sign. i hate all you bitches exclamation mark. wish you bitches die relieved face ..goodnight smiling face with heart-eyes key red heart</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  ... predictions\n","0   0   ...  1         \n","1   1   ...  1         \n","2   2   ...  1         \n","3   3   ...  1         \n","4   4   ...  0         \n","5   5   ...  1         \n","6   6   ...  1         \n","7   7   ...  1         \n","8   8   ...  1         \n","9   9   ...  0         \n","10  10  ...  1         \n","11  11  ...  1         \n","12  12  ...  1         \n","13  13  ...  0         \n","14  14  ...  1         \n","15  15  ...  0         \n","16  16  ...  1         \n","17  17  ...  0         \n","18  18  ...  1         \n","19  19  ...  0         \n","20  20  ...  1         \n","21  21  ...  1         \n","22  22  ...  1         \n","23  23  ...  0         \n","24  24  ...  1         \n","25  25  ...  1         \n","26  26  ...  0         \n","27  27  ...  1         \n","28  28  ...  1         \n","29  29  ...  1         \n","30  30  ...  1         \n","31  31  ...  1         \n","32  32  ...  1         \n","33  33  ...  1         \n","34  34  ...  1         \n","35  35  ...  1         \n","36  36  ...  1         \n","37  37  ...  1         \n","38  38  ...  1         \n","39  39  ...  1         \n","\n","[40 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"B9lyIifupp6U","colab_type":"text"},"source":["<b>Our test dataset is supervised for HatEval so perhaps we can do some analysis here by peeking at where we went wrong</b>"]},{"cell_type":"code","metadata":{"id":"w_ek6QNkOXqe","colab_type":"code","outputId":"5be94371-329c-4d07-8b27-c74ea86308e4","executionInfo":{"status":"ok","timestamp":1585325322040,"user_tz":0,"elapsed":915,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["if DATASET == \"HatEval\":\n","  def get_missed_HS(df):\n","    df = df.loc[(df[\"label\"] == 1) & (df[\"predictions\"] == 0)]\n","    return df\n","\n","  def get_misclassified_HS(df):\n","    df = df.loc[(df[\"label\"] == 0) & (df[\"predictions\"] == 1)]\n","    return df\n","  #Perhaps having a separate function for both is a good idea\n","\n","  df = get_missed_HS(test)\n","  df.to_csv('gs://csc3002/hateval2019/falsenegatives.csv', sep=',',  index = True, encoding = 'utf-8')\n","df.head(50)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","      <th>predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>if its gets you upset - hes the best man for the job.#gohome strong borders send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>276</th>\n","      <td>276</td>\n","      <td>stephen miller the public charge law has been on the books for 100 years it prevents immigrants from burdening the u.s taxpayer if they cant take care of themselvers or their kids send them back trump maga red nation rising</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>282</th>\n","      <td>282</td>\n","      <td>\" it may have to do with plain honest speech. so many politicians have the \"\"political speech\"\" bug. theyre accustomed to stating what people want to hear &amp; not what they need to . like saying it is fucktard ed to believe the mass refugee rape fuge e immigration was good is idiodic\"</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>285</th>\n","      <td>285</td>\n","      <td>morning. twitter is being anal and trying to be a safe space for terminally offended. don't say cunt fuck bitch wanker ... cunts</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>367</th>\n","      <td>367</td>\n","      <td>\"everyone's condemning miller with uncles article, yet no one mentioning in 1st para \"\"he set foot on ellis island on january 7, 1903, \"\" he came &amp; sent for family legally! big differences than people overstaying visas, lying to get in, and crashing borders!#sendthemback\"</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>438</th>\n","      <td>438</td>\n","      <td>\"#navy rescued manchester bomber salman abedi from war-torn libya three years before deadly terror attack via west is \"\"rescuing\"\" people that will kill them.#sendthemhome bombing attack have some dignity and throw them out\"</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>490</th>\n","      <td>490</td>\n","      <td>there are no innocent people in detention centres send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>543</th>\n","      <td>543</td>\n","      <td>\"please don't call it \"\"rescue\"\" - it's human trafficking ports closed send them back benefit seekers</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>741</th>\n","      <td>741</td>\n","      <td>pouting face ragnar, the polygamous bitch ass cunt</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>879</th>\n","      <td>879</td>\n","      <td>1. never said you were taught. 2. you called me bitch first. 3. i don't fuckin</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>898</th>\n","      <td>898</td>\n","      <td>animals. pure and simple. they are not to blame for where they were born or how they were raised but, as adult men who have escaped their homeland, they alone are responsible for how they act during each &amp; every moment after crossing the border. period. send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1002</th>\n","      <td>1002</td>\n","      <td>by god there is a lot of woman and children got off that boat eh,poor holiday makers paying good money to go on holiday to witness that round them up send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1272</th>\n","      <td>1272</td>\n","      <td>some separated children in us opt to join their deported parents - but if they didn't come in through the front door, they shouldn't have a choice. send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1317</th>\n","      <td>1317</td>\n","      <td>yooo y'all remember when tyler said \"rape a pregnant bitch and tell my friends i had a 3sum\" now he's singing about calling him lol growth</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1461</th>\n","      <td>1461</td>\n","      <td>whoever has said zouis is dead or has gave me any shit for believing in zouis you can choke bitch</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1465</th>\n","      <td>1465</td>\n","      <td>there will be a cronulla roits 2 in melbourne. we are sick of it send them home</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1557</th>\n","      <td>1557</td>\n","      <td>t went from \"rape a pregnant bitch and tell my friends i had a threesome\" to singing about gardens and love wow the character growth</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1628</th>\n","      <td>1628</td>\n","      <td>do you think beyonc is nice or a bitch a little bit of both, depends on the time wonhoe w...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1941</th>\n","      <td>1941</td>\n","      <td>every man cheat and every bitch lie. call it even lil lil lil whore blac young st avc</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>1996</td>\n","      <td>8. i remember live dming u dwp. ur a g and i hope yr 11 treats u right. remember my adivce u bitch ass cunt. ilysm!!</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2200</th>\n","      <td>2200</td>\n","      <td>. says dreamers represent the best of what america has to offer 60k with arrest records not our bestnote to kamala harris uscis director cissna~you could be arrested a whole lot of times and still get daca#nodaca</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2381</th>\n","      <td>2381</td>\n","      <td>send them home no daca they are returning to their home countries educated, healthy &amp; multilingual</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2470</th>\n","      <td>2470</td>\n","      <td>idgaf what the fuck y'all got going on. he ain't my problem no more bitch. ok hand#iknowyoulurkinghoe</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2524</th>\n","      <td>2524</td>\n","      <td>i'm a happy bitch as long as i have the rainbond around and can drag hyunyoung whoever i want to.</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2531</th>\n","      <td>2531</td>\n","      <td>a law broken is still a law, family or not send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2550</th>\n","      <td>2550</td>\n","      <td>katara u dumb retarded cunt bitch he is an actual human being with actual feelings</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2911</th>\n","      <td>2911</td>\n","      <td>fiona a whore, sammi a psycho annoying bitch, carl a wanna be, debbie a try hard, mickey is actually a sweetheart, and frank a scammer</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2922</th>\n","      <td>2922</td>\n","      <td>. republicans in congress dont want to see another republican impeached &amp; on secret recording said this all goes away if the republicans dont keep the house nunes tape shows plot to fire rosenstein vote blue no matter who send them home</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2970</th>\n","      <td>2970</td>\n","      <td>likes to call me a cunt &amp; a bitch but i tell him the truth about himself and he can't handle it</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2993</th>\n","      <td>2993</td>\n","      <td>dumb fag ass prick cunt bitch</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2998</th>\n","      <td>2998</td>\n","      <td>everyone wanna be carefree, a hoe, spiritual, allladat summer 17' like it's new face with rolling eyes face with rolling eyes bitch, find yourself and stop following the hashtags</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        id  ... predictions\n","17    17    ...  0         \n","276   276   ...  0         \n","282   282   ...  0         \n","285   285   ...  0         \n","367   367   ...  0         \n","438   438   ...  0         \n","490   490   ...  0         \n","543   543   ...  0         \n","741   741   ...  0         \n","879   879   ...  0         \n","898   898   ...  0         \n","1002  1002  ...  0         \n","1272  1272  ...  0         \n","1317  1317  ...  0         \n","1461  1461  ...  0         \n","1465  1465  ...  0         \n","1557  1557  ...  0         \n","1628  1628  ...  0         \n","1941  1941  ...  0         \n","1996  1996  ...  0         \n","2200  2200  ...  0         \n","2381  2381  ...  0         \n","2470  2470  ...  0         \n","2524  2524  ...  0         \n","2531  2531  ...  0         \n","2550  2550  ...  0         \n","2911  2911  ...  0         \n","2922  2922  ...  0         \n","2970  2970  ...  0         \n","2993  2993  ...  0         \n","2998  2998  ...  0         \n","\n","[31 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"-kH3pri2hFlU","colab_type":"code","outputId":"1474d146-7549-4eef-f877-f96178edbb39","executionInfo":{"status":"ok","timestamp":1585325350673,"user_tz":0,"elapsed":1293,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["if DATASET == \"HatEval\":\n","  df = get_misclassified_HS(test)\n","  df.to_csv('gs://csc3002/hateval2019/falsepositives.csv', sep=',',  index = True, encoding = 'utf-8')\n","df.head(50)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","      <th>predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>anyone whoever doubted louis &amp; said he couldnt sing go listen to back to you &amp; if u dont change ur mind ur just a bitter bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>this is nancynancy called my pay raise crumbs nancy doesn't want to fund the military nancy puts illegal aliens rights before citizen rights.nancy wants to house the illegals before our homeless veterans don't be a nancy. trump wednesday wisdom maga</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>well bitch tell me how you download viss</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>your shoes are ugly and anarcho primitivism sucks, bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>this is a pretty sight build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>what is the thinking behind veterans against trump ?#potus has reformed &amp; improved the va, while hundreds died during obama reign waiting for treatment.they can think what they want, but let's be rational.#maga kag potus trump news vote red no daca walk away red wave</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>y'all: \"you a hoe\" hoe me: \"that's hoe culture to you bitch\"</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>i have an idea. build that wall detention centers would be reduced dramatically if not eliminated. problem solved.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>bitch you the one that said you wanted to play</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>30</td>\n","      <td>my nail girl was hyping me up the whole time \"i love your hair\" \"i love your eyelashes\" \"i love your outfit\" like yass bitch, here's ya tip</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>33</td>\n","      <td>a nigga that beat thou women is a bitch hundred points</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>34</td>\n","      <td>i'm not out here entertaining dudes bout shit. 1. i'm pregnant with a whole ass baby. 2. what kinda dirty trick ass bitch u take me for?</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>35</td>\n","      <td>my mom called me a shoe whore and it's true &lt;/3 lol</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>39</td>\n","      <td>mann fuck a bitch , fuck feelings middle finger .yall hoes sneaky litter in bin sign. i hate all you bitches exclamation mark. wish you bitches die relieved face ..goodnight smiling face with heart-eyes key red heart</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>40</td>\n","      <td>ok i hate to be like that bitch but if your in the us how do you buy the shoes?</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>41</td>\n","      <td>taehyung prolly out there like wow did this bitch really just call my beautiful silky $3000 striped navy b</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>42</td>\n","      <td>they couldn't deal with his hair. too vegas bitch. vegas whore teas were spilt.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>43</td>\n","      <td>i want them and to fight you bc a bitch can't even buy those shoes in america</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>45</td>\n","      <td>like he ever kept out any threats. he's lying as usual. build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>46</td>\n","      <td>the list of things i wish had done at the beginning of his term is long: travel ban end daca end tps interior enforcement endh4ead h-1b reform, termination of the interim waiver of admissability, etc.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>48</td>\n","      <td>build that wall ted cruz has a plan on how to help republicans win big in november!</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>50</td>\n","      <td>17-year-old charged in killing of teenager in annapolis linked to ms-13 no amnesty no daca deport them all these are not anomalies this is becoming the norm.immigration reform does not entail giving a free pass!</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>52</td>\n","      <td>toe up ass hoe stop worrying bout me and get them roaches out yo baby bed punk bitch speaking headyou wan me clean yo house for you??</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>53</td>\n","      <td>they can scrim whoever they fucking want this isn't a fucking chall you dumb bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>56</td>\n","      <td>bitch you were supposed to be home 30mins ago you fat hoe</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>57</td>\n","      <td>u have no right to judge which women are better you bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>59</th>\n","      <td>59</td>\n","      <td>it's kind of funny, really. were the space aliens illegal? now we need a wall &amp; a roof! i think this country has gone totally nuts. god bless for his strength &amp; courage. build that wall and if he blocks the democrats out, that's okay with me!</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>64</td>\n","      <td>i'd just like to know why this turned into a whole ass thread sksksksks bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>67</th>\n","      <td>67</td>\n","      <td>how did people respond to tweets today?tuesday 11 sep 2018 16:00:32 utctop hashtags:#neverforget#september11th#911memorial#buildthatwall#retweet#trumpmost influential accounts:</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>70</th>\n","      <td>70</td>\n","      <td>how's everything going with the bills to stop immigration</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>71</th>\n","      <td>71</td>\n","      <td>same shoes everyday but bitch i'm still swaggin'</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>72</th>\n","      <td>72</td>\n","      <td>emotional manipulation is at the forefront of the migrant situation -- of course feel for the victims of violence in the northern triangle. the perpetrators know this and use it to their advantage for financial and selfish gain. open borders, big business. build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>74</th>\n","      <td>74</td>\n","      <td>fuck whoever drank 2 of my teas. fuck you. and your fucking family. and i wish bad juju karma on you. because you're a mf cunt waffle.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>76</th>\n","      <td>76</td>\n","      <td>i'm saving money rn to fuckin buy those puma shoes bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>79</th>\n","      <td>79</td>\n","      <td>niggas be eating sleep for dinner cause he feeding a bitch that aint letting him in the same vicinity as the box, smh</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>80</th>\n","      <td>80</td>\n","      <td>bitch i love you with my whole heart ur my fave person ever thank u red heart</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>81</th>\n","      <td>81</td>\n","      <td>u poor deluded fool. like those 2016 polls with all ur illegal aliens voting on polls,the ones u &amp; ur corrupt dems paid to win hc the election but lost.we're awake &amp; ur the ancient history.</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>82</th>\n","      <td>82</td>\n","      <td>understood this in ohio. now in florida. cant wait to vote in desantis. his opponent is a socialist commie. maga no daca how do you like us now</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>83</th>\n","      <td>83</td>\n","      <td>the usa has a major debt problem and thus cannot afford big tax cuts for the wealthy, big globalist corporations, rinos, &amp; wall street. usa americans america pj net end daca no amnesty congress</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>84</td>\n","      <td>westminster terror attack suspect named as 'sudanese rapefugee who drove around london looking for targets' before driving car into cyclists</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>85</th>\n","      <td>85</td>\n","      <td>me: these shoes look scary me to me: you're a prison psychologist, suck it up, bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>86</th>\n","      <td>86</td>\n","      <td>only 9%?!? thats shameful! it should be 900%!!! no amnesty no daca</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>88</th>\n","      <td>88</td>\n","      <td>50centfrosty films look at my half dollar popsicle, bitch.... grape</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>91</th>\n","      <td>91</td>\n","      <td>period boobs got you feelin like a whole new bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>92</th>\n","      <td>92</td>\n","      <td>thanks for allowing violent foreign men to walk among us &amp; threaten us because of law/court system favours them over decent law abiding people. civil war is looming in this country as you do nothing to help or protect the people. send them back</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>93</th>\n","      <td>93</td>\n","      <td>plan to whore out my sexy bitch tomorrow night and look for hot tops. here is the funniest response i get face with tears of joy</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>96</td>\n","      <td>you killed tht bitch. i'm a drake stan and this hoe sonically better than more life fire i knew when u ran up on faree old u was back</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>99</td>\n","      <td>this refers 2 a favorable political effort by google 2 dem party's pres. candidate - they were surprised that \"sure-fire\" voters they gave rides 2 didn't all vote 4 their woman! they expected it, like the blacks used 2 do, but began 2 wake up. next up? muslims!#stoptheinvasion</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>101</th>\n","      <td>101</td>\n","      <td>ingraham angle so right dropping the ball and huge mistake to approve any spending before build that wall funding</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>103</th>\n","      <td>103</td>\n","      <td>take your shoes off when you enter my home. bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id  ... predictions\n","0    0    ...  1         \n","2    2    ...  1         \n","3    3    ...  1         \n","10   10   ...  1         \n","11   11   ...  1         \n","21   21   ...  1         \n","22   22   ...  1         \n","25   25   ...  1         \n","28   28   ...  1         \n","30   30   ...  1         \n","33   33   ...  1         \n","34   34   ...  1         \n","35   35   ...  1         \n","39   39   ...  1         \n","40   40   ...  1         \n","41   41   ...  1         \n","42   42   ...  1         \n","43   43   ...  1         \n","45   45   ...  1         \n","46   46   ...  1         \n","48   48   ...  1         \n","50   50   ...  1         \n","52   52   ...  1         \n","53   53   ...  1         \n","56   56   ...  1         \n","57   57   ...  1         \n","59   59   ...  1         \n","64   64   ...  1         \n","67   67   ...  1         \n","70   70   ...  1         \n","71   71   ...  1         \n","72   72   ...  1         \n","74   74   ...  1         \n","76   76   ...  1         \n","79   79   ...  1         \n","80   80   ...  1         \n","81   81   ...  1         \n","82   82   ...  1         \n","83   83   ...  1         \n","84   84   ...  1         \n","85   85   ...  1         \n","86   86   ...  1         \n","88   88   ...  1         \n","91   91   ...  1         \n","92   92   ...  1         \n","93   93   ...  1         \n","96   96   ...  1         \n","99   99   ...  1         \n","101  101  ...  1         \n","103  103  ...  1         \n","\n","[50 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"2sC42do8vqZV","colab_type":"code","outputId":"a31440ae-c888-4344-9ab6-8fd8a0119cb3","executionInfo":{"status":"ok","timestamp":1585325196680,"user_tz":0,"elapsed":4645,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":572}},"source":["df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","      <th>predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>anyone whoever doubted louis &amp; said he couldnt sing go listen to back to you &amp; if u dont change ur mind ur just a bitter bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>this is nancynancy called my pay raise crumbs nancy doesn't want to fund the military nancy puts illegal aliens rights before citizen rights.nancy wants to house the illegals before our homeless veterans don't be a nancy. trump wednesday wisdom maga</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>well bitch tell me how you download viss</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>your shoes are ugly and anarcho primitivism sucks, bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>this is a pretty sight build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2986</th>\n","      <td>2986</td>\n","      <td>i hope you are not offering them a choice. to hell with the tories send them home</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2988</th>\n","      <td>2988</td>\n","      <td>illegal alien is the correct term. there are no illegal immigrants. there are only legal immigrants. if you snuck into the usa then you are a criminal illegal alien. illegal aliens illegal immigration build the wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2990</th>\n","      <td>2990</td>\n","      <td>amen!finally, we have a puts americans first &amp; our veterans first not a few really rich kneelers!#buildthatwall boycott nike trump2020 trump train trump army trump ville america first</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2991</th>\n","      <td>2991</td>\n","      <td>trump: pentagon will build wall!trump says he could use the military to build his wall if congress won't fund it through homeland security's budget build that wall build the wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2995</th>\n","      <td>2995</td>\n","      <td>bitch bts already burned the whole universe with not today what do u mean hot but even better is there anything left to burn</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1369 rows × 4 columns</p>\n","</div>"],"text/plain":["        id  ... predictions\n","0     0     ...  1         \n","2     2     ...  1         \n","3     3     ...  1         \n","10    10    ...  1         \n","11    11    ...  1         \n","...   ..    ... ..         \n","2986  2986  ...  1         \n","2988  2988  ...  1         \n","2990  2990  ...  1         \n","2991  2991  ...  1         \n","2995  2995  ...  1         \n","\n","[1369 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"fpDsa8ejf14-","colab_type":"text"},"source":["# Using Tensorboard to Get Deeper Insight"]},{"cell_type":"code","metadata":{"id":"x3yg60YrO-Ub","colab_type":"code","outputId":"9b5458df-1e8f-4ee7-fc5f-33451376938b","executionInfo":{"status":"ok","timestamp":1581533997663,"user_tz":0,"elapsed":16002,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip   #Downloads file to google drive"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-02-12 18:59:42--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 34.233.35.85, 3.229.196.117, 34.193.139.214, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|34.233.35.85|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13773305 (13M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n","\n","ngrok-stable-linux- 100%[===================>]  13.13M  34.5MB/s    in 0.4s    \n","\n","2020-02-12 18:59:43 (34.5 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13773305/13773305]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y97-_YvVXeIy","colab_type":"code","outputId":"ffe1589a-0c5e-41b9-91a9-37673560dd33","executionInfo":{"status":"ok","timestamp":1581701877698,"user_tz":0,"elapsed":2183,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDQ4Te6RRh1B7ENukR6-4xoNAaV2ajfGgNpCD6y=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":217}},"source":["def get_tensorboard(path_to_event_file = OUTPUT_DIR):\n","  get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 --reload_multifile=true &'\n",".format(path_to_event_file))\n","  \n","  get_ipython().system_raw('./ngrok http 6006 &')\n","\n","  !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","      \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n","\n","get_tensorboard(OUTPUT_DIR)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"<string>\", line 1, in <module>\n","  File \"/usr/lib/python3.6/json/__init__.py\", line 299, in load\n","    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n","  File \"/usr/lib/python3.6/json/__init__.py\", line 354, in loads\n","    return _default_decoder.decode(s)\n","  File \"/usr/lib/python3.6/json/decoder.py\", line 339, in decode\n","    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n","  File \"/usr/lib/python3.6/json/decoder.py\", line 357, in raw_decode\n","    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n","json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"],"name":"stdout"}]}]}