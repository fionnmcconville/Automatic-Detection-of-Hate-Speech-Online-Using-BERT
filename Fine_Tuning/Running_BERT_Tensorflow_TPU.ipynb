{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Running_BERT_Tensorflow_TPU.ipynb","provenance":[{"file_id":"1EBH_dCvSIfTM-VlKeBfV2Em3exIJzkih","timestamp":1576806467871},{"file_id":"1JuDopUfWUBPOSoIOBuqs7S2tH2FJHokl","timestamp":1575402454368},{"file_id":"1VNukx0WgDZ6kdgs4gvDj6zZ5czaNCAO6","timestamp":1575299325999},{"file_id":"https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb","timestamp":1574879594425}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"j0a4mTk9o1Qg","colab_type":"code","colab":{}},"source":["# Copyright 2019 Google Inc.\n","\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Q4eUHbzUO-c","colab_type":"text"},"source":["<b>Open in colab for full functionality (if not already open in colab) </b>\n","\n","<a href=\"https://drive.google.com/file/d/1PUEEOCedG7BHVYfxG2t5cwwf0cUNij93/view?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> "]},{"cell_type":"markdown","metadata":{"id":"dCpvgG0vwXAZ","colab_type":"text"},"source":["# Detecting Hate Speech Tweets With BERT"]},{"cell_type":"markdown","metadata":{"id":"xiYrZKaHwV81","colab_type":"text"},"source":["We are using bert-tensorflow for this classification task. At the moment I'm making sure it's tensorflow version 1.x because tensorflow version 2 gives issues with Bert at the moment. I believe Tensorflow hopes to have this issue resolved in tensorflow v 2.1\n","\n","We are using a TPU as a GPU does not have the required memory for Large BERT models- it can only cope with the base model. We'll see if there a TPU detected and we'll set it to a global environment variable so it can be accessed by our BERT functions later."]},{"cell_type":"code","metadata":{"id":"mYEcG2vlPumC","colab_type":"code","outputId":"8f95e0a0-8e20-493c-ffdd-3e0bf79ea18a","executionInfo":{"status":"ok","timestamp":1588513436149,"user_tz":-60,"elapsed":79350,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":904}},"source":["!pip install gcsfs \n","import pandas as pd\n","import numpy as np\n","\n","#Make sure to use tensorflow version 1.x, version 2 doesn't work with bert\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","#!pip install gast==0.2.2\n","import os\n","\n","#For cross-validation and grid search\n","from itertools import product\n","from google.cloud import storage\n","from IPython.display import display\n","\n","import sklearn\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from sklearn import metrics\n","\n","\n","import html\n","import re\n","import json\n","import pprint\n","import random\n","import string\n","import nltk\n","from datetime import datetime\n","import time\n","\n","assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n","TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","print('TPU address is', TPU_ADDRESS)\n","\n","#Below we give ourselves as well as the TPU access to our private GCS bucket\n","from google.colab import auth\n","auth.authenticate_user()\n","tf.reset_default_graph()  \n","with tf.Session(TPU_ADDRESS) as session:\n","  # Upload credentials to TPU.\n","  with open('/content/adc.json', 'r') as f:\n","    auth_info = json.load(f)\n","  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n","\n","USE_TPU=True\n","try:\n","  #tf.config.experimental_connect_to_host(TPU_ADDRESS)\n","  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n","  tf.config.experimental_connect_to_cluster(cluster_resolver)\n","  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n","  tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n","except Exception as ex:\n","  print(ex)\n","  USE_TPU=False\n","\n","print(\"        USE_TPU:\", USE_TPU)\n","print(\"Eager Execution:\", tf.executing_eagerly())\n","\n","assert not tf.executing_eagerly(), \"Eager execution on TPUs have issues currently\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting gcsfs\n","  Downloading https://files.pythonhosted.org/packages/18/3b/454be7c97d05e15eb20a0099f425f0ed6b7552e352c77adb923c3872ba14/gcsfs-0.6.1-py2.py3-none-any.whl\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.2)\n","Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.7.2)\n","Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.23.0)\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.7.3)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (3.1.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (46.1.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2020.4.5.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n","Installing collected packages: gcsfs\n","Successfully installed gcsfs-0.6.1\n","TensorFlow 1.x selected.\n","TPU address is grpc://10.24.148.34:8470\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","INFO:tensorflow:Initializing the TPU system: 10.24.148.34:8470\n","INFO:tensorflow:Finished initializing TPU system.\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.24.148.34:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 8683200615022957186)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9226226371191721150)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2160041704481069622)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3637750446220270026)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1901758220501687172)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 1300034293481789848)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 8208820991044687334)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15920630802117453453)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15400997574767701873)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16177985237882721680)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 4461409481405910974)\n","        USE_TPU: True\n","Eager Execution: False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sRh9YYYfgUTE","colab_type":"text"},"source":["Setting a random seed so we can attempt reproducability of results.\n","\n","Also checking version of tensorflow"]},{"cell_type":"code","metadata":{"id":"dgVteKeTgXrg","colab_type":"code","outputId":"698ed04b-64f4-4f19-cb65-82ed61149ace","executionInfo":{"status":"ok","timestamp":1588513436150,"user_tz":-60,"elapsed":79331,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Setting the graph-level random seed for the default graph. Different than operation level seed\n","SEED = 3060\n","tf.reset_default_graph()\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","tf.set_random_seed(SEED) \n","random.seed(SEED)\n","np.random.seed(SEED)\n","print(\"Tensorflow Version:\", tf.__version__)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Tensorflow Version: 1.15.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KVB3eOcjxxm1","colab_type":"text"},"source":["Below we will set the directory where we will store our output model. To ensure the right variables are loaded in our run config function later, our output directory must be in the same directory as our pre-trained bert model directory.\n","\n","Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."]},{"cell_type":"code","metadata":{"id":"US_EAnICvP7f","colab_type":"code","cellView":"form","outputId":"bebeaffa-479c-4b7a-f81a-bee35c8dd6b3","executionInfo":{"status":"ok","timestamp":1588513440851,"user_tz":-60,"elapsed":84010,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["#Large whole word masking BERT pre-trained weights\n","bert_model_name = 'wwm_uncased_L-24_H-1024_A-16' \n","\n","#Where we output the fine tuned model\n","output_dir = os.path.join(bert_model_name, 'output1')\n","\n","DATASET = \"HatEval\" #@param [\"HatEval\", \"AnalyticsVidhya\", \"Custom_HS\", \"Custom_OFF\"]\n","\n","#@markdown Whether or not to use the further pretrained model\n","FURTHER_PRETRAINED = True #@param {type:\"boolean\"}\n","if FURTHER_PRETRAINED == True:\n","\n","  further_pretrained_model = os.path.join(bert_model_name, 'further_pretrained_model1')\n","  output_dir = os.path.join(further_pretrained_model, 'output1')#output1\n","\n","#@markdown Whether or not to clear/delete the directory and create a new one\n","DO_DELETE = True #@param {type:\"boolean\"}\n","#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","USE_BUCKET = True #@param {type:\"boolean\"}\n","BUCKET = 'csc3002' #@param {type:\"string\"}\n","os.environ[\"GCLOUD_PROJECT\"] = \"csc3002\"\n","\n","if USE_BUCKET:\n","  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, output_dir)\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","if DO_DELETE:\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","    # Doesn't matter if the directory didn't exist\n","    pass\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["***** Model output directory: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1 *****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yDFCBYMB-m3v","colab_type":"text"},"source":["<b> If you're not connected to a TPU environment but still want to access GCS bucket - run below: </b>"]},{"cell_type":"code","metadata":{"id":"65aCrle2Bmmi","colab_type":"code","outputId":"bbb7ca04-276a-4f1f-bf18-f31da2f028e4","executionInfo":{"status":"ok","timestamp":1588513440852,"user_tz":-60,"elapsed":83987,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["\"\"\"from google.colab import drive\n","drive.mount('/content/drive')\n","!gcloud auth activate-service-account --key-file '/content/drive/My Drive/storageCreds.json'\n","\"\"\""],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"from google.colab import drive\\ndrive.mount('/content/drive')\\n!gcloud auth activate-service-account --key-file '/content/drive/My Drive/storageCreds.json'\\n\""]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"UmeFZJIvTy33","colab_type":"text"},"source":["<b>Setting up Data Based Upon Choice of DATASET</b>\n"]},{"cell_type":"code","metadata":{"id":"jhIIdbOETxC4","colab_type":"code","colab":{}},"source":["if DATASET == 'HatEval':\n","  dirc = 'gs://csc3002/hateval2019'\n","\n","  rawTrain = pd.read_csv(os.path.join(dirc, 'hateval2019_en_train.csv'), sep=',',  index_col = False, encoding = 'utf-8')\n","  rawTrain.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","  rawTrain.drop(['TR', 'AG'], inplace = True, axis = 1)\n","\n","  rawDev = pd.read_csv(os.path.join(dirc, 'hateval2019_en_dev.csv'), sep=',',  index_col = False, encoding = 'utf-8')\n","  rawDev.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","  rawDev.drop(['TR', 'AG'], inplace = True, axis = 1)\n","\n","  rawTest = pd.read_csv(os.path.join(dirc, 'hateval2019_en_test.csv'), sep=',',  index_col = False, encoding = 'utf-8')\n","  rawTest.rename(columns={'text': 'tweet', 'HS': 'label'}, inplace=True)\n","  rawTest.drop(['TR', 'AG'], inplace = True, axis = 1)\n","\n","elif DATASET == \"AnalyticsVidhya\":\n","  dirc = 'gs://csc3002/trial'\n","\n","  rawTrain= pd.read_csv(os.path.join(dirc, 'train_E6oV3lV.csv'),  sep=',',  index_col = False, encoding = 'utf-8')\n","  \n","  #Make sure it's a stratified sample so evaluation is truly representative\n","  rawTrain, rawDev = train_test_split(rawTrain, test_size=0.20, random_state=SEED, stratify = rawTrain.label)\n","\n","  rawTest = pd.read_csv(os.path.join(dirc, 'test_tweets_anuFYb8.csv'), sep=',',  index_col = False, encoding = 'utf-8')\n","\n","#Classifying Hate Speech in Dataset constructed in the CSC3002-Hate_Speech_Detection_Assembling_and_Cleaning_the_Fine_Tuning_Data.ipynb notebook\n","elif DATASET == 'Custom_HS': \n","\n","  rawTrain= pd.read_csv('gs://csc3002/Raw_Data/final.csv',  sep=',',  index_col = False, encoding = 'utf-8')\n","  rawTrain.rename(columns={'Tweet': 'tweet', 'Hate_Speech': 'label'}, inplace=True)\n","  rawTrain['label'] = rawTrain['label'].astype(int)\n","  rawTrain, rawTest = train_test_split(rawTrain, test_size=0.20, random_state=SEED, stratify = rawTrain.label)\n","\n","#Classifying Offensive Speech in Dataset constructed in the CSC3002-Hate_Speech_Detection_Assembling_and_Cleaning_the_Fine_Tuning_Data.ipynb notebook\n","elif DATASET == 'Custom_OFF':\n","  rawTrain= pd.read_csv('gs://csc3002/Raw_Data/final.csv',  sep=',',  index_col = False, encoding = 'utf-8')\n","  rawTrain.rename(columns={'Tweet': 'tweet', 'Offensive': 'label'}, inplace=True)\n","  rawTrain = rawTrain[rawTrain.label != '-']\n","  rawTrain['label'] = rawTrain['label'].astype(int)\n","  rawTrain, rawTest = train_test_split(rawTrain, test_size=0.20, random_state=SEED, stratify = rawTrain.label)\n","\n","else:\n","  raise ValueError('No Valid DATASET chosen')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pmFYvkylMwXn","colab_type":"text"},"source":["# Training Data\n","I've stored all of the data, (train, dev and test),  in my google bucket for ease of access, authentication will have to be provided"]},{"cell_type":"code","metadata":{"id":"UvknR984WeDW","colab_type":"code","outputId":"609f6025-4e5d-476c-daa6-f92244ddab99","executionInfo":{"status":"ok","timestamp":1588513447647,"user_tz":-60,"elapsed":90752,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["!gcloud config set project 'my-project-csc3002'\n","\n","train = rawTrain.sample(frac=1, random_state = SEED) #Shuffling really helps model performance\n","train.reset_index(drop = True, inplace = True)\n","pd.set_option('display.max_colwidth', None)\n","if DATASET == 'Custom_OFF':\n","  classification_type = 'offensive'\n","else:\n","  classification_type = 'hate'\n","print(\"Out of {} tweets in this database, {} are not {}, {} are {}\".format(len(train.index),\n","                                                      len(train[train['label']==0]), classification_type,\n","                                                      len(train[train['label']==1]), classification_type,))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Updated property [core/project].\n","\n","\n","To take a quick anonymous survey, run:\n","  $ gcloud survey\n","\n","Out of 9000 tweets in this database, 5217 are not hate, 3783 are hate\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"57uQmKX8yQGq","colab_type":"text"},"source":["<b>Original Dataset </b>"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"f8f4551c-12a6-4b4d-f531-0dd3bd7a5ac0","executionInfo":{"status":"ok","timestamp":1588513447650,"user_tz":-60,"elapsed":90733,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"id":"TblezF_V2DrA","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train.head(30)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>680</td>\n","      <td>How come Allah is not helping you it is up to Christian countries to  protect you feed you ,The countries hit by violence from islam take refugees in feed them etcPlease no more explaining about your hard times we are doing our best for uYes there is good and bad every where</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3995</td>\n","      <td>With todays #JalalabadAttack &amp;amp; other vicious attacks claimed by ISIS, I smell a spillover of refugees in Pak again. This time we should not open borders for them. We cant afford terrorists taking undue advantage. Let Americans deal with it.#Afghanistan #Jalalabad</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2713</td>\n","      <td>https://t.co/i9LJDjtGz7Migration greatest threat’ to Austrian security, says top military figure.EU and Europe bitterly dividedðŸ‘‰major confrontations between the two.Nothing more counterproductive than “centers” on European territory or euro bribes for migrants.#Visegrad #V4 https://t.co/VnPCTe7opC</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7509</td>\n","      <td>When all your friends are out hoe'in and you're stuck at home in a shitty relationship https://t.co/X9oz1Tx7TC</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6384</td>\n","      <td>I wonder if rick will make another deal with those crazy ass women 🤔 and if that crazy ass nigga will actually hoe Daryl again 😐</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>628</td>\n","      <td>Worker Charged With Sexually Molesting Eight Children at Immigrant Shelter https://t.co/D6HcH03nGL via @CitizenTruth_ #realDonaldTrump do something about this disgrace and stop separating children from their parents.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>3241</td>\n","      <td>UN seeks new funding pledges for Palestinian refugees... https://t.co/SNJhD1PWxT https://t.co/DlHQ8fc5N6</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7620</td>\n","      <td>If you really wanna know what someone you're fucking thinks about you, make them show you how you're stored in their phone...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4533</td>\n","      <td>Going to make Du'a at the shrine of Imam Reza(AS) for the refugees in Athens.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>3755</td>\n","      <td>Poor kid. Someone wise must have told him, \"When the world gives you lemons, make lemonade.\" He listened. His lemonade should now be offered with ICE in abundance. #BuildTheWall #SendThemBack https://t.co/8AM7fgo9ph</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>8329</td>\n","      <td>@TVRav You got reply in before mine, but what I'm pointing out is women work their lives around being safe from *some men*, not all men</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>7123</td>\n","      <td>Her- remember when you said you loved me? Me- https://t.co/hv9WiaqXTu</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>8755</td>\n","      <td>I stand with the victims of sexual harassment or rape. No matter if they're a woman or a man. A rape is a rape what… https://t.co/PpiL6mBtqV</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1438</td>\n","      <td>Canada's @JustinTrudeau appointed to his cabinet immigrants from such cultures. One is in charge of immigration &amp;amp; refugee affairs - license to flood Canada with own kind. The other, Education Minister, tried to sneak into new Education code female genital mutilation as a right!</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>8019</td>\n","      <td>@graysonslays marcos you skank hannah montana is my thinf</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>5320</td>\n","      <td>@_miidnightr Wanna speak down to my girl bitch then step up big nose ass bitch. Saw that shit off.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>3184</td>\n","      <td>Philippines - Christians are facing starvation as they continue to hide from jihadists.  An estimate 1500 remain... https://t.co/P04dSj0nbs</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1459</td>\n","      <td>Thank you so much to @WValderrama &amp;amp; @BoomEventsLA for hosting such an amazing event this past weekend! No child should face immigration court w/o an attorney by their side &amp;amp; this event helped raise critical awareness for these immigrant &amp;amp; refugee children #KINDLA #GivingKINDness https://t.co/v3bxbNMsYx</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>229</td>\n","      <td>Could you open up your home to refugees in need? https://t.co/0F2rzmYCSw</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>502</td>\n","      <td>It's unfortunate that such racist tactics are used against migrant communities. Remember those who are spreading fear are those who cut the funding of vital services for migrants such as healthcare and education!  https://t.co/82hkXRW6L1 via @theage</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>8491</td>\n","      <td>bitch what the fuck do you mean you cant find the fucking page cunt i need to vote for my boys dont you understand https://t.co/xbdlkyXah7</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>1102</td>\n","      <td>World Refugee Day was this past Wednesday, but there is still time to make a difference for the 20,000 refugees who will arrive in the U.S. this year. Rally the support of your friends and family by becoming a fundraiser for refugees. Get started here âž_x009d_ https://t.co/QzFjSqWwJ8 https://t.co/vS9TqvpVm6</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>2795</td>\n","      <td>US Senate Calls On Julian Assange To Testify https://t.co/Qqy6GO6jm7 https://t.co/arrxfZVTdb</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>7537</td>\n","      <td>When you find a bag of drugs on the ground https://t.co/j8JcFgLCHP</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>4857</td>\n","      <td>When you're flexin on these bitches https://t.co/7fR6wozeU4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>8971</td>\n","      <td>@SenKamalaHarris So now the word hysterical is a completely sexist term? So let's get this straight nobody can say the word monkey anymore when they are describing anyting and the word hysterical can no longer be used to describe a hysterical woman?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>3747</td>\n","      <td>VIDEO: Immigrant activist who climbed Statue of Liberty has a new song: \"America, you motherfuckers,... https://t.co/YSjmys3MPK</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>7407</td>\n","      <td>@Scouse_ma hi 😘</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>3288</td>\n","      <td>Picture: the couple Engelbert and Spera, both Jews. Engelbert is now elected in the Austrian parliament for the Ã–VP. He says in the newspaper HaÃ retz the danger for Jews doesnt come from FPÃ–Nazis, more from Islamic refugees, which import antisemitism to Austria. https://t.co/PhsxnDXWxP</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>1675</td>\n","      <td>* Croatia ðŸ‡­ðŸ‡·:  The Croatian authorities, surprisingly, reject the EU’s bid of 6,000 euros per migrant.  ðŸ‘_x008f_ https://t.co/hccQmsQ9Vl #v4 #visegrad https://t.co/1x1MXOQm2t</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id  ... label\n","0    680  ...     0\n","1   3995  ...     1\n","2   2713  ...     0\n","3   7509  ...     0\n","4   6384  ...     1\n","5    628  ...     0\n","6   3241  ...     0\n","7   7620  ...     0\n","8   4533  ...     0\n","9   3755  ...     0\n","10  8329  ...     0\n","11  7123  ...     0\n","12  8755  ...     0\n","13  1438  ...     1\n","14  8019  ...     0\n","15  5320  ...     1\n","16  3184  ...     0\n","17  1459  ...     0\n","18   229  ...     0\n","19   502  ...     0\n","20  8491  ...     0\n","21  1102  ...     0\n","22  2795  ...     0\n","23  7537  ...     0\n","24  4857  ...     0\n","25  8971  ...     0\n","26  3747  ...     0\n","27  7407  ...     0\n","28  3288  ...     0\n","29  1675  ...     1\n","\n","[30 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"aRLD57l1z631","colab_type":"text"},"source":["### Text Preprocessing\n","\n","The text pre-processing for this project is detailed in the notebook `Text_Preprocessing.ipynb` in the github repo. Below is an import of the repo into the google colab workspace so I can retrieve and use these functions at convenience\n","\n","Also below is a function which loads whichever dataset I choose to load from my GCS bucket or local system. This will be useful later when I want to quickly load in data without the messy, long-winded code to go along with it."]},{"cell_type":"code","metadata":{"id":"QR1Ee-91AMgd","colab_type":"code","cellView":"form","outputId":"fe9f569a-1b36-4b00-a2ce-8da133c973ee","executionInfo":{"status":"ok","timestamp":1588513543483,"user_tz":-60,"elapsed":186547,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#@title Text Pre-Processing Options\n","HASHTAG_SEGMENTATION = True #@param {type:\"boolean\"}\n","EMOJI_REPLACEMENT = \"Replace_Emoji_v1\" #@param [\"None\", \"Replace_Emoji_v1\", \"Replace_Emoji_v2\"]\n","LEMMATIZE = False #@param {type:\"boolean\"}\n","REMOVE_STOPWORDS = False #@param {type:\"boolean\"}\n","REMOVE_PUNCTUATION = True #@param {type:\"boolean\"}\n","\n","options = [HASHTAG_SEGMENTATION, EMOJI_REPLACEMENT, LEMMATIZE, REMOVE_STOPWORDS, REMOVE_PUNCTUATION]\n","\n","!git clone https://gitlab2.eeecs.qub.ac.uk/csc3002_fionn/csc3002_detecting_hate_speech.git\n","%cd csc3002_detecting_hate_speech\n","%cd Text_Preprocessing/\n","#!ls\n","import preprocessing as pre\n","#Return to original workspace\n","%cd ../..\n","\n","train = pre.loadData(train, options = options, dataset = DATASET)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Cloning into 'csc3002_detecting_hate_speech'...\n","remote: Enumerating objects: 473, done.\u001b[K\n","remote: Counting objects: 100% (473/473), done.\u001b[K\n","remote: Compressing objects: 100% (293/293), done.\u001b[K\n","remote: Total 473 (delta 258), reused 362 (delta 170)\u001b[K\n","Receiving objects: 100% (473/473), 1.66 GiB | 24.88 MiB/s, done.\n","Resolving deltas: 100% (258/258), done.\n","Checking out files: 100% (76/76), done.\n","/content/csc3002_detecting_hate_speech\n","/content/csc3002_detecting_hate_speech/Text_Preprocessing\n","/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g020ACV22uFG","colab_type":"text"},"source":["**Cleaned tweet text dataset**"]},{"cell_type":"code","metadata":{"id":"lBP7J70v2ueo","colab_type":"code","outputId":"2cde1616-bd5a-4d59-e214-66e814b17781","executionInfo":{"status":"ok","timestamp":1588513543484,"user_tz":-60,"elapsed":186519,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train[:30]"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>479</th>\n","      <td>794</td>\n","      <td>germany claims success with program to support migrant returnees</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3794</th>\n","      <td>2648</td>\n","      <td>many people are deluded to the facts concerning the migrant crisis in europe people think that most are refugees fleeing war and destruction many think that most are syrian others believe that they are all skilled educated and civilised people just trying to find a better life</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2512</th>\n","      <td>8887</td>\n","      <td>user unless cow tipping is classed as physical assault you should shut your fucking whore mouth i hope you never show your face outside of whatever hole you went to hide in because frankly i would love to drop kick you off a fucking balcony xoxoxo</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7308</th>\n","      <td>4459</td>\n","      <td>germany moroccan migrant spits on a german woman and gets slapped in the face by a german grocery worker free speech time user israel</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6183</th>\n","      <td>4059</td>\n","      <td>user democrats and liberals are lying about illegal immigration its time we take back our country send them back indepen ce day happy 4th of juluy fourth of july happy birthday america god bless america trump maga red nation rising</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>427</th>\n","      <td>1862</td>\n","      <td>boat carrying 160 migrants sinks off northern cyprus coast 16 dead</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3040</th>\n","      <td>782</td>\n","      <td>illegal alien with long criminal record murdered man in north carolina via user build the wall enforce us laws end daca ban sanctuary cities keep americans safe</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7419</th>\n","      <td>4555</td>\n","      <td>it weirds me out how obsessed right wingers are with pedophiles it seems like they spend a lot of their days trying to figure out how and where people are fucking kids hey guys maybe check out the immigrant concentration camps you might get some hits there if you cared</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4332</th>\n","      <td>8279</td>\n","      <td>sex t from daddy my little toy which hole shall i stuff slut be right back just being carried away in a tsunami of cunt juice ocean</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3554</th>\n","      <td>1374</td>\n","      <td>china invites all un countries to use its future space station  cbc news united nations china space station</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8128</th>\n","      <td>5150</td>\n","      <td>user have a thought for the women that sick cunt raped woody allen you dumb cunt</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6922</th>\n","      <td>371</td>\n","      <td>sharia law tessa is that not tony blairs wife no y do you let islam law into the country stupid woman with stupid promises she cant keep</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8554</th>\n","      <td>5403</td>\n","      <td>be careful what you do to a good woman because you will have to deal with the bitch you created smirk</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1237</th>\n","      <td>2772</td>\n","      <td>sarah jeong is exactly why we dont need non white immigration they come over here with complete hatred for the native population thanks for redpilling america</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7818</th>\n","      <td>8753</td>\n","      <td>when a woman says shes a fan of a certain sport andor team and you berate and quiz her incessantly on playersstats just to make her feel stupid and basically to prove her wrong youre a fuckin ass hole women are allowed to enjoy sports without knowin all the specifics fuck off</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5119</th>\n","      <td>5667</td>\n","      <td>user mermaids are women therefore they suck and have to be sacrificed women suck</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2983</th>\n","      <td>6446</td>\n","      <td>user you re just a dumb little cunt who had an abortion to go be a hoe and you eat your tampons tryna be fe</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1258</th>\n","      <td>8817</td>\n","      <td>user stfu bitch you sound like a whole ass pussy bro no one cares</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>3288</td>\n","      <td>picture the couple engelbert and spera both jews engelbert is now elected in the austrian parliament for the vp he says in the newspaper ha retz the danger for jews doesnt come from fp nazis more from islamic refugees which import antisemitism to austria</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>301</th>\n","      <td>2596</td>\n","      <td>user welcome is a wish now about 60000 refugees will deported from sweden homeeurope is divided in that persons which mean the welfare is endless and the others which are afraid about local culture</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8290</th>\n","      <td>6216</td>\n","      <td>tryin to send that bitch back to her maker and if you got a daughter older than 15 ima rape her take her right there in front of you dmx</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>901</th>\n","      <td>1185</td>\n","      <td>please visit today to honor the thousands of victims killed by illegal alienswe are building a memorial wall re tweet help build that wall in their memoryfollow user and user durden</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2594</th>\n","      <td>8400</td>\n","      <td>ok not all men must leave me alone just some annoying ones if im interested in you you will know</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7336</th>\n","      <td>2687</td>\n","      <td>user stanton trudeau announces that isqra khalid is moral conscience of canada and that moslems must be at the forefront fighting white racists who dont want to be replaced by immigrants</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4656</th>\n","      <td>3698</td>\n","      <td>ghettoes are never good  locals fear new migrant anchor centers in germany will fuel tensions</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8770</th>\n","      <td>7981</td>\n","      <td>user not all men im generalising but you get the idea</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3546</th>\n","      <td>3943</td>\n","      <td>user please honor your threat to de fund planned parenthood and sanctuary cities user needs to do their job illegal aliens are criminals deport them all no amnesty build that wall</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7206</th>\n","      <td>3909</td>\n","      <td>i know i would say this but this weeks user insiders newsletter was super insightful breaks down cfta analyses new migration data examines a little known but important western sahara development and more</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3087</th>\n","      <td>7984</td>\n","      <td>someone not all men me yes you are right lee everret would not do this</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1474</th>\n","      <td>4018</td>\n","      <td>user where is the just a migrant gets citizenship after walking through the tunnel the uk poor get jailed non payment of bedroom tax</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        id  ... label\n","479    794  ...     0\n","3794  2648  ...     0\n","2512  8887  ...     1\n","7308  4459  ...     1\n","6183  4059  ...     1\n","427   1862  ...     0\n","3040   782  ...     1\n","7419  4555  ...     0\n","4332  8279  ...     0\n","3554  1374  ...     0\n","8128  5150  ...     1\n","6922   371  ...     0\n","8554  5403  ...     1\n","1237  2772  ...     1\n","7818  8753  ...     0\n","5119  5667  ...     1\n","2983  6446  ...     1\n","1258  8817  ...     1\n","28    3288  ...     0\n","301   2596  ...     0\n","8290  6216  ...     1\n","901   1185  ...     1\n","2594  8400  ...     0\n","7336  2687  ...     0\n","4656  3698  ...     0\n","8770  7981  ...     0\n","3546  3943  ...     1\n","7206  3909  ...     0\n","3087  7984  ...     0\n","1474  4018  ...     0\n","\n","[30 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"DFj9LFwT1Jsy","colab_type":"code","cellView":"form","outputId":"ce4da7c5-580c-4953-a13c-4436bf538e1b","executionInfo":{"status":"ok","timestamp":1588513543484,"user_tz":-60,"elapsed":186473,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@markdown Tick box if you wish to oversample the hate speech data to correct imbalance in the dataset\n","OVERSAMPLE = False #@param {type:\"boolean\"}\n","\n","#@markdown You can oversample hate speech or non-hate speech data\n","oversampleLabel = \"Hate\" #@param [\"Hate\", \"Not Hate\"]\n","#@markdown Choose the number of times over you want the subsample you've selected to be mulitplied.\n","multiplier = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n","#@markdown If you just want to balance the class labelling of the set, set multiplier to 1.\n","\n","#@markdown If the label is set to the majority class and multiplier = 1 then this will result in an undersample\n","\n","def oversample(train, lab = \"Hate\", multiplier = 1):\n","    \n","    neg_train = train.loc[train['label'] == 0]\n","    pos_train = train.loc[train['label'] == 1]\n","    \n","     #Whether we're sampling from hate or not hate for the term\n","    if lab == \"Hate\":\n","      aug_set = pos_train\n","    else:\n","      aug_set = neg_train\n","\n","    ids = np.arange(len(aug_set))\n","\n","    #You can multiply your set by a chosen number\n","    if multiplier > 1:\n","      choices = np.random.choice(ids, len(aug_set) * multiplier )\n","\n","    #Or you can simply match the opposite label and either undersmple of oversample\n","    else:\n","      choices = np.random.choice(ids, (len(train) - len(aug_set)))\n","\n","    aug_train = aug_set.iloc[choices]\n","\n","    size = len(train)\n","    if lab == \"Hate\":\n","      train = pd.concat([aug_train, neg_train], axis=0)\n","    else:\n","      train = pd.concat([aug_train, pos_train], axis=0)\n","    \n","    print(len(train)-size , \"added tweets\\n\")\n","    #shuffle\n","    train = train.sample(frac = 1, random_state=SEED) #Shuffle data\n","    return train\n","\n","if OVERSAMPLE == True:\n","  print(\"OVERSAMPLING DATA\")\n","  train = oversample(train,oversampleLabel, multiplier )                                         \n","print(\"Out of {} tweets in this database, {} are not {}, {} are {}\".format(len(train.index),\n","                                                      len(train[train['label']==0]), classification_type,\n","                                                      len(train[train['label']==1]), classification_type))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Out of 9000 tweets in this database, 5217 are not hate, 3783 are hate\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6uMuDR0pqChY","colab_type":"text"},"source":["Loading in dev data and specifying global variables"]},{"cell_type":"code","metadata":{"id":"IuMOGwFui4it","colab_type":"code","outputId":"51119753-3203-425d-eff1-08ba33f1c748","executionInfo":{"status":"ok","timestamp":1588513543485,"user_tz":-60,"elapsed":186459,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["dev = pre.loadData(rawDev, options = options, dataset = DATASET)\n","\n","DATA_COLUMN = 'tweet'\n","LABEL_COLUMN = 'label'\n","# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = [0, 1]\n","\n","print(\"Size of training data\", len(train.index))\n","print(\"Size of development data\", len(dev.index), '\\n')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Size of training data 9000\n","Size of development data 1000 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DO0dWxHYi_UA","colab_type":"text"},"source":["# Setting Up BERT Training "]},{"cell_type":"markdown","metadata":{"id":"HhIvI0kDXoUq","colab_type":"text"},"source":["##Custom BERT Repositroy For Custom Functionality\n","\n","Rather than using the official BERT setup, I have instead forked the BERT repo and customised it to allow for a more tailor-made approach when evaluating and fine-tuning the model\n","\n","The`create_model` function in my BERT repo has been edited to allow for multiple <b>Fine-Tuning</b> strategies. Normally, the default function from BERT simply fine-tunes a single layer that will be trained on top of BERT to adapt it to our classification problem. This strategy of using a pre-trained model, then fine-tuning it is called <b>Transfer Learning</b>.\n","\n","Also the `model_fn` method in my BERT repo provides far more detailed metrics than just accuracy and loss - which is all the default repo provides. It has metrics such as F-Score, AUC, precision and recall; so I can better analyse the performance of different models"]},{"cell_type":"code","metadata":{"id":"ACDvt5FYXvvm","colab_type":"code","outputId":"4a7a4df8-62f0-4182-8aaf-8e54c82d8bc7","executionInfo":{"status":"ok","timestamp":1588513615939,"user_tz":-60,"elapsed":779,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["%cd csc3002_detecting_hate_speech\n","%cd bert\n","import run_classifier\n","import optimization\n","import tokenization\n","import modeling\n","#Return to original workspace\n","%cd ../.."],"execution_count":13,"outputs":[{"output_type":"stream","text":["/content/csc3002_detecting_hate_speech\n","/content/csc3002_detecting_hate_speech/bert\n","WARNING:tensorflow:From /content/csc3002_detecting_hate_speech/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V399W0rqNJ-Z","colab_type":"text"},"source":["## BERT Preprocessing and Setup\n","We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n","\n","- `text_a` is the text we want to classify, which in this case, is the `tweet` field in our Dataframe. \n","- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n","- `label` is the label for our example, i.e. HS, Not HS"]},{"cell_type":"code","metadata":{"id":"p9gEt5SmM6i6","colab_type":"code","colab":{}},"source":["# Use the InputExample class from BERT's run_classifier code to create examples from the data\n","train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for book-keeping, unused in this example\n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)\n","\n","dev_InputExamples = dev.apply(lambda x: run_classifier.InputExample(guid=None, \n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCZWZtKxObjh","colab_type":"text"},"source":["Next, we need to preprocess our data so that it matches the data BERT was trained on.\n","\n","\n","1. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n","2. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n","3. Map our words to indexes using a vocab file that BERT provides\n","4. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n","5. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n","\n","Happily, we don't have to worry about most of these details. It's automated with the below inbuilt functions\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Aws4Q_SXZENG","colab_type":"text"},"source":["Below is a way to retrieve desired BERT parameters, such as it's pre-trained checkpoints and it's vocab file, from my google storage bucket where I've downloaded the uncased LARGE version of bert."]},{"cell_type":"code","metadata":{"id":"UtZavIhEaWF5","colab_type":"code","outputId":"852ec9ff-2f97-42c1-cfcc-8d4cc3ff2774","executionInfo":{"status":"ok","timestamp":1588513622175,"user_tz":-60,"elapsed":1950,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["bucket_dir = 'gs://csc3002'\n","bert_ckpt_dir = os.path.join(bucket_dir, bert_model_name)\n","\n","#For further pretrained model\n","if FURTHER_PRETRAINED:\n","  further_pretrained_model = os.path.join(bert_model_name, 'further_pretrained_model1')\n","  further_pretrained_model = os.path.join(bucket_dir, further_pretrained_model)\n","  bert_ckpt_file = tf.train.latest_checkpoint(further_pretrained_model)\n","  print(\"\\nUsing BERT checkpoint from directory:\", os.path.join(further_pretrained_model))\n","\n","else:\n","  bert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n","  print(\"\\nUsing BERT checkpoint from directory:\", bert_ckpt_dir)\n","\n","print(\"\\nBERT checkpoint file is:\", bert_ckpt_file)\n","\n","#Setting up BERT config, vocab file and tokenizer - all default from the BERT repo\n","bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")\n","vocab_file = os.path.join(bert_ckpt_dir, \"vocab1.txt\")\n","  \n","tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file)\n","\n","print(\"\\nMake sure that the function loads a checkpoint, if it doesn't an error will be thrown here\")\n","assert bert_ckpt_file is not None, \"No BERT checkpoint file loaded\"\n","\n","print(\"\\nUsing vocab file:\", vocab_file)\n","print(\"\\nBelow is an example of the BERT tokenizer in action:\")\n","tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["\n","Using BERT checkpoint from directory: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1\n","\n","BERT checkpoint file is: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/model.ckpt-80000\n","WARNING:tensorflow:From /content/csc3002_detecting_hate_speech/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","\n","Make sure that the function loads a checkpoint, if it doesn't an error will be thrown here\n","\n","Using vocab file: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/vocab1.txt\n","\n","Below is an example of the BERT tokenizer in action:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['this',\n"," 'here',\n"," \"'\",\n"," 's',\n"," 'an',\n"," 'example',\n"," 'of',\n"," 'using',\n"," 'the',\n"," 'bert',\n"," 'token',\n"," '##izer']"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"0OEzfFIt6GIc","colab_type":"text"},"source":["Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."]},{"cell_type":"code","metadata":{"id":"I_xOUO7T_k50","colab_type":"code","cellView":"form","colab":{}},"source":["# BERT is limited to 512 tokens in length\n","MAX_SEQ_LENGTH = 256 #@param {type:\"slider\", min:128, max:512, step:32}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cm8RLoJ31WLa","outputId":"7fcf4244-c819-4dd6-e94d-c99a1a0bcf34","executionInfo":{"status":"ok","timestamp":1588513629380,"user_tz":-60,"elapsed":5135,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Convert our train and dev features to InputFeatures that BERT understands.\n","train_features =  run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","dev_features = run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /content/csc3002_detecting_hate_speech/bert/run_classifier.py:913: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n","INFO:tensorflow:Writing example 0 of 9000\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] germany claims success with program to support migrant return ##ees [SEP]\n","INFO:tensorflow:input_ids: 101 2762 4447 3112 2007 2565 2000 2490 20731 2709 10285 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] many people are del ##uded to the facts concerning the migrant crisis in europe people think that most are refugees fleeing war and destruction many think that most are syrian others believe that they are all skilled educated and civil ##ised people just trying to find a better life [SEP]\n","INFO:tensorflow:input_ids: 101 2116 2111 2024 3972 13936 2000 1996 8866 7175 1996 20731 5325 1999 2885 2111 2228 2008 2087 2024 8711 14070 2162 1998 6215 2116 2228 2008 2087 2024 9042 2500 2903 2008 2027 2024 2035 10571 5161 1998 2942 5084 2111 2074 2667 2000 2424 1037 2488 2166 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] user unless cow tipping is classed as physical assault you should shut your fucking whore mouth i hope you never show your face outside of whatever hole you went to hide in because frankly i would love to drop kick you off a fucking balcony x ##ox ##ox ##o [SEP]\n","INFO:tensorflow:input_ids: 101 5310 4983 11190 25486 2003 27811 2004 3558 6101 2017 2323 3844 2115 8239 17219 2677 1045 3246 2017 2196 2265 2115 2227 2648 1997 3649 4920 2017 2253 2000 5342 1999 2138 19597 1045 2052 2293 2000 4530 5926 2017 2125 1037 8239 11673 1060 11636 11636 2080 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] germany moroccan migrant spit ##s on a german woman and gets slapped in the face by a german grocery worker free speech time user israel [SEP]\n","INFO:tensorflow:input_ids: 101 2762 17494 20731 13183 2015 2006 1037 2446 2450 1998 4152 11159 1999 1996 2227 2011 1037 2446 13025 7309 2489 4613 2051 5310 3956 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] user democrats and liberals are lying about illegal immigration its time we take back our country send them back ind ##ep ##en ce day happy 4th of jul ##uy fourth of july happy birthday america god bless america trump mag ##a red nation rising [SEP]\n","INFO:tensorflow:input_ids: 101 5310 8037 1998 13350 2024 4688 2055 6206 7521 2049 2051 2057 2202 2067 2256 2406 4604 2068 2067 27427 13699 2368 8292 2154 3407 4343 1997 21650 26230 2959 1997 2251 3407 5798 2637 2643 19994 2637 8398 23848 2050 2417 3842 4803 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:Writing example 0 of 1000\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] multuser fight russia now you pussy pot ##us [SEP]\n","INFO:tensorflow:input_ids: 101 998 2954 3607 2085 2017 22418 8962 2271 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] hai ##nes city fl ##a wil ##iba ##ld ##o garcia an illegal alien from mexico charged with sexual battery on an 8 y ##r old girl ##gar ##cia has an alias lorenzo sip ##rian ##o with prior criminal history ms ##m silence build that wall [SEP]\n","INFO:tensorflow:input_ids: 101 15030 5267 2103 13109 2050 19863 18410 6392 2080 7439 2019 6206 7344 2013 3290 5338 2007 4424 6046 2006 2019 1022 1061 2099 2214 2611 6843 7405 2038 2019 14593 12484 10668 6862 2080 2007 3188 4735 2381 5796 2213 4223 3857 2008 2813 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] user study con ##fl ##ants illegal immigrants with legal immigrants to get a low crime number ##r trump mag ##a red nation rising red hen [SEP]\n","INFO:tensorflow:input_ids: 101 5310 2817 9530 10258 11390 6206 7489 2007 3423 7489 2000 2131 1037 2659 4126 2193 2099 8398 23848 2050 2417 3842 4803 2417 21863 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] multuser already won most hysterical woman with her 2 day twitter freak ##out over leg room on a plane [SEP]\n","INFO:tensorflow:input_ids: 101 998 2525 2180 2087 25614 2450 2007 2014 1016 2154 10474 11576 5833 2058 4190 2282 2006 1037 4946 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] user southern your ##e just a ska ##nk and everyone knows it otherwise you wouldn ##t be making excuses for not settling down whilst hanging out with multiple different guys [SEP]\n","INFO:tensorflow:input_ids: 101 5310 2670 2115 2063 2074 1037 24053 8950 1998 3071 4282 2009 4728 2017 2876 2102 2022 2437 21917 2005 2025 9853 2091 5819 5689 2041 2007 3674 2367 4364 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N3xwnxcqhLjA","colab_type":"text"},"source":["##Fine-Tuning Model\n","\n","I'm using Tensorflow estimators as they're easier to use with TPU disributed training. Also the BERT tutorials have been demonstrated using estimators.\n","\n","Below we load in our `RUN_CONFIG` for fine tuning - where we define our distribution strategy, how often we get summary metrics and how often we checkpoint. Our parameters for BERT Fine-Tuning - defined below, and our model function - as defined in our custom BERT repositry."]},{"cell_type":"code","metadata":{"id":"1J5F87pQKD3I","colab_type":"code","cellView":"form","colab":{}},"source":["#Set below to a high value if you do not wish to checkpoint model while training. The train_and_evaluate function below will checkpoint at every evaluation regardless\n","SAVE_CHECKPOINTS_STEPS = 100000 \n","#@markdown Summary steps gives us an idea of how the model is performing by returning average loss every nth step. Changing this does not effect performance, however there is overhead if this value is too small.\n","SUMMARY_STEPS =  100#@param {type:\"number\"}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QPYzNbvcx94h","colab_type":"code","outputId":"8628df70-327f-4793-d911-e9a7d43f2373","executionInfo":{"status":"ok","timestamp":1588513629386,"user_tz":-60,"elapsed":1883,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["RUN_CONFIG = tf.compat.v1.estimator.tpu.RunConfig(  \n","    #I think the output file must be a sub-directory of the main BERT file\n","    model_dir=OUTPUT_DIR, \n","    tf_random_seed=SEED,\n","    cluster=cluster_resolver,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=SUMMARY_STEPS,    #Shows us summary metrics every 100 steps\n","        num_shards=8,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","print(RUN_CONFIG.session_config)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.24.148.34:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ALgKgxiFMX5C","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Fine-Tuning Options\n","#@markdown Toggle options for fine-tuning model below\n","FT_MODEL = \"Default\" #@param ['Default', 'Multi-Layer Perceptron', 'BiLTSM']\n","#@markdown If oversampling, weighted loss is not necessary. Weights can be class weights or custom, edit within cell to chooose\n","LOSS_FN = \"weighted_loss\" #@param  ['Default','focal_loss','weighted_loss']\n","WEIGHTED_BIAS = False \n","#@markdown Normalise the pooled [CLS] token for text classification?\n","NORMALISE_EMBEDDINGS = False #@param {type:\"boolean\"}\n","#@markdown Dropout probability of hidden neurons in the fine-tuning stage\n","DROPOUT = 0.1 #@param {type:\"slider\", min:0.1, max:0.6, step:0.05}\n","TRAIN_BATCH_SIZE = 32 #@param {type:\"slider\", min:16, max:32, step:16}\n","\n","#Must be set to 8 because on a TPU, model will truncate last few entries in prediction/evaluation if they don't fit in the specified batch size\n","#As there are 8 TPU cores, this ensures each instance will be attended to\n","EVAL_BATCH_SIZE = 8 \n","PREDICT_BATCH_SIZE = 8 \n","\n","LEARNING_RATE = 0.00002 #@param {type:\"slider\", min:1e-5, max:5e-5, step:1e-6}\n","#@markdown The parameters below are not relevant if the FT_MODEL is set to 'Default\n","NUM_EXTRA_LAYERS = 2 #@param {type:\"slider\", min:1, max:3, step:1}\n","HIDDEN_SIZE = 256 #@param {type:\"slider\", min:32, max:384, step:4}\n","#pos = train.label.value_counts()[1]\n","#neg = train.label.value_counts()[0]\n","pos = 1\n","neg = 2\n","#neg = train.label.value_counts()[0]\n","FT_PARAMS = [FT_MODEL, LOSS_FN, NUM_EXTRA_LAYERS, HIDDEN_SIZE, pos, neg, WEIGHTED_BIAS, NORMALISE_EMBEDDINGS, DROPOUT]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7siI74tc27eN","colab_type":"text"},"source":["Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator.\n","\n","This is a pretty standard design pattern for working with Tensorflow Estimators"]},{"cell_type":"code","metadata":{"id":"JUu_zpYV25z5","colab_type":"code","colab":{}},"source":["# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=True,)\n","\n","# Input function for dev data, we feed in our previously created dev_features for this\n","test_input_fn = run_classifier.input_fn_builder(\n","    features=dev_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FqOhpYMeRRE1","colab_type":"text"},"source":["The estimator will not be defined globally, as the parameters we will feed into it will dynamically change depending on the function."]},{"cell_type":"markdown","metadata":{"id":"BOXk7HODjN3u","colab_type":"text"},"source":["# Cross Validation Evaluation\n","\n","Does not provide in depth tensorflow logging but it does provide evaluation at the end. We combine the  training and dev data created above and evaluate on a stratified fraction of the combined data for each fold.\n","\n","This evaluation is used to test if a change in code or addition of a feature has benefited the overall system"]},{"cell_type":"code","metadata":{"id":"gMlfX8QOjOZG","colab_type":"code","cellView":"form","colab":{}},"source":["#@markdown Cross-Validation Params:\n","NUM_TRAIN_STEPS = 750 #@param {type:\"slider\", min:0, max:10000, step:50}\n","FOLDS = 5 #@param {type:\"slider\", min:3, max:10, step:1}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOaQC-y_jOuT","colab_type":"code","colab":{}},"source":["def bertCV(data, folds = FOLDS):\n","\n","  #Filter out all log messages so console isn't consumed with memory\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","  #Dataframe where grid search results will be stored. Empty to begin with\n","  eval_df = pd.DataFrame(columns = ['F1 Score', 'auc', 'Accuracy'] )\n","  \n","  WARMUP_PROPORTION = 0.1\n","  \n","  k = 1 # Fold counter\n","  \n","  #Stratified K fold ensures the folds are made by preserving the percentage of samples for each class.\n","  cv = StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n","\n","  # Sticking within the training dataset for evaluation. Data is the combination of the provided train and dev sets\n","  for train_index, dev_index in cv.split(data.tweet, data.label): \n","    \n","    #Shuffling again because otherwise the StratifiedKFold function groups a lot of 0's at the start\n","    training  = data.iloc[train_index]\n","    training = training.sample(frac = 1, random_state=SEED)\n","    develop = data.iloc[dev_index]\n","    develop = develop.sample(frac = 1, random_state=SEED)\n","\n","    \n","    \"\"\"Unlike before where I only one test set and one training set, this time I have K different sets of training and testing.\n","    Therefore, in each fold I need to get a new set of data and convert it to features each time.\"\"\"\n","    \n","    # Use the InputExample class from BERT's run_classifier code to create examples from the data\n","\n","    train_InputExamples = training.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                      text_a = x[DATA_COLUMN], \n","                                                                      text_b = None, \n","                                                                      label = x[LABEL_COLUMN]), axis = 1)\n","\n","    dev_InputExamples = develop.apply(lambda x: run_classifier.InputExample(guid=None, \n","                                                                      text_a = x[DATA_COLUMN], \n","                                                                      text_b = None, \n","                                                                      label = x[LABEL_COLUMN]), axis = 1)\n","    \n","    #Convert these examples to features that BERT can interpret\n","    train_features =  run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","    dev_features = run_classifier.convert_examples_to_features(dev_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","\n","    #Delete prior model graph, checkpoints and eval files to make room for new model each loop\n","    try:\n","      tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","    except:\n","    # Doesn't matter if the directory didn't exist\n","      pass\n","    tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","    num_warmup_steps = int(NUM_TRAIN_STEPS * WARMUP_PROPORTION)\n","\n","    # Model configs\n","    \n","    model_fn = run_classifier.model_fn_builder(\n","    bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","    num_labels=len(label_list),\n","    init_checkpoint=bert_ckpt_file,\n","    learning_rate=LEARNING_RATE,\n","    num_train_steps=NUM_TRAIN_STEPS,\n","    num_warmup_steps=num_warmup_steps,\n","    use_tpu=True,\n","    ft_params = FT_PARAMS)\n","\n","    estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","      use_tpu=True,\n","      model_fn=model_fn,\n","      config=RUN_CONFIG,\n","      train_batch_size=TRAIN_BATCH_SIZE,\n","      eval_batch_size=EVAL_BATCH_SIZE,\n","      predict_batch_size=PREDICT_BATCH_SIZE)\n","    \n","    # Create an input function for training. drop_remainder = True for using TPUs.\n","    train_input_fn = run_classifier.input_fn_builder(\n","        features=train_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=True,\n","        drop_remainder=True)\n","\n","    #input function for dev data, we feed in our previously created dev_features for this\n","    dev_input_fn = run_classifier.input_fn_builder(\n","        features=dev_features,\n","        seq_length=MAX_SEQ_LENGTH,\n","        is_training=False,\n","        drop_remainder=True)\n","   \n","    \n","    current_time = datetime.now()\n","    estimator.train(input_fn=train_input_fn, max_steps=NUM_TRAIN_STEPS )\n","    train_time = datetime.now() - current_time\n","    \n","\n","    #You need to provide number of steps for a TPU\n","    eval_steps = int(len(dev_InputExamples) / EVAL_BATCH_SIZE)\n","\n","    #Eval may be slightly WRONG on the TPU because it will truncate the last batch.\n","    eval_results = estimator.evaluate(input_fn=dev_input_fn, steps=eval_steps)\n","\n","    row = pd.Series({'F1 Score': eval_results['F1_Score'], 'auc': eval_results['auc'], 'Accuracy': eval_results['eval_accuracy'] * 100,'Precision': eval_results['precision'],'Recall': eval_results['recall'],\\\n","                                    'False Negatives': eval_results['false_negatives'],'False Positives': eval_results['false_positives'],\\\n","                    'True Negatives':eval_results['true_negatives'] ,'True Positives': eval_results['true_positives'], 'Training Time': train_time })\n","    row = pd.Series(row, name = 'Fold ' + str(k))\n","\n","    \"\"\"Below statement controls for whenever we get a bad fold which results in a model predicting only one class.\n","    This isn't truly representative of normal performance and can bring down CV score, so we omit model evaluation\n","    if the below statement is true\"\"\"\n","    if eval_results['false_negatives'] < 1 or eval_results['false_positives'] < 1: \n","      print(\"Classifier predicts one class. Thus not recording this metric as it will skew CV\\n\")\n","      #k = k + 1\n","      continue\n","\n","    eval_df = eval_df.append(row)\n","    print(\"Fold \" + str(k) + \":\\tF-Score:\", eval_df[\"F1 Score\"][k-1])\n","    print(\"Training took time \", train_time)\n","    print('---------------------------------------------------------------------------------------------------------\\n')\n","    k = k + 1 #Increment on fold counter\n","\n","  row = eval_df.mean(axis = 0)\n","  row = pd.Series(row, name = 'CV Average')\n","  eval_df = eval_df.append(row)\n","  print(\"\\nTraining Batch Size: \", TRAIN_BATCH_SIZE, \"\\tLearn Rate: \", LEARNING_RATE, \"\\tNumSteps: \", NUM_TRAIN_STEPS)\n","  display(eval_df)\n","\n","  return row # Also return row of CV-Average"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ggwT2gRuj8Fi","colab_type":"text"},"source":["#### Cross-Validation of Cross-Validation\n","\n","Tensorflow 1.x is non-deterministic, which has resulted in the variability between each run to be greater than the difference in performance gained between introductions of different configurations and parameters. This makes it difficult to determine what is the best pre-training, text preprocessing and fine-tuning pipeline to undertake.\n","\n","To better ensure the reliability of experiments my solution is to have a 5 fold cross-validation of a cross-validated sample of my data which will reduce the variance run to run significantly."]},{"cell_type":"code","metadata":{"id":"Nwg3RMrSj90X","colab_type":"code","outputId":"cc355388-95ce-4f1f-9fa0-63c00db66e6a","executionInfo":{"status":"ok","timestamp":1588100837312,"user_tz":-60,"elapsed":10672753,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["data = pre.loadData(rawTrain, rawDev, options = options, dataset = DATASET)\n","\n","#Stratified K fold ensures the folds are made by preserving the percentage of samples for each class.\n","folds = 5\n","cv = StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n","eval_df = pd.DataFrame(columns = ['F1 Score', 'auc', 'Accuracy'])\n","\n","#This will be a 5-fold CV so the sample each time will be a fifth of the data\n","i = 1\n","for __, data_index in cv.split(data.tweet, data.label):\n","  dat = data.iloc[data_index]\n","  CV_Av = bertCV(dat)\n","  CV_Av = pd.Series(CV_Av, name = 'CV Average' + str(i))\n","  eval_df = eval_df.append(CV_Av)\n","\n","row = eval_df.mean(axis = 0)\n","row = pd.Series(row, name = '2MLP + 0.3 dropout')\n","eval_df1 = pd.read_csv('gs://csc3002/hateval2019/models_eval_df.csv', sep=',',  index_col = 0, encoding = 'utf-8')\n","eval_df1 = eval_df1.append(row)\n","eval_df1.to_csv('gs://csc3002/hateval2019/models_eval_df.csv', sep=',',  index = True, encoding = 'utf-8')\n","eval_df1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","\n","Fold 1:\tF-Score: 0.7507330775260925\n","Training took time  0:08:09.590302\n","---------------------------------------------------------------------------------------------------------\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SHqEs8G6C8X7","colab_type":"text"},"source":["# Running Evaluation Whilst Training - Early Stopping hooks\n","\n","Below is a custom function to run evaluation on the fine-tuned BERT model whilst training. The `tf.train_and_evaluate` function for tensorflow estimators doesn't support early stopping hooks with a distributed TPU strategy as of writing. \n","\n","Thus, instead of being able to evaluate the model in memory, we must save the model graph and metadata to a checkpoint and reload it every n steps we want to run an evaluation.\n","\n","The function finds the optimum number of steps the fine-tuning should run for based upon F1 Score by testing the trained model to that point against the previously created dev set.\n"]},{"cell_type":"markdown","metadata":{"id":"LtA8C9ANYbe7","colab_type":"text"},"source":["<b>First setting up params and function that dynamically loads global setp from checkpoint dir"]},{"cell_type":"code","metadata":{"id":"fQsD0qy6DE5U","colab_type":"code","colab":{}},"source":["#We'll set a large value for train steps because we want to make this model run\n","#for as long as possible before it finds the optimimum model\n","\n","params = {'train_steps': 1500000, #An early stop will occur before this step is reached\n","            'num_train_features': len(train_features),\n","            'num_eval_features': len(dev_features)\n","            }\n","\n","          \n","def load_global_step_from_checkpoint_dir(checkpoint_dir):\n","  try:\n","    checkpoint_reader = tf.train.NewCheckpointReader(\n","        tf.train.latest_checkpoint(checkpoint_dir))\n","    return checkpoint_reader.get_tensor(tf.GraphKeys.GLOBAL_STEP)\n","  except:  \n","    return 0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SvjkAkTJYzG5","colab_type":"text"},"source":["<b>Train and evaluate function </b>"]},{"cell_type":"code","metadata":{"id":"rOMmCdDJ5DLm","colab_type":"code","colab":{}},"source":["def train_and_evaluate(out_dir, params, steps_per_eval, eval_after_step, stop_after_iter, metric):\n","\n","#Delete prior model graph, checkpoints and eval files to enable consecutive runs, rather than resetting runtime\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","  # Doesn't matter if the directory didn't exist\n","    pass\n","  tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","  if DATASET == 'Hateval':\n","    \n","    #Cannot be dynamically set so we'll set warmup to these statuc values, should be around 8 - 12% of steps\n","    num_warmup_steps = 70\n","  else:\n","    num_warmup_steps = 300\n","\n","  max_steps = params['train_steps']\n","\n","\n","  model_fn = run_classifier.model_fn_builder(\n","    bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","    num_labels=len(label_list),\n","    init_checkpoint=bert_ckpt_file,\n","    learning_rate=LEARNING_RATE,\n","    num_train_steps=NUM_TRAIN_STEPS,\n","    num_warmup_steps=num_warmup_steps,\n","    use_tpu=True,\n","    ft_params = FT_PARAMS)\n","\n","  estimator = tf.contrib.tpu.TPUEstimator(\n","    use_tpu=True,\n","    model_fn=model_fn,\n","    config=RUN_CONFIG,\n","    train_batch_size=TRAIN_BATCH_SIZE,\n","    eval_batch_size=EVAL_BATCH_SIZE,\n","    predict_batch_size=PREDICT_BATCH_SIZE)\n","      \n"," # load last checkpoint step and start from there\n","  current_step = load_global_step_from_checkpoint_dir(out_dir)\n","  \n","  print('\\nMax number of training steps is {:d}. Current step {:d}.'\\\n","        .format(max_steps , current_step))\n","\n","  start_timestamp = time.time()  # This time will include compilation time\n","  best_score = 0\n","  best_model = 0\n","  while current_step < max_steps:\n","    # Train for up to steps_per_eval number of steps.\n","    # At the end of training, a checkpoint will be written to --model_dir.\n","    if current_step < eval_after_step:\n","      next_checkpoint = eval_after_step\n","    else:\n","      next_checkpoint = min(current_step + steps_per_eval, max_steps)\n","    estimator.train(input_fn=train_input_fn, max_steps=next_checkpoint)\n","    current_step = next_checkpoint\n","    print('\\nFinished training up to step {:d}. Elapsed seconds {:d}.\\n'.format(\n","                    next_checkpoint, int(time.time() - start_timestamp)))\n","\n","    print('\\nStarting to evaluate at step {:d} \\n'.format(next_checkpoint))\n","    eval_results = estimator.evaluate(\n","      input_fn=test_input_fn,\n","      steps=params['num_eval_features'] // EVAL_BATCH_SIZE)\n","    print('\\nEval results at step {:d}: \\n'.format(next_checkpoint), eval_results)\n","    print('\\n')\n","    \n","    current_score = eval_results[metric]\n","    if current_score > best_score:\n","      best_score = current_score \n","      best_model = current_step\n","      score_buffer = [] #Reset buffer\n","    else:\n","      score_buffer.append(current_score)\n","    #If 3 times in a row evaluation results haven't improved; we stop training\n","    if len(score_buffer) == stop_after_iter:\n","      elapsed_time = int(time.time() - start_timestamp)\n","      \n","      print('\\nFinished training at step {:d} as there has been no improvement on the previous {:d} iterations'.format(current_step, stop_after_iter),\n","      '\\nElapsed seconds {:d}. \\n'.format(elapsed_time), \n","      \"\\nBest model is at step {:d} with the best F-score {:f}\".format(best_model, best_score))\n","      \n","      # Remotely edit the protocol buffer file so best model step is loaded\n","      storage_client = storage.Client()\n","      bucket = storage_client.get_bucket(BUCKET)\n","      blob = bucket.get_blob(os.path.join(output_dir, 'checkpoint'))\n","      string_blob = blob.download_as_string()\n","      temp_str = string_blob.decode()\n","      temp_str = temp_str.replace(str(current_step), str(best_model), 1)\n","      string_blob = temp_str.encode()\n","      blob.upload_from_string(string_blob)\n","      new_ckpt_file = tf.train.latest_checkpoint(OUTPUT_DIR)\n","      \n","      assert new_ckpt_file is not None, \"File was not edited correctly\"\n","      return new_ckpt_file, best_model\n","    \n","  elapsed_time = int(time.time() - start_timestamp)\n","  print('\\nFinished training up to step {:d}. Elapsed seconds {:d}. \\n'.format(max_steps, elapsed_time))\n","  # Remotely edit the protocol buffer file so best model step is loaded\n","  storage_client = storage.Client()\n","  bucket = storage_client.get_bucket(BUCKET)\n","  blob = bucket.get_blob(os.path.join(output_dir, 'checkpoint'))\n","  string_blob = blob.download_as_string()\n","  temp_str = string_blob.decode()\n","  temp_str = temp_str.replace(str(current_step), str(best_model), 1)\n","  string_blob = temp_str.encode()\n","  blob.upload_from_string(string_blob)\n","  new_ckpt_file = tf.train.latest_checkpoint(OUTPUT_DIR)\n","  \n","  assert new_ckpt_file is not None, \"File was not edited correctly\"\n","  return new_ckpt_file, best_model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CWD2qtHSeykT","colab_type":"text"},"source":["Now run the train_and_evaluate function. We can toggle the steps_per_eval in the params to control how often we checkpoint and evaluate"]},{"cell_type":"code","metadata":{"id":"3MKmvUBXkKEA","colab_type":"code","cellView":"form","colab":{}},"source":["#@title `train_and_evaluate` Params:\n","#@markdown The metric we evaluate on:\n","metric = \"F1_Score\" #@param [\"F1_Score\", \"auc\", \"eval_loss\", \"eval_accuracy\"]\n","#@markdown The step to begin evaluation at:\n","eval_after_step = 600 #@param {type:\"slider\", min:100, max:10000, step:100}\n","#@markdown After eval_after_step value, how often we evaluate model:\n","steps_per_eval = 100 #@param {type:\"slider\", min:50, max:1000, step:50}\n","#@markdown If no increase in metric after this many iterations, we stop function:\n","stop_after_iter = 2 #@param {type:\"slider\", min:1, max:10, step:1}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPY3ilir5RsY","colab_type":"code","outputId":"faeff000-f2c4-416f-dcf5-e388653e6c26","executionInfo":{"status":"ok","timestamp":1587834910110,"user_tz":-60,"elapsed":2107359,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["new_ckpt_file, best_model = train_and_evaluate(OUTPUT_DIR, params, steps_per_eval , eval_after_step, stop_after_iter, metric) \n","print(\"\\nBest checkpoint for model is at\", new_ckpt_file)\n","print(\"If training from scratch below - train with both train & dev set - recommended steps for training is\", int((best_model + (best_model) *0.2)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","train_batch_size=32  eval_batch_size=8  max_steps=1500000\n","WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f74758d3e18>) includes params argument, but params are not passed to Estimator.\n","INFO:tensorflow:Using config: {'_model_dir': 'gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1', '_tf_random_seed': 3060, '_save_summary_steps': 100, '_save_checkpoints_steps': 100000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.93.26.106:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f746f5d86a0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.93.26.106:8470', '_evaluation_master': 'grpc://10.93.26.106:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f7480784828>}\n","INFO:tensorflow:_TPUContext: eval_on_tpu True\n","\n","Max number of training steps is 1500000.Current step 0.\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.93.26.106:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 1844458405544473000)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 13979878266200047441)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13888816125458795035)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9137339701391963521)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 12005622833657184557)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 11061496171486753239)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 7807762849859781555)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 11513120344828326353)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 17565857546622016556)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 9282909018018295004)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2980000763503795627)\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","INFO:tensorflow:Calling model_fn.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","\n","INFO:tensorflow:Using loss type:focal_loss\n","WARNING:tensorflow:From /content/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /content/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n","\n","WARNING:tensorflow:From /content/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /content/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","INFO:tensorflow:Using Bi-Directional LTSM for Fine-Tuning. 2 extra layer(s)\n","WARNING:tensorflow:From /content/bert/run_classifier.py:615: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/rnn/python/ops/rnn.py:239: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /content/bert/run_classifier.py:776: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","WARNING:tensorflow:From /content/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","WARNING:tensorflow:From /content/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n","\n","WARNING:tensorflow:From /content/bert/run_classifier.py:785: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n","\n","WARNING:tensorflow:From /content/bert/run_classifier.py:786: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n","\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:751: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer Variable.assign which has equivalent behavior in 2.X.\n","INFO:tensorflow:Initialized dataset iterators in 1 seconds\n","INFO:tensorflow:Installing graceful shutdown hook.\n","INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 0 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:loss = 0.29955453, step = 100\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (1, 38)\n","INFO:tensorflow:loss = 0.2689337, step = 200 (45.473 sec)\n","INFO:tensorflow:global_step/sec: 2.19911\n","INFO:tensorflow:examples/sec: 70.3714\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 0.14488147, step = 300 (37.767 sec)\n","INFO:tensorflow:global_step/sec: 2.64784\n","INFO:tensorflow:examples/sec: 84.7308\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (3, 0)\n","INFO:tensorflow:loss = 0.0074071954, step = 400 (39.826 sec)\n","INFO:tensorflow:global_step/sec: 2.51089\n","INFO:tensorflow:examples/sec: 80.3484\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (4, 60)\n","INFO:tensorflow:loss = 0.29092228, step = 500 (38.158 sec)\n","INFO:tensorflow:global_step/sec: 2.62069\n","INFO:tensorflow:examples/sec: 83.862\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 0.0041930005, step = 600 (37.847 sec)\n","INFO:tensorflow:global_step/sec: 2.64222\n","INFO:tensorflow:examples/sec: 84.5511\n","INFO:tensorflow:Saving checkpoints for 600 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt.\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Loss for final step: 0.0041930005.\n","INFO:tensorflow:training_loop marked as finished\n","\n","Finished training up to step 600. Elapsed seconds 597.\n","\n","\n","Starting to evaluate at step 600 \n","\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (1, 256)\n","INFO:tensorflow:  name = input_mask, shape = (1, 256)\n","INFO:tensorflow:  name = label_ids, shape = (1,)\n","INFO:tensorflow:  name = segment_ids, shape = (1, 256)\n","\n","INFO:tensorflow:Using loss type:focal_loss\n","INFO:tensorflow:Using Bi-Directional LTSM for Fine-Tuning. 2 extra layer(s)\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3322: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-05-02T11:54:36Z\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt-600\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 0 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Initialized dataset iterators in 0 seconds\n","INFO:tensorflow:Enqueue next (125) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (125) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:Evaluation [125/125]\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Finished evaluation at 2020-05-02-11:56:24\n","INFO:tensorflow:Saving dict for global step 600: F1_Score = 0.7824222, auc = 0.80436385, eval_accuracy = 0.797, eval_loss = 0.1844742, false_negatives = 62.0, false_positives = 141.0, global_step = 600, loss = 0.12206413, precision = 0.7213439, recall = 0.85480094, true_negatives = 432.0, true_positives = 365.0\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 600: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt-600\n","INFO:tensorflow:evaluation_loop marked as finished\n","\n","Eval results at step 600: \n"," {'F1_Score': 0.7824222, 'auc': 0.80436385, 'eval_accuracy': 0.797, 'eval_loss': 0.1844742, 'false_negatives': 62.0, 'false_positives': 141.0, 'loss': 0.12206413, 'precision': 0.7213439, 'recall': 0.85480094, 'true_negatives': 432.0, 'true_positives': 365.0, 'global_step': 600}\n","\n","\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","\n","INFO:tensorflow:Using loss type:focal_loss\n","INFO:tensorflow:Using Bi-Directional LTSM for Fine-Tuning. 2 extra layer(s)\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt-600\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file utilities to get mtimes.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 600 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt.\n","INFO:tensorflow:Initialized dataset iterators in 2 seconds\n","INFO:tensorflow:Installing graceful shutdown hook.\n","INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 0 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:loss = 0.0080419695, step = 700\n","INFO:tensorflow:Saving checkpoints for 700 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt.\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Loss for final step: 0.0080419695.\n","INFO:tensorflow:training_loop marked as finished\n","\n","Finished training up to step 700. Elapsed seconds 1034.\n","\n","\n","Starting to evaluate at step 700 \n","\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (1, 256)\n","INFO:tensorflow:  name = input_mask, shape = (1, 256)\n","INFO:tensorflow:  name = label_ids, shape = (1,)\n","INFO:tensorflow:  name = segment_ids, shape = (1, 256)\n","\n","INFO:tensorflow:Using loss type:focal_loss\n","INFO:tensorflow:Using Bi-Directional LTSM for Fine-Tuning. 2 extra layer(s)\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-05-02T12:01:52Z\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt-700\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 0 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Initialized dataset iterators in 0 seconds\n","INFO:tensorflow:Enqueue next (125) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (125) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:Evaluation [125/125]\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Finished evaluation at 2020-05-02-12:04:01\n","INFO:tensorflow:Saving dict for global step 700: F1_Score = 0.79216534, auc = 0.81453663, eval_accuracy = 0.809, eval_loss = 0.19722769, false_negatives = 63.0, false_positives = 128.0, global_step = 700, loss = 0.12556912, precision = 0.7398374, recall = 0.852459, true_negatives = 445.0, true_positives = 364.0\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 700: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt-700\n","INFO:tensorflow:evaluation_loop marked as finished\n","\n","Eval results at step 700: \n"," {'F1_Score': 0.79216534, 'auc': 0.81453663, 'eval_accuracy': 0.809, 'eval_loss': 0.19722769, 'false_negatives': 63.0, 'false_positives': 128.0, 'loss': 0.12556912, 'precision': 0.7398374, 'recall': 0.852459, 'true_negatives': 445.0, 'true_positives': 364.0, 'global_step': 700}\n","\n","\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","\n","INFO:tensorflow:Using loss type:focal_loss\n","INFO:tensorflow:Using Bi-Directional LTSM for Fine-Tuning. 2 extra layer(s)\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt-700\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 700 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt.\n","INFO:tensorflow:Initialized dataset iterators in 2 seconds\n","INFO:tensorflow:Installing graceful shutdown hook.\n","INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 0 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:loss = 0.006988541, step = 800\n","INFO:tensorflow:Saving checkpoints for 800 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt.\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Loss for final step: 0.006988541.\n","INFO:tensorflow:training_loop marked as finished\n","\n","Finished training up to step 800. Elapsed seconds 1498.\n","\n","\n","Starting to evaluate at step 800 \n","\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (1, 256)\n","INFO:tensorflow:  name = input_mask, shape = (1, 256)\n","INFO:tensorflow:  name = label_ids, shape = (1,)\n","INFO:tensorflow:  name = segment_ids, shape = (1, 256)\n","\n","INFO:tensorflow:Using loss type:focal_loss\n","INFO:tensorflow:Using Bi-Directional LTSM for Fine-Tuning. 2 extra layer(s)\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-05-02T12:09:38Z\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt-800\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 0 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Initialized dataset iterators in 0 seconds\n","INFO:tensorflow:Enqueue next (125) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (125) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:Evaluation [125/125]\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Finished evaluation at 2020-05-02-12:11:25\n","INFO:tensorflow:Saving dict for global step 800: F1_Score = 0.7973712, auc = 0.8197723, eval_accuracy = 0.815, eval_loss = 0.20315175, false_negatives = 63.0, false_positives = 122.0, global_step = 800, loss = 0.1265269, precision = 0.74897116, recall = 0.852459, true_negatives = 451.0, true_positives = 364.0\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 800: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt-800\n","INFO:tensorflow:evaluation_loop marked as finished\n","\n","Eval results at step 800: \n"," {'F1_Score': 0.7973712, 'auc': 0.8197723, 'eval_accuracy': 0.815, 'eval_loss': 0.20315175, 'false_negatives': 63.0, 'false_positives': 122.0, 'loss': 0.1265269, 'precision': 0.74897116, 'recall': 0.852459, 'true_negatives': 451.0, 'true_positives': 364.0, 'global_step': 800}\n","\n","\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","\n","INFO:tensorflow:Using loss type:focal_loss\n","INFO:tensorflow:Using Bi-Directional LTSM for Fine-Tuning. 2 extra layer(s)\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt-800\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 800 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt.\n","INFO:tensorflow:Initialized dataset iterators in 2 seconds\n","INFO:tensorflow:Installing graceful shutdown hook.\n","INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 0 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:loss = 0.0068127727, step = 900\n","INFO:tensorflow:Saving checkpoints for 900 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt.\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Loss for final step: 0.0068127727.\n","INFO:tensorflow:training_loop marked as finished\n","\n","Finished training up to step 900. Elapsed seconds 1945.\n","\n","\n","Starting to evaluate at step 900 \n","\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (1, 256)\n","INFO:tensorflow:  name = input_mask, shape = (1, 256)\n","INFO:tensorflow:  name = label_ids, shape = (1,)\n","INFO:tensorflow:  name = segment_ids, shape = (1, 256)\n","\n","INFO:tensorflow:Using loss type:focal_loss\n","INFO:tensorflow:Using Bi-Directional LTSM for Fine-Tuning. 2 extra layer(s)\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-05-02T12:17:04Z\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt-900\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 0 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Initialized dataset iterators in 0 seconds\n","INFO:tensorflow:Enqueue next (125) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (125) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:Evaluation [125/125]\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Finished evaluation at 2020-05-02-12:18:39\n","INFO:tensorflow:Saving dict for global step 900: F1_Score = 0.7973712, auc = 0.8197723, eval_accuracy = 0.815, eval_loss = 0.20315175, false_negatives = 63.0, false_positives = 122.0, global_step = 900, loss = 0.1265269, precision = 0.74897116, recall = 0.852459, true_negatives = 451.0, true_positives = 364.0\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 900: gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt-900\n","INFO:tensorflow:evaluation_loop marked as finished\n","\n","Eval results at step 900: \n"," {'F1_Score': 0.7973712, 'auc': 0.8197723, 'eval_accuracy': 0.815, 'eval_loss': 0.20315175, 'false_negatives': 63.0, 'false_positives': 122.0, 'loss': 0.1265269, 'precision': 0.74897116, 'recall': 0.852459, 'true_negatives': 451.0, 'true_positives': 364.0, 'global_step': 900}\n","\n","\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","\n","INFO:tensorflow:Using loss type:focal_loss\n","INFO:tensorflow:Using Bi-Directional LTSM for Fine-Tuning. 2 extra layer(s)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I7Qwmr3GP1-6","colab_type":"text"},"source":["# Testing Model\n","\n"]},{"cell_type":"code","metadata":{"id":"ndDfzpRan8SA","colab_type":"code","cellView":"form","colab":{}},"source":["#@title  Final Train + Test Params:\n","#@markdown - <b>JUST_PREDICT:</b> If you wish to use already trained model, which is the best checkpoint found in `train_and_evaluate()` if it has been executed.\n","\n","#@markdown - <b>TRAIN_FROM_SCRATCH:</b> If you want to build new model from scratch - using data from both train and dev data for training - then set NUM_TRAIN_STEPS to recommened value printed above.\n","OPTION = \"TRAIN_FROM_SCRATCH\" #@param [\"TRAIN_FROM_SCRATCH\", \"JUST_PREDICT\"]\n","\n","#@markdown Below params only necessary if training from scratch\n","LEARNING_RATE = 0.00002 #@param {type:\"slider\", min:1e-5, max:5e-5, step:1e-6}\n","NUM_TRAIN_STEPS = 1050 #@param {type:\"slider\", min:0, max:10000, step:50}\n","\n","#@markdown Tick box if you wish to oversample the hate speech data to correct imbalance in the dataset\n","OVERSAMPLE = False #@param {type:\"boolean\"}\n","\n","#@markdown You can oversample hate speech or non-hate speech data\n","oversampleLabel = \"Hate\" #@param [\"Hate\", \"Not Hate\"]\n","#@markdown Choose the number of times over you want the subsample you've selected to be mulitplied.\n","multiplier = 2 #@param {type:\"slider\", min:1, max:10, step:1}\n","#@markdown If you just want to balance the class labelling of the set, set multiplier to 1. If the label is set to the majority class and multiplier = 1 then this will result in an undersample\n","\n","def oversample(train, lab = \"Hate\", multiplier = 1):\n","    \n","    neg_train = train.loc[train['label'] == 0]\n","    pos_train = train.loc[train['label'] == 1]\n","    \n","     #Whether we're sampling from hate or not hate for the term\n","    if lab == \"Hate\":\n","      aug_set = pos_train\n","    else:\n","      aug_set = neg_train\n","\n","    ids = np.arange(len(aug_set))\n","\n","    #You can multiply your set by a chosen number\n","    if multiplier > 1:\n","      choices = np.random.choice(ids, len(aug_set) * multiplier )\n","\n","    #Or you can simply match the opposite label and either undersmple of oversample\n","    else:\n","      choices = np.random.choice(ids, (len(train) - len(aug_set)))\n","\n","    aug_train = aug_set.iloc[choices]\n","\n","    size = len(train)\n","    if lab == \"Hate\":\n","      train = pd.concat([aug_train, neg_train], axis=0)\n","    else:\n","      train = pd.concat([aug_train, pos_train], axis=0)\n","    \n","    print(len(train)-size , \"added tweets\\n\")\n","    #shuffle\n","    train = train.sample(frac = 1, random_state=SEED) #Shuffle data\n","    return train\n","\n","if OVERSAMPLE == True:\n","  print(\"OVERSAMPLING DATA\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cXLCiTsGnnVE","colab_type":"text"},"source":["Loading in train and test data...\n","\n","<i>N.B Edit words array within this cell if you'd like to oversample based on words</i>"]},{"cell_type":"code","metadata":{"id":"c3KKifBM4kx5","colab_type":"code","outputId":"bb5cb5f8-6da9-42db-9536-92c419e9940e","executionInfo":{"status":"ok","timestamp":1588517330029,"user_tz":-60,"elapsed":9534,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["train = pre.loadData(rawTrain, rawDev, options = options, dataset = DATASET)\n","if OVERSAMPLE == True:\n","  \n","  train = oversample(train,oversampleLabel, multiplier) \n","  \n","print(\"\\nOut of {} tweets in the training database, {} are not {}, {} are {}\".format(len(train.index),\n","                                                      len(train[train['label']==0]), classification_type,\n","                                                      len(train[train['label']==1]), classification_type))\n","test = pre.loadData(rawTest, options = options, dataset = DATASET)\n","\n","#Authors have identified some dupliactes in the set \n","test.drop_duplicates(subset = \"tweet\", inplace = True)\n","\n","print(\"\\nOut of {} tweets in the testing database, {} are not {}, {} are {}\".format(len(test.index),\n","                                                      len(test[test['label']==0]), classification_type,\n","                                                      len(test[test['label']==1]), classification_type))\n","test.head()"],"execution_count":33,"outputs":[{"output_type":"stream","text":["\n","Out of 10000 tweets in the training database, 5790 are not hate, 4210 are hate\n","\n","Out of 2971 tweets in the testing database, 1714 are not hate, 1257 are hate\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2317</th>\n","      <td>31035</td>\n","      <td>anyone whoever doubted louis and said he couldnt sing go listen to back to you and if u dont change ur mind ur just a bitter bitch</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2867</th>\n","      <td>34114</td>\n","      <td>user bitch i was fuckn up till 430 but your hoe ass didnt text me back rage</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1378</th>\n","      <td>34255</td>\n","      <td>this is nancynancy called my pay raise crumbs nancy doesnt want to fund the military nancy puts illegal aliens rights before citizen rightsnancy wants to house the illegals before our homeless veterans dont be a nancyuser trump wednesday wisdom maga</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2281</th>\n","      <td>34280</td>\n","      <td>user well bitch tell me how you download viss</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1481</th>\n","      <td>33582</td>\n","      <td>tx man arrested trying to get into house with knife heriberto coronado 28 is alleged to have held a knife to a female victims throat at one point as well he was also named in a detainer on an immigration charge deport them all</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id  ... label\n","2317  31035  ...     0\n","2867  34114  ...     1\n","1378  34255  ...     0\n","2281  34280  ...     0\n","1481  33582  ...     0\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"ZcNSY0kA8N4N","colab_type":"text"},"source":["Function to get predictions on test data"]},{"cell_type":"code","metadata":{"id":"mvpPwfj08PmW","colab_type":"code","colab":{}},"source":["def getPrediction(in_sentences):\n","  #Makes output less verbose\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","  labels = [0, 1]\n","  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n","  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n","  predictions = list(estimator.predict(predict_input_fn))\n","\n","  #Initialise empty predicted labels array\n","  predicted_classes = [None] * len(predictions)\n","\n","  #Use a for loop to iterate through probabilities and for each prediction assign a label\n","  #corresponding to which label has the highest probability\n","  for i in range(0, len(predictions)):\n","    if predictions[i]['probabilities'][0] > predictions[i]['probabilities'][1]:\n","      predicted_classes[i] = 0\n","    else:\n","      predicted_classes[i] = 1\n","  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO) # Reset tensorflow verboisty to normal\n","\n","  return predicted_classes"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rt4d5H3A6Iqu","colab_type":"text"},"source":["Converting to features, setting run and model configs.\n","\n","Then training on train and dev set and predicting on unseen test set "]},{"cell_type":"code","metadata":{"id":"Xf0Ofg3M6HBC","colab_type":"code","outputId":"e1a172ec-f20e-4528-e809-151b20af89cf","executionInfo":{"status":"ok","timestamp":1588518095892,"user_tz":-60,"elapsed":770167,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["if OPTION == 'TRAIN_FROM_SCRATCH':\n","  \n","  bert_ckpt_file = tf.train.latest_checkpoint(OUTPUT_DIR.replace('output1', ''))\n","  train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, \n","                                                                        text_a = x[DATA_COLUMN], \n","                                                                        text_b = None, \n","                                                                        label = x[LABEL_COLUMN]), axis = 1)\n","\n","  train_features =  run_classifier.convert_examples_to_features(train_InputExamples, label_list, \\\n","                                                                MAX_SEQ_LENGTH, tokenizer)\n","\n","\n","  #Delete prior model graph, checkpoints and eval files to make room for new model each loop\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","  # Doesn't matter if the directory didn't exist\n","    pass\n","  tf.gfile.MakeDirs(OUTPUT_DIR)\n","\n","if OPTION == 'JUST_PREDICT':\n","  bert_ckpt_file = new_ckpt_file\n","  \n","\n","# Compute # warmup steps\n","num_warmup_steps = int(NUM_TRAIN_STEPS * 0.1)\n","\n","# Model configs\n","\n","model_fn = run_classifier.model_fn_builder(\n","  bert_config= run_classifier.modeling.BertConfig.from_json_file(bert_config_file),\n","  num_labels=len(label_list),\n","  init_checkpoint=bert_ckpt_file,\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=NUM_TRAIN_STEPS,\n","  num_warmup_steps=num_warmup_steps,\n","  use_tpu=True,\n","  ft_params = FT_PARAMS)\n","\n","estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n","  use_tpu=True,\n","  model_fn=model_fn,\n","  config=RUN_CONFIG,\n","  train_batch_size=TRAIN_BATCH_SIZE,\n","  eval_batch_size=EVAL_BATCH_SIZE,\n","  predict_batch_size=PREDICT_BATCH_SIZE)\n","\n","# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=True)\n","\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n","\n","\n","if OPTION != \"JUST_PREDICT\":\n","\n","  print(\"\\nThe model will stop training when it reaches\", NUM_TRAIN_STEPS, \"as a checkpoint\")\n","  print(f'Beginning Training!')\n","  current_time = datetime.now()\n","  estimator.train(input_fn=train_input_fn, max_steps=NUM_TRAIN_STEPS)\n","  train_time = datetime.now() - current_time\n","  print(\"Training took time \", train_time)\n"," \n","#Run testing\n","predictions = getPrediction(test.tweet)\n","if DATASET == \"HatEval\":\n","\n","  test['predictions'] = predictions\n","  test.to_csv('gs://csc3002/hateval2019/predictions.csv', sep=',',  index = True, encoding = 'utf-8')\n","\n","  row = pd.Series({'F1 Score': metrics.f1_score(test.label, test.predictions),\\\n","                   \"Macro F1 Score\": metrics.f1_score(test.label, test.predictions, average = 'macro'),\\\n","                   'auc': metrics.roc_auc_score(test.label, test.predictions),\\\n","                   'Accuracy': metrics.accuracy_score(test.label, test.predictions),\\\n","                   'Precision': metrics.precision_score(test.label, test.predictions),\\\n","                   'Recall': metrics.recall_score(test.label, test.predictions),\\\n","                   'Training Time': train_time, 'steps':  NUM_TRAIN_STEPS})\n","    \n","  display(row)\n","\n","elif DATASET == \"AnalyticsVidhya\":\n","  test['label'] = predictions\n","  print(test.label.value_counts())\n","  print(predictions[0:20])\n","  test.drop(columns = ['tweet'], axis = 1,inplace = True)\n","  test.to_csv('gs://csc3002/trial/submission.csv', sep=',', index = False)\n"],"execution_count":35,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 10000\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] drake i love you we should date rihanna the savage [SEP]\n","INFO:tensorflow:input_ids: 101 7867 1045 2293 2017 2057 2323 3058 25439 1996 9576 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] an honour to meet so many dedicated aid workers here this is mit ##un from user who d recently attended training on child protection provided by user building up specialist refugee protection expertise within national ngos is part of our work here aid works ro ##hing ##ya [SEP]\n","INFO:tensorflow:input_ids: 101 2019 6225 2000 3113 2061 2116 4056 4681 3667 2182 2023 2003 10210 4609 2013 5310 2040 1040 3728 3230 2731 2006 2775 3860 3024 2011 5310 2311 2039 8325 13141 3860 11532 2306 2120 22165 2003 2112 1997 2256 2147 2182 4681 2573 20996 12053 3148 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] despite bo ##ko hara ##m threat some displaced nigerian ##s return via user [SEP]\n","INFO:tensorflow:input_ids: 101 2750 8945 3683 18820 2213 5081 2070 12936 11884 2015 2709 3081 5310 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] user rage of women of trump in the white house made them individually fearless to turn on wei ##nstein blame 1 man not all men [SEP]\n","INFO:tensorflow:input_ids: 101 5310 7385 1997 2308 1997 8398 1999 1996 2317 2160 2081 2068 14258 22518 2000 2735 2006 11417 15493 7499 1015 2158 2025 2035 2273 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","INFO:tensorflow:*** Example ***\n","INFO:tensorflow:guid: None\n","INFO:tensorflow:tokens: [CLS] french police officers injured by drunk uk bound migrants in calais via user [SEP]\n","INFO:tensorflow:input_ids: 101 2413 2610 3738 5229 2011 7144 2866 5391 16836 1999 23116 3081 5310 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:label: 0 (id = 0)\n","WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f5a586de0d0>) includes params argument, but params are not passed to Estimator.\n","INFO:tensorflow:Using config: {'_model_dir': 'gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1', '_tf_random_seed': 3060, '_save_summary_steps': 100, '_save_checkpoints_steps': 100000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.24.148.34:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5a4e1cab00>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.24.148.34:8470', '_evaluation_master': 'grpc://10.24.148.34:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f5a698ba898>}\n","INFO:tensorflow:_TPUContext: eval_on_tpu True\n","\n","The model will stop training when it reaches 1050 as a checkpoint\n","Beginning Training!\n","INFO:tensorflow:Querying Tensorflow master (grpc://10.24.148.34:8470) for TPU system metadata.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 8683200615022957186)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9226226371191721150)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2160041704481069622)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3637750446220270026)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1901758220501687172)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 1300034293481789848)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 8208820991044687334)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15920630802117453453)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15400997574767701873)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 16177985237882721680)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 4461409481405910974)\n","INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:*** Features ***\n","INFO:tensorflow:  name = input_ids, shape = (4, 256)\n","INFO:tensorflow:  name = input_mask, shape = (4, 256)\n","INFO:tensorflow:  name = label_ids, shape = (4,)\n","INFO:tensorflow:  name = segment_ids, shape = (4, 256)\n","\n","INFO:tensorflow:Using loss type:weighted_loss\n","INFO:tensorflow:Using original BERT Model for Fine-Tuning\n","\n","WARNING:tensorflow:From /content/csc3002_detecting_hate_speech/bert/run_classifier.py:732: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:TPU job name worker\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt.\n","INFO:tensorflow:Initialized dataset iterators in 1 seconds\n","INFO:tensorflow:Installing graceful shutdown hook.\n","INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","INFO:tensorflow:Init TPU system\n","INFO:tensorflow:Initialized TPU in 0 seconds\n","INFO:tensorflow:Starting infeed thread controller.\n","INFO:tensorflow:Starting outfeed thread controller.\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (0, 0)\n","INFO:tensorflow:loss = 0.04279881, step = 100\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 0.010956753, step = 200 (30.729 sec)\n","INFO:tensorflow:global_step/sec: 3.25429\n","INFO:tensorflow:examples/sec: 104.137\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (2, 18)\n","INFO:tensorflow:loss = 0.609822, step = 300 (22.959 sec)\n","INFO:tensorflow:global_step/sec: 4.35552\n","INFO:tensorflow:examples/sec: 139.377\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 0.0019513774, step = 400 (22.865 sec)\n","INFO:tensorflow:global_step/sec: 4.37353\n","INFO:tensorflow:examples/sec: 139.953\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (4, 93)\n","INFO:tensorflow:loss = 0.0025481374, step = 500 (23.017 sec)\n","INFO:tensorflow:global_step/sec: 4.34453\n","INFO:tensorflow:examples/sec: 139.025\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 0.3547603, step = 600 (23.073 sec)\n","INFO:tensorflow:global_step/sec: 4.33399\n","INFO:tensorflow:examples/sec: 138.688\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 0.031051274, step = 700 (26.168 sec)\n","INFO:tensorflow:global_step/sec: 3.82159\n","INFO:tensorflow:examples/sec: 122.291\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (7, 29)\n","INFO:tensorflow:loss = 0.00052083755, step = 800 (23.020 sec)\n","INFO:tensorflow:global_step/sec: 4.34399\n","INFO:tensorflow:examples/sec: 139.008\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 0.0006449515, step = 900 (23.122 sec)\n","INFO:tensorflow:global_step/sec: 4.32488\n","INFO:tensorflow:examples/sec: 138.396\n","INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n","INFO:tensorflow:loss = 0.0005740599, step = 1000 (22.981 sec)\n","INFO:tensorflow:global_step/sec: 4.35145\n","INFO:tensorflow:examples/sec: 139.246\n","INFO:tensorflow:Enqueue next (50) batch(es) of data to infeed.\n","INFO:tensorflow:Dequeue next (50) batch(es) of data from outfeed.\n","INFO:tensorflow:Outfeed finished for iteration (10, 0)\n","INFO:tensorflow:loss = 0.00060103193, step = 1050 (13.611 sec)\n","INFO:tensorflow:global_step/sec: 3.67349\n","INFO:tensorflow:examples/sec: 117.552\n","INFO:tensorflow:Saving checkpoints for 1050 into gs://csc3002/wwm_uncased_L-24_H-1024_A-16/further_pretrained_model1/output1/model.ckpt.\n","INFO:tensorflow:Stop infeed thread controller\n","INFO:tensorflow:Shutting down InfeedController thread.\n","INFO:tensorflow:InfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Infeed thread finished, shutting down.\n","INFO:tensorflow:infeed marked as finished\n","INFO:tensorflow:Stop output thread controller\n","INFO:tensorflow:Shutting down OutfeedController thread.\n","INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n","INFO:tensorflow:Outfeed thread finished, shutting down.\n","INFO:tensorflow:outfeed marked as finished\n","INFO:tensorflow:Shutdown TPU system.\n","INFO:tensorflow:Loss for final step: 0.00060103193.\n","INFO:tensorflow:training_loop marked as finished\n","Training took time  0:11:03.402173\n","WARNING: Entity <function _InputsWithStoppingSignals.insert_stopping_signal.<locals>._map_fn at 0x7f5a58a72488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","WARNING: Entity <function _InputsWithStoppingSignals.insert_stopping_signal.<locals>._map_fn at 0x7f5a557ecea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","WARNING: Entity <function _InputsWithStoppingSignals.__init__.<locals>._set_mask at 0x7f5a4fde6268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["F1 Score                0.648506\n","Macro F1 Score          0.516756\n","auc                     0.609023\n","Accuracy                0.552676\n","Precision               0.485737\n","Recall                  0.975338\n","Training Time     0:11:03.402173\n","steps                       1050\n","dtype: object"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"_Do-iXtCOWBX","colab_type":"text"},"source":["## Error Analysis\n","\n","NOTE: Not available for AnalyticsVidhya dataset as it is an unsupervised set"]},{"cell_type":"code","metadata":{"id":"mEA145I2ZcTP","colab_type":"code","outputId":"633ed231-7456-40f5-9780-2b2f01b32fbd","executionInfo":{"status":"ok","timestamp":1588518095895,"user_tz":-60,"elapsed":769248,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["test.head(40)"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","      <th>predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2317</th>\n","      <td>31035</td>\n","      <td>anyone whoever doubted louis and said he couldnt sing go listen to back to you and if u dont change ur mind ur just a bitter bitch</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2867</th>\n","      <td>34114</td>\n","      <td>user bitch i was fuckn up till 430 but your hoe ass didnt text me back rage</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1378</th>\n","      <td>34255</td>\n","      <td>this is nancynancy called my pay raise crumbs nancy doesnt want to fund the military nancy puts illegal aliens rights before citizen rightsnancy wants to house the illegals before our homeless veterans dont be a nancyuser trump wednesday wisdom maga</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2281</th>\n","      <td>34280</td>\n","      <td>user well bitch tell me how you download viss</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1481</th>\n","      <td>33582</td>\n","      <td>tx man arrested trying to get into house with knife heriberto coronado 28 is alleged to have held a knife to a female victims throat at one point as well he was also named in a detainer on an immigration charge deport them all</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>408</th>\n","      <td>30771</td>\n","      <td>user whatever it takes take the money out of welfare payments to the lazy and use that take it from aid to mexico and south and central america just build that wall</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2370</th>\n","      <td>30691</td>\n","      <td>this bitch got one time to fuck wit me or my dad n ima let her whore ass know</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>248</th>\n","      <td>32096</td>\n","      <td>ban rape fugees ban islam in the us ten year old german boy raped by migrant classmates on school trip via user</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2861</th>\n","      <td>31583</td>\n","      <td>when hoes talk shit about me i started to get mad like hell bitch i can make your life like a living hell dont play with me</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2200</th>\n","      <td>34306</td>\n","      <td>and anyone who says otherwise is either a grade a asshole or has no idea what they are talking about being depressed is a bitch but</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2336</th>\n","      <td>32255</td>\n","      <td>user your shoes are ugly and anarcho primitivism sucks bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1160</th>\n","      <td>30974</td>\n","      <td>multuser this is a pretty sight build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>689</th>\n","      <td>34118</td>\n","      <td>illegal criminal alien raped young girl in alabamaelias pablo who is a resident of guatemala and an illegal alien to the united states has been charged with having sex with a child under the age of 12 deport them all build the damn wall</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2173</th>\n","      <td>30707</td>\n","      <td>im tryna go ghost n come back a whole new bitch like bam</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2444</th>\n","      <td>32163</td>\n","      <td>im actually in the same position but this bitch wanna act like a cunt towards me so guess what u aint knowing shit</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1802</th>\n","      <td>30510</td>\n","      <td>kyungsoo and fucking baekhyun in todays performance of the eve fucking told me im a dirty crack whore bitchhhhh</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2983</th>\n","      <td>32356</td>\n","      <td>you can continue to sit in the punk bitch section with all the other blocked hoes</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>236</th>\n","      <td>31387</td>\n","      <td>user if its gets you upset hes the best man for the job go home strong borders send them back</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2828</th>\n","      <td>32210</td>\n","      <td>bitch take kardash out yo name you aint worthy hoe</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1790</th>\n","      <td>31768</td>\n","      <td>hate seeing articles about what men should or should not weardo to whoever comes up with this sht bitch mind your own business</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>501</th>\n","      <td>32309</td>\n","      <td>i want them all deported i want my vote to count again deport them all no daca no amnesty build the damn wall use the army core of engineers</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1032</th>\n","      <td>30385</td>\n","      <td>what is the thinking behind veterans against trump potus has reformed and improved the va while hundreds died during obama reign waiting for treatmentthey can think what they want but lets be rational maga kag potus trump news vote red no daca walk away red wave</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1551</th>\n","      <td>33424</td>\n","      <td>yall you a hoe hoe me thats hoe culture to you bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2218</th>\n","      <td>32819</td>\n","      <td>as usual i scroll a misogynists timeline and its sports slut shaming wmn calling hoes ugly w sad why dont women like me rts mixed in</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2675</th>\n","      <td>32514</td>\n","      <td>hey bitch stop quoting jimmy tatro you fake ass bitch hoe</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1230</th>\n","      <td>32576</td>\n","      <td>user i have an idea build that wall detention centers would be reduced dramatically if not eliminated problem solved</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2037</th>\n","      <td>32392</td>\n","      <td>eyebrows waxedfeel like a whole new bitch upside down face</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2577</th>\n","      <td>33727</td>\n","      <td>got my young bitch kickin flavor she gone flex on you hoes</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1653</th>\n","      <td>32425</td>\n","      <td>user bitch you the one that said you wanted to play</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2686</th>\n","      <td>34035</td>\n","      <td>and bitch why is you still watching my page hoe you been blocked joy df</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1953</th>\n","      <td>33369</td>\n","      <td>my nail girl was hyping me up the whole time i love your hair i love your eyelashes i love your outfit like yass bitch heres ya tip</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2385</th>\n","      <td>31324</td>\n","      <td>let this hoe touch this kid i swear to fucking god i will murder you bitch facts</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2408</th>\n","      <td>31180</td>\n","      <td>you hoes cant rap stop it bitch hand</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2102</th>\n","      <td>32296</td>\n","      <td>a nigga that beat thou women is a bitch 100</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2011</th>\n","      <td>31788</td>\n","      <td>im not out here entertaining dudes bout shit 1 im pregnant with a whole ass baby 2 what kinda dirty trick ass bitch u take me for</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2034</th>\n","      <td>33483</td>\n","      <td>my mom called me a shoe whore and its true 3 lol</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>33249</td>\n","      <td>illegal immigrants dont be europe</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>612</th>\n","      <td>33505</td>\n","      <td>too many immigrants unlicensed to kill illegal aliens illegal immigration transportation build that wall user user</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2708</th>\n","      <td>32344</td>\n","      <td>multuser u a bitch</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1631</th>\n","      <td>32534</td>\n","      <td>mann fuck a bitch fuck feelings reversed hand with middle finger extended yall hoes sneaky put litter in its place i hate all you bitches heavy exclamation mark wish you bitches die relieved goodnight heart eyes key heart</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id  ... predictions\n","2317  31035  ...           0\n","2867  34114  ...           1\n","1378  34255  ...           1\n","2281  34280  ...           1\n","1481  33582  ...           0\n","408   30771  ...           1\n","2370  30691  ...           1\n","248   32096  ...           1\n","2861  31583  ...           1\n","2200  34306  ...           0\n","2336  32255  ...           1\n","1160  30974  ...           1\n","689   34118  ...           1\n","2173  30707  ...           0\n","2444  32163  ...           1\n","1802  30510  ...           0\n","2983  32356  ...           1\n","236   31387  ...           1\n","2828  32210  ...           1\n","1790  31768  ...           1\n","501   32309  ...           1\n","1032  30385  ...           1\n","1551  33424  ...           1\n","2218  32819  ...           0\n","2675  32514  ...           1\n","1230  32576  ...           1\n","2037  32392  ...           0\n","2577  33727  ...           1\n","1653  32425  ...           1\n","2686  34035  ...           1\n","1953  33369  ...           1\n","2385  31324  ...           1\n","2408  31180  ...           1\n","2102  32296  ...           1\n","2011  31788  ...           0\n","2034  33483  ...           1\n","43    33249  ...           1\n","612   33505  ...           1\n","2708  32344  ...           1\n","1631  32534  ...           1\n","\n","[40 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"B9lyIifupp6U","colab_type":"text"},"source":["### False Negatives\n","\n","There are far fewer false negatives than false positives"]},{"cell_type":"code","metadata":{"id":"w_ek6QNkOXqe","colab_type":"code","outputId":"016a812e-5380-4fbd-e3b1-2d50ac04698b","executionInfo":{"status":"ok","timestamp":1588169388088,"user_tz":-60,"elapsed":1541,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def get_missed_HS(df):\n","  df = df.loc[(df[\"label\"] == 1) & (df[\"predictions\"] == 0)]\n","  return df\n","\n","def get_misclassified_HS(df):\n","  df = df.loc[(df[\"label\"] == 0) & (df[\"predictions\"] == 1)]\n","  return df\n","#Perhaps having a separate function for both is a good idea\n","\n","df = get_missed_HS(test)\n","df.to_csv('gs://csc3002/hateval2019/falsenegatives.csv', sep=',',  index = True, encoding = 'utf-8')\n","df.sample(50)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","      <th>predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2580</th>\n","      <td>31940</td>\n","      <td>user another son of a dirty fucking korean whore bitch</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>30401</td>\n","      <td>the other invasion according to homeland security stats more than 600k aliens overstayed us visas in 2017 deport ice send them home</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>30321</td>\n","      <td>stephen miller the public charge law has been on the books for 100 years it prevents immigrants from burdening the us taxpayer if they cant take care of themselvers or their kids send them back trump maga red nation rising</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>152</th>\n","      <td>33854</td>\n","      <td>multuser it may have to do with plain honest speech so many politicians have the political speech bug theyre accustomed to stating what people want to hear not what they need to like saying it is fucktard ed to believe the mass refugee rape fuge e immigration was good is idiodic</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2681</th>\n","      <td>30599</td>\n","      <td>morning twitter is being anal and trying to be a safe space for terminally offended dont say cunt fuck bitch wanker cunts</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>33193</td>\n","      <td>everyones condemning miller with uncles article yet no one mentioning in 1st para he set foot on ellis island on january 7 1903 he came sent for family legally big differences than people overstaying visas lying to get in and crashing borders send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>31745</td>\n","      <td>user hes wanted for murder but who exactly was paying for their medical bills taxpayers yet funds to support tax payers is running out send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>32754</td>\n","      <td>navy rescued manchester bomber salman abedi from war torn libya three years before deadly terror attack via user west is rescuing people that will kill them send them home bombing attack have some dignity and throw them out</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2740</th>\n","      <td>30844</td>\n","      <td>bitch whore son of bitch shit pig dinosaur elephant rhino monkey dog</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>261</th>\n","      <td>30542</td>\n","      <td>multuser there are no innocent people in detention centres send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>33932</td>\n","      <td>please dont call it rescue its human trafficking ports closed send them back benefit seekers</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2636</th>\n","      <td>31709</td>\n","      <td>user cunt boy got it fucked up i handle shit and handled his bitch ass</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2483</th>\n","      <td>33430</td>\n","      <td>bitch i clearly realized that already look at my name dumb cunt</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2817</th>\n","      <td>30517</td>\n","      <td>magnolia lil wayne remix and i fuck these hoes if i dont dose off on that drank if i do bitch you to blame coochie lame</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2997</th>\n","      <td>31912</td>\n","      <td>multuser the wise user 1 never said you were taught 2 you called me bitch first 3 i dont fuckin</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>212</th>\n","      <td>31373</td>\n","      <td>animals pure and simple they are not to blame for where they were born or how they were raised but as adult men who have escaped their homeland they alone are responsible for how they act during each every moment after crossing the border period send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>447</th>\n","      <td>33715</td>\n","      <td>user stopimmigration</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>230</th>\n","      <td>31101</td>\n","      <td>some separated children in us opt to join their deported parents but if they didnt come in through the front door they shouldnt have a choice send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>446</th>\n","      <td>32394</td>\n","      <td>refugees go home</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2682</th>\n","      <td>32177</td>\n","      <td>yooo yall remember when tyler said rape a pregnant bitch and tell my friends i had a 3sum now hes singing about calling him lol growth</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2422</th>\n","      <td>33256</td>\n","      <td>user thats a good bitch i will be coming to toronto by end of the year so you can kiss my shoes lol</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2563</th>\n","      <td>30769</td>\n","      <td>sarah the triceratops is such a bitchy lil cunt</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>234</th>\n","      <td>30160</td>\n","      <td>heres some elite globalist jews plotting to take over poland you cant make this stuff up eu europe zionism judaism anti white globalism leftism open borders illegal immigration cultural subversion nationalism jewish nwo</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2961</th>\n","      <td>30741</td>\n","      <td>whoever has said zouis is dead or has gave me any shit for believing in zouis you can choke bitch</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>242</th>\n","      <td>31978</td>\n","      <td>there will be a cronulla roits 2 in melbourne we are sick of it send them home</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2654</th>\n","      <td>30485</td>\n","      <td>t went from rape a pregnant bitch and tell my friends i had a threesome to singing about gardens and love wow the character growth</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>584</th>\n","      <td>30970</td>\n","      <td>freddie clenched her fists as the car got close enough to make out the protesters wordshumans first refugees go home mallory shouted into a megaphoneheat burned through freddies face she slowed to a stop in front of the middle building btr2sday</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>174</th>\n","      <td>30852</td>\n","      <td>multuser the democrats dont want to fix immigration because they look at immigrants as democratic votes the republicans dont want to fix immigration because 1 they are spineless 2 they are afraid that the communist dnc will accuse them of being heartless send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2625</th>\n","      <td>32066</td>\n","      <td>on me ima have this bitch blasted i had enough of this bullshit i hate crying cuz i have to hold in the urge to kill an old cunt</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2793</th>\n","      <td>30721</td>\n","      <td>hoes be like happy birthday son i cant believe your 8 already like what you cant believe bitch you had him in high school</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>257</th>\n","      <td>31587</td>\n","      <td>the us is 21 trillion in debt we cant import the poor and give them welfare trump maga send them back immigration red nation rising via user</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2734</th>\n","      <td>30189</td>\n","      <td>get you some money you too old to be gossiping like a lil bitch for a broke hoe yall niggas real life faggots</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2981</th>\n","      <td>30878</td>\n","      <td>dumb cunt ass cheating bitch doesnt deserve shit in this world but herpes multiple slightly smiling face</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>106</th>\n","      <td>32191</td>\n","      <td>user contemplate no youre semi woke1 firang hatred for gypsies us a fact2 any anti firang force even the jihadi rapefugee hordes are a plus for hindusif you disagree with 1 its okay if you think 2 is absurd you have issues end of story</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>32716</td>\n","      <td>new olympic sport illegal migrant stowaways jump lorry illegal immigration boris johnson</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>131</th>\n","      <td>30950</td>\n","      <td>user sea tow the boat back to the safe ports in africa send them back its not late to save europe not wanted not needed</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>86</th>\n","      <td>31600</td>\n","      <td>why am i no longer shocked when i read about the brutality of women from those that europe attempts to help send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2831</th>\n","      <td>31031</td>\n","      <td>im a happy bitch as long as i have the rainbond around and can drag hyunyoung whoever i want to</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>518</th>\n","      <td>33529</td>\n","      <td>multuser a law broken is still a law family or not send them back</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2608</th>\n","      <td>33275</td>\n","      <td>katara u dumb retarded cunt bitch he is an actual human being with actual feelings</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2967</th>\n","      <td>31025</td>\n","      <td>whoever just unfollowed me you a bitch</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>81</th>\n","      <td>30426</td>\n","      <td>so sad the elite politicians and virtue signallers just dont get it illegal immigration is not multiculturism</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2452</th>\n","      <td>31601</td>\n","      <td>okay you know what you chabby fans cant bitch that gabi is over everyday when that whore abigail has been on for 10 days straight days</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2809</th>\n","      <td>30795</td>\n","      <td>am i the only one who agrees with montana gabby is a snakey cunt karmas a bitchmultiple kissing heart love island 2017</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>32220</td>\n","      <td>user republicans in congress dont want to see another republican impeached multuser on secret recording said this all goes away if the republicans dont keep the house nunes tape shows plot to fire rosenstein vote blue no matter who send them home</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2996</th>\n","      <td>30104</td>\n","      <td>user 202 likes to call me a cunt a bitch but i tell him the truth about himself and he cant handle it</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2895</th>\n","      <td>31091</td>\n","      <td>everyone wanna be carefree a hoe spiritual allladat summer 17 like its new multiple face with rolling eyes bitch find yourself and stop following the hashtags</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id  ... predictions\n","2580  31940  ...           0\n","57    30401  ...           0\n","54    30321  ...           0\n","152   33854  ...           0\n","2681  30599  ...           0\n","98    33193  ...           0\n","148   31745  ...           0\n","22    32754  ...           0\n","2740  30844  ...           0\n","261   30542  ...           0\n","95    33932  ...           0\n","2636  31709  ...           0\n","2483  33430  ...           0\n","2817  30517  ...           0\n","2997  31912  ...           0\n","212   31373  ...           0\n","447   33715  ...           0\n","230   31101  ...           0\n","446   32394  ...           0\n","2682  32177  ...           0\n","2422  33256  ...           0\n","2563  30769  ...           0\n","234   30160  ...           0\n","2961  30741  ...           0\n","242   31978  ...           0\n","2654  30485  ...           0\n","584   30970  ...           0\n","174   30852  ...           0\n","2625  32066  ...           0\n","2793  30721  ...           0\n","257   31587  ...           0\n","2734  30189  ...           0\n","2981  30878  ...           0\n","106   32191  ...           0\n","97    32716  ...           0\n","131   30950  ...           0\n","86    31600  ...           0\n","2831  31031  ...           0\n","518   33529  ...           0\n","2608  33275  ...           0\n","2967  31025  ...           0\n","81    30426  ...           0\n","2452  31601  ...           0\n","2809  30795  ...           0\n","65    32220  ...           0\n","2996  30104  ...           0\n","2895  31091  ...           0\n","\n","[47 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"markdown","metadata":{"id":"sinnd-BhpaeN","colab_type":"text"},"source":["### False Positives\n","\n","By far the most misclassifications are false postives, (over 90%)"]},{"cell_type":"code","metadata":{"id":"-kH3pri2hFlU","colab_type":"code","outputId":"8f2f4204-fc13-42ab-824a-b619d6c35293","executionInfo":{"status":"ok","timestamp":1588169388329,"user_tz":-60,"elapsed":1121,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["df = get_misclassified_HS(test)\n","df.to_csv('gs://csc3002/hateval2019/falsepositives.csv', sep=',',  index = True, encoding = 'utf-8')\n","df.sample(50)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","      <th>predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1378</th>\n","      <td>34255</td>\n","      <td>this is nancynancy called my pay raise crumbs nancy doesnt want to fund the military nancy puts illegal aliens rights before citizen rightsnancy wants to house the illegals before our homeless veterans dont be a nancyuser trump wednesday wisdom maga</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2281</th>\n","      <td>34280</td>\n","      <td>user well bitch tell me how you download viss</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2336</th>\n","      <td>32255</td>\n","      <td>user your shoes are ugly and anarcho primitivism sucks bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1160</th>\n","      <td>30974</td>\n","      <td>multuser this is a pretty sight build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2173</th>\n","      <td>30707</td>\n","      <td>im tryna go ghost n come back a whole new bitch like bam</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1790</th>\n","      <td>31768</td>\n","      <td>hate seeing articles about what men should or should not weardo to whoever comes up with this sht bitch mind your own business</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1032</th>\n","      <td>30385</td>\n","      <td>what is the thinking behind veterans against trump potus has reformed improved the va while hundreds died during obama reign waiting for treatmentthey can think what they want but lets be rational maga kag potus trump news vote red no daca walk away red wave</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1551</th>\n","      <td>33424</td>\n","      <td>yall you a hoe hoe me thats hoe culture to you bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1230</th>\n","      <td>32576</td>\n","      <td>user i have an idea build that wall detention centers would be reduced dramatically if not eliminated problem solved</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1653</th>\n","      <td>32425</td>\n","      <td>user bitch you the one that said you wanted to play</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1953</th>\n","      <td>33369</td>\n","      <td>my nail girl was hyping me up the whole time i love your hair i love your eyelashes i love your outfit like yass bitch heres ya tip</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2102</th>\n","      <td>32296</td>\n","      <td>a nigga that beat thou women is a bitch 100</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2034</th>\n","      <td>33483</td>\n","      <td>my mom called me a shoe whore and its true 3 lol</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1631</th>\n","      <td>32534</td>\n","      <td>mann fuck a bitch fuck feelings reversed hand with middle finger extended yall hoes sneaky put litter in its place i hate all you bitches heavy exclamation mark wish you bitches die relieved goodnight heart eyes key heart</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1762</th>\n","      <td>31627</td>\n","      <td>ok i hate to be like that bitch but if your in the us how do you buy the shoes</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1911</th>\n","      <td>30202</td>\n","      <td>user twt taehyung prolly out there like wow did this bitch really just call my beautiful silky 3000 striped navy b</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1507</th>\n","      <td>32173</td>\n","      <td>user they couldnt deal with his hair too vegas bitch vegas whore teas were spilt</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1613</th>\n","      <td>33817</td>\n","      <td>i want them and to fight you bc a bitch cant even buy those shoes in america</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>990</th>\n","      <td>34098</td>\n","      <td>multuser like he ever kept out any threats hes lying as usual build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>973</th>\n","      <td>32791</td>\n","      <td>build that wall ted cruz has a plan on how to help republicans win big in november</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>840</th>\n","      <td>34493</td>\n","      <td>17 year old charged in killing of teenager in annapolis linked to ms 13 no amnesty no daca deport them all multuser these are not anomalies this is becoming the normimmigration reform does not entail giving a free pass</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1616</th>\n","      <td>32631</td>\n","      <td>toe up ass hoe stop worrying bout me and get them roaches out yo baby bed punk bitch speaking head in silhouette you wan me clean yo house for you</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1612</th>\n","      <td>31320</td>\n","      <td>multuser they can scrim whoever they fucking want this isnt a fucking chall you dumb bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1654</th>\n","      <td>30888</td>\n","      <td>user bitch you were supposed to be home 30mins ago you fat hoe</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2253</th>\n","      <td>30802</td>\n","      <td>u have no right to judge which women are better you bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2166</th>\n","      <td>32909</td>\n","      <td>it pisses me off when someone says that a girl is a whore like grow the fuck up and learn to respect women</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>866</th>\n","      <td>34500</td>\n","      <td>user its kind of funny really were the space aliens illegal now we need a wall a roof i think this country has gone totally nuts god bless user for his strength courage build that wall and if he blocks the democrats out thats okay with me</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1282</th>\n","      <td>34382</td>\n","      <td>how did people respond to user tweets todaytuesday 11 sep 2018 160032 utctop hashtags never forget september 11th 911 memorial build that wall re tweet trump most influential accountsuseruseruseruseruser</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>812</th>\n","      <td>34257</td>\n","      <td>multuser hows everything going with the bills to stop immigration</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>785</th>\n","      <td>31645</td>\n","      <td>emotional manipulation is at the forefront of the migrant situation of course feel for the victims of violence in the northern triangle the perpetrators know this and use it to their advantage for financial and selfish gain open borders big business build that wall</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1995</th>\n","      <td>32697</td>\n","      <td>im saving money rn to fuckin buy those puma shoes bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1896</th>\n","      <td>32230</td>\n","      <td>user niggas be eating sleep for dinner cause he feeding a bitch that aint letting him in the same vicinity as the box smh</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1321</th>\n","      <td>32975</td>\n","      <td>u poor deluded fool like those 2016 polls with all ur illegal aliens voting on pollsthe ones u ur corrupt dems paid to win hc the election but lostwere awake ur the ancient historymultuser user</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1132</th>\n","      <td>32980</td>\n","      <td>multuser imperatorrex user understood this in ohio now in florida cant wait to vote in desantis his opponent is a socialist commie maga no daca how do you like us now</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>964</th>\n","      <td>31449</td>\n","      <td>user the usa has a major debt problem and thus cannot afford big tax cuts for the wealthy big globalist corporations rinos wall street usa americans america pj net end daca no amnesty congress multuser jordan user</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>164</th>\n","      <td>32078</td>\n","      <td>westminster terror attack suspect named as sudanese rapefugee who drove around london looking for targets before driving car into cyclists</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1663</th>\n","      <td>31394</td>\n","      <td>me these shoes look scary me to me youre a prison psychologist suck it up bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>987</th>\n","      <td>30262</td>\n","      <td>multuser only 9 thats shameful it should be 900 no amnesty no daca</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2009</th>\n","      <td>31259</td>\n","      <td>period boobs got you feelin like a whole new bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>828</th>\n","      <td>34393</td>\n","      <td>multuser thanks for allowing violent foreign men to walk among us threaten us because of user lawcourt system favours them over decent law abiding people civil war is looming in this country as you do nothing to help or protect the people send them back</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1888</th>\n","      <td>31001</td>\n","      <td>plan to whore out my sexy bitch tomorrow night and look for hot tops here is the funniest response i get joy</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1508</th>\n","      <td>34587</td>\n","      <td>user you killed tht bitch im a drake stan and this hoe sonically better than more life fire i knew when u ran up on faree old u was back</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1332</th>\n","      <td>33255</td>\n","      <td>multuser this refers 2 a favorable political effort by google 2 dem partys pres candidate they were surprised that sure fire voters they gave rides 2 didnt all vote 4 their woman they expected it like the blacks used 2 do but began 2 wake up next up muslims stop the invasion</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1325</th>\n","      <td>31877</td>\n","      <td>ingraham angle so right user dropping the ball and huge mistake to approve any spending before build that wall funding user</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1694</th>\n","      <td>33221</td>\n","      <td>take your shoes off when you enter my home bitch</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1120</th>\n","      <td>31022</td>\n","      <td>its happening folks its happeningpresident multuser is reviewing plans to build that wall 2 options either user or user god bless america god bless president trump via user</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1340</th>\n","      <td>31231</td>\n","      <td>user here is an encouraging word i heard it suggested recently maybe by or on user tv show or maybe by ann herself that somebody should tell our us president multuser we do not need to go to us congress for funds to build that wall build the wall build the damn wall now</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1434</th>\n","      <td>31267</td>\n","      <td>user youve got this thank you for your service and commitment to keeping america safe and prosperous build that wall user</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1128</th>\n","      <td>34454</td>\n","      <td>user build that wall ryan mcconnell try to coax trump away from shutdown using props and flattery</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>982</th>\n","      <td>33761</td>\n","      <td>multuser regal multuser mcmindes user survivor multuser please follow back turn out 2018 maga build that wall kag drain the swamp</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id  ... predictions\n","1378  34255  ...           1\n","2281  34280  ...           1\n","2336  32255  ...           1\n","1160  30974  ...           1\n","2173  30707  ...           1\n","1790  31768  ...           1\n","1032  30385  ...           1\n","1551  33424  ...           1\n","1230  32576  ...           1\n","1653  32425  ...           1\n","1953  33369  ...           1\n","2102  32296  ...           1\n","2034  33483  ...           1\n","1631  32534  ...           1\n","1762  31627  ...           1\n","1911  30202  ...           1\n","1507  32173  ...           1\n","1613  33817  ...           1\n","990   34098  ...           1\n","973   32791  ...           1\n","840   34493  ...           1\n","1616  32631  ...           1\n","1612  31320  ...           1\n","1654  30888  ...           1\n","2253  30802  ...           1\n","2166  32909  ...           1\n","866   34500  ...           1\n","1282  34382  ...           1\n","812   34257  ...           1\n","785   31645  ...           1\n","1995  32697  ...           1\n","1896  32230  ...           1\n","1321  32975  ...           1\n","1132  32980  ...           1\n","964   31449  ...           1\n","164   32078  ...           1\n","1663  31394  ...           1\n","987   30262  ...           1\n","2009  31259  ...           1\n","828   34393  ...           1\n","1888  31001  ...           1\n","1508  34587  ...           1\n","1332  33255  ...           1\n","1325  31877  ...           1\n","1694  33221  ...           1\n","1120  31022  ...           1\n","1340  31231  ...           1\n","1434  31267  ...           1\n","1128  34454  ...           1\n","982   33761  ...           1\n","\n","[50 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"-eP69YnOne5Z","colab_type":"text"},"source":[" ### Inspecting textual content of datasets"]},{"cell_type":"code","metadata":{"id":"ypYB357MkB9N","colab_type":"code","colab":{}},"source":["hatetrain = train[train['label'] == 1]\n","nothatetrain = train[train['label'] == 0]\n","\n","hatetest = test[test['label'] == 1]\n","nothatetest = test[test['label'] == 0]\n","\n","falsepos = get_misclassified_HS(test)\n","falseneg = get_missed_HS(test)\n","\n","dfs = [train, hatetrain, nothatetrain, test, hatetest, nothatetest, falsepos, falseneg]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AwVt4lOYoRQV","colab_type":"text"},"source":["#### % of tweets containing a particular word/phrase\n","\n","Use function below to get a percent of tweets within a dataset containing a certain term"]},{"cell_type":"code","metadata":{"id":"q8JEmV3Zkhac","colab_type":"code","outputId":"da5f413d-47cc-4894-a160-bb05abf2f3c5","executionInfo":{"status":"ok","timestamp":1588116645880,"user_tz":-60,"elapsed":784,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":297}},"source":["def containsWordPercent(word, dfs, saveresult = False):\n","  inds = [\"Train\", \"Train HS\", \"Train ~HS\", \"Test\", \"Test HS\", \"Test ~HS\", \"False Positives\", \"False Negatives\"]\n","  finaldf = pd.DataFrame(columns = [\"No. of Rows Containing {}\".format(word), \"% of Rows Containing {}\".format(word)])\n","  for i, df in  enumerate(dfs):\n","    wordnum = (df.tweet.str.contains(word).sum())\n","    wordperc = wordnum/len(df.index) * 100\n","\n","    row = pd.Series({\"No. of Rows Containing {}\".format(word): wordnum, \"% of Rows Containing {}\".format(word): wordperc })\n","    row = pd.Series(row, name = inds[i])\n","    finaldf = finaldf.append(row)\n","  \n","  finaldf[\"No. of Rows Containing {}\".format(word)] =\\\n","  finaldf[\"No. of Rows Containing {}\".format(word)].astype(int)\n","\n","  display(finaldf)\n","\n","  if saveresult == True:\n","    finaldf.to_csv('gs://csc3002/hateval2019/{}percent.csv'.format(word), sep=',',  index = True, encoding = 'utf-8')\n","\n","\n","containsWordPercent(\"maga\", dfs)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No. of Rows Containing maga</th>\n","      <th>% of Rows Containing maga</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Train</th>\n","      <td>288</td>\n","      <td>2.880000</td>\n","    </tr>\n","    <tr>\n","      <th>Train HS</th>\n","      <td>249</td>\n","      <td>5.914489</td>\n","    </tr>\n","    <tr>\n","      <th>Train ~HS</th>\n","      <td>39</td>\n","      <td>0.673575</td>\n","    </tr>\n","    <tr>\n","      <th>Test</th>\n","      <td>374</td>\n","      <td>12.466667</td>\n","    </tr>\n","    <tr>\n","      <th>Test HS</th>\n","      <td>107</td>\n","      <td>8.492063</td>\n","    </tr>\n","    <tr>\n","      <th>Test ~HS</th>\n","      <td>267</td>\n","      <td>15.344828</td>\n","    </tr>\n","    <tr>\n","      <th>False Positives</th>\n","      <td>267</td>\n","      <td>18.619247</td>\n","    </tr>\n","    <tr>\n","      <th>False Negatives</th>\n","      <td>0</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 No. of Rows Containing maga  % of Rows Containing maga\n","Train                                    288                   2.880000\n","Train HS                                 249                   5.914489\n","Train ~HS                                 39                   0.673575\n","Test                                     374                  12.466667\n","Test HS                                  107                   8.492063\n","Test ~HS                                 267                  15.344828\n","False Positives                          267                  18.619247\n","False Negatives                            0                   0.000000"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"HLrDLQMf7TOV","colab_type":"text"},"source":["#### Most Common Words in Dataset\n","Function below shows the most common words - excluding stopwords. \n","\n","If less than n words are returned then increase maxval within the function `n_mostCommonWords()`\n"]},{"cell_type":"code","metadata":{"id":"6ZdxvvS1tzIE","colab_type":"code","outputId":"8cefe42a-13e6-43fc-ea6a-2aed4f64b7e1","executionInfo":{"status":"ok","timestamp":1587929215247,"user_tz":-60,"elapsed":694,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":669}},"source":["from collections import Counter\n","\n","def n_mostCommonWords(n, df):\n","  maxval = 100\n","  finaldf = pd.DataFrame(columns = ['words', 'count'])\n","  commonwords = Counter(\" \".join(df[\"tweet\"]).split()).most_common(maxval)\n","  wordlist = [pre.remove_stopwords(word[0]) for word in commonwords]\n","  wordlist = [w for w in wordlist if w != '']\n","  countlist = [word[1] for word in commonwords if word[0] in wordlist]\n","  finaldf['words'] = wordlist[:n]\n","  finaldf['count'] = countlist[:n]\n","\n","  percentlist = [(df.tweet.str.contains(word).sum()/len(df.index)) * 100 for word in wordlist[:n]]\n","  finaldf[\"% of tweets containing word\"] = percentlist\n","\n","  display(finaldf)\n","\n","n_mostCommonWords(20, falsepos)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>count</th>\n","      <th>% of tweets containing word</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>user</td>\n","      <td>640</td>\n","      <td>54.588235</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>wall</td>\n","      <td>567</td>\n","      <td>36.549020</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>build</td>\n","      <td>511</td>\n","      <td>35.921569</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>bitch</td>\n","      <td>489</td>\n","      <td>37.411765</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>multuser</td>\n","      <td>315</td>\n","      <td>20.784314</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>maga</td>\n","      <td>261</td>\n","      <td>20.862745</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>daca</td>\n","      <td>220</td>\n","      <td>15.921569</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>trump</td>\n","      <td>210</td>\n","      <td>16.392157</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>america</td>\n","      <td>191</td>\n","      <td>15.843137</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>illegal</td>\n","      <td>190</td>\n","      <td>12.313725</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>hoe</td>\n","      <td>142</td>\n","      <td>21.647059</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>face</td>\n","      <td>139</td>\n","      <td>6.509804</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>aliens</td>\n","      <td>137</td>\n","      <td>8.862745</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>away</td>\n","      <td>131</td>\n","      <td>10.039216</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>back</td>\n","      <td>128</td>\n","      <td>10.588235</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>vote</td>\n","      <td>123</td>\n","      <td>11.215686</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>walk</td>\n","      <td>123</td>\n","      <td>9.882353</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>like</td>\n","      <td>121</td>\n","      <td>9.333333</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>first</td>\n","      <td>117</td>\n","      <td>8.705882</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>dont</td>\n","      <td>109</td>\n","      <td>7.607843</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       words  count  % of tweets containing word\n","0       user    640                    54.588235\n","1       wall    567                    36.549020\n","2      build    511                    35.921569\n","3      bitch    489                    37.411765\n","4   multuser    315                    20.784314\n","5       maga    261                    20.862745\n","6       daca    220                    15.921569\n","7      trump    210                    16.392157\n","8    america    191                    15.843137\n","9    illegal    190                    12.313725\n","10       hoe    142                    21.647059\n","11      face    139                     6.509804\n","12    aliens    137                     8.862745\n","13      away    131                    10.039216\n","14      back    128                    10.588235\n","15      vote    123                    11.215686\n","16      walk    123                     9.882353\n","17      like    121                     9.333333\n","18     first    117                     8.705882\n","19      dont    109                     7.607843"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"fpDsa8ejf14-","colab_type":"text"},"source":["# Using Tensorboard to Get Deeper Insight"]},{"cell_type":"code","metadata":{"id":"x3yg60YrO-Ub","colab_type":"code","outputId":"728472eb-378d-4702-c748-5130f80a5a67","executionInfo":{"status":"ok","timestamp":1587476004222,"user_tz":-60,"elapsed":5138,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-04-21 13:33:21--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 52.73.195.55, 34.192.215.160, 52.3.53.111, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|52.73.195.55|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13773305 (13M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","ngrok-stable-linux- 100%[===================>]  13.13M  28.3MB/s    in 0.5s    \n","\n","2020-04-21 13:33:21 (28.3 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y97-_YvVXeIy","colab_type":"code","outputId":"a3adf5d5-28fb-499d-bf5d-72bfd74b077c","executionInfo":{"status":"ok","timestamp":1587476006138,"user_tz":-60,"elapsed":6646,"user":{"displayName":"Fionn McConville","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc6-YgdnmYK6Gwcczlh_CMfOtdcmJ49ErnzI66=s64","userId":"01326935138765014160"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def get_tensorboard(path_to_event_file = OUTPUT_DIR):\n","  get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 --reload_multifile=true &'\n",".format(path_to_event_file))\n","  \n","  get_ipython().system_raw('./ngrok http 6006 &')\n","\n","  !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","      \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n","\n","get_tensorboard(OUTPUT_DIR)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["https://828d683a.ngrok.io\n"],"name":"stdout"}]}]}